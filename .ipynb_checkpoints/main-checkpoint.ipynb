{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 'research' is activated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the environment is activated\n",
    "if \"CONDA_DEFAULT_ENV\" in os.environ:\n",
    "    print(f\"Environment '{os.environ['CONDA_DEFAULT_ENV']}' is activated.\")\n",
    "else:\n",
    "    print(\"No specific environment is activated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yNSyFZvf0Leo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['MEDV'],\n",
       " Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Boston dataset\n",
    "dataset = \"boston\"\n",
    "raw_df = pd.read_csv(\"data/boston.csv\")\n",
    "target = [\"MEDV\"]\n",
    "features = raw_df.columns.drop(target)\n",
    "target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the store sales dataset\n",
    "# dataset = \"store_sales\"\n",
    "# excel_file = pd.ExcelFile(\"data/store_sales.xlsx\")\n",
    "# sheet_names = excel_file.sheet_names\n",
    "\n",
    "# # Read the data\n",
    "# raw_df = pd.read_excel(excel_file, sheet_name=sheet_names[2])  # 2, 9\n",
    "# iri_key_counts = raw_df[\"IRI_KEY\"].value_counts()\n",
    "# iri_keys = iri_key_counts[iri_key_counts > 300].index\n",
    "\n",
    "\n",
    "# target = [\"Total.Volume\"]\n",
    "# features = [\"F\", \"D\", \"Unit.Price\"]\n",
    "\n",
    "# raw_df = raw_df[raw_df[\"IRI_KEY\"] == iri_keys[0]]\n",
    "\n",
    "# sheet_names, iri_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the California housing dataset\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# dataset = \"california\"\n",
    "# housing = fetch_california_housing()\n",
    "# target = housing.target_names\n",
    "# features = housing.feature_names\n",
    "# raw_df = pd.concat(\n",
    "#     [\n",
    "#         pd.DataFrame(housing.data, columns=housing.feature_names),\n",
    "#         pd.DataFrame(housing.target, columns=housing.target_names),\n",
    "#     ],\n",
    "#     axis=1,\n",
    "# )\n",
    "# target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the soybean dataset\n",
    "# dataset = \"soybean\"\n",
    "# raw_df = pd.read_excel(\"data/soybean.xlsx\")\n",
    "# # X = raw_df.values[:-1, [5, 6, 15, 16, 17, 26,\n",
    "# #                         34, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]]  # 9 check yield 12 rm band\n",
    "# # y = raw_df.values[:-1, 11]\n",
    "# X = raw_df.iloc[:-1, [5, 6, 15, 16, 17, 26,\n",
    "#                   34, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]]\n",
    "# y = raw_df.iloc[:-1, [11]]\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load LengthOfStay\n",
    "# dataset = \"LengthOfStay\"\n",
    "# raw_df = pd.read_csv(\"data/LengthOfStay.csv\")\n",
    "# raw_df = raw_df.drop(columns=[\"eid\", \"vdate\", \"discharged\"])\n",
    "# target = [\"lengthofstay\"]\n",
    "# features = raw_df.columns.drop(target)\n",
    "# target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load HospitalStay\n",
    "# dataset = \"HospitalStay\"\n",
    "# raw_df = pd.read_csv(\"data/Healthcare_Investments_and_Hospital_Stay.csv\")\n",
    "# target = [\"Hospital_Stay\"]\n",
    "# features = raw_df.columns.drop(target)\n",
    "# target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dataset: boston'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677083</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT  \n",
       "count  506.000000  \n",
       "mean    12.653063  \n",
       "std      7.141062  \n",
       "min      1.730000  \n",
       "25%      6.950000  \n",
       "50%     11.360000  \n",
       "75%     16.955000  \n",
       "max     37.970000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MEDV\n",
       "count  506.000000\n",
       "mean    22.532806\n",
       "std      9.197104\n",
       "min      5.000000\n",
       "25%     17.025000\n",
       "50%     21.200000\n",
       "75%     25.000000\n",
       "max     50.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = raw_df[features]\n",
    "y = raw_df[target]\n",
    "\n",
    "display(f\"Dataset: {dataset}\", X.describe(), y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Outliers\n",
    "# def remove_outliers(df, threshold=3):\n",
    "#     z_scores = np.abs((df - df.mean()) / df.std())\n",
    "#     return df[(z_scores < threshold).all(axis=1)]\n",
    "# filtered_train_data = train_data\n",
    "# for col in train_data.columns:\n",
    "#     value_counts = train_data[col].value_counts().sort(by=\"count\")\n",
    "#     valid = value_counts.filter(pl.col(\"count\") > value_counts[\"count\"].max()/len(value_counts))[col]\n",
    "#     filtered_train_data = filtered_train_data.filter(pl.col(col).is_in(valid))\n",
    "# display(filtered_train_data, filtered_train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_data(X, y, cols=4):\n",
    "    fig, axs = plt.subplots(\n",
    "        (X.shape[1]+X.shape[1]*y.shape[1] + y.shape[1]+cols-1)//cols, cols, figsize=(20, 15))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    a = 0\n",
    "    for i, col in enumerate(X.columns):\n",
    "        data = X[col].to_numpy()\n",
    "        ax = axs[a+i]\n",
    "        sns.histplot(data, kde=True, ax=ax)\n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel(col)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\")\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\")\n",
    "    a += i + 1\n",
    "    for i, col in enumerate(X.columns):\n",
    "        data = X[col].to_numpy()\n",
    "        for j, ycol in enumerate(y.columns):\n",
    "            ax = axs[a+i+j*y.shape[1]]\n",
    "            sns.scatterplot(x=data, y=y[ycol], ax=ax)\n",
    "            ax.set_title(f\"{col} vs {ycol}\")\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel(ycol)\n",
    "    a += i + 1\n",
    "    for i, col in enumerate(y.columns):\n",
    "        data = y[col].to_numpy()\n",
    "        ax = axs[a+i]\n",
    "        sns.histplot(data, kde=True, ax=ax)\n",
    "        ax.set_title(col)\n",
    "    a += i+1\n",
    "    for j in range(a, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search\n",
    "from optuna.integration.sklearn import OptunaSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    TargetEncoder,\n",
    ")\n",
    "\n",
    "CATEGORICAL_PREPROCESSORS = {\n",
    "    \"drop\": \"drop\",\n",
    "    \"ordinal\": OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "    \"one_hot\": OneHotEncoder(\n",
    "        handle_unknown=\"ignore\", max_categories=20, sparse_output=False\n",
    "    ),\n",
    "    \"target\": TargetEncoder(target_type=\"continuous\"),\n",
    "}\n",
    "\n",
    "SCALERS = {\n",
    "    \"identity\": None,\n",
    "    \"standard\": StandardScaler(),\n",
    "    \"minmax\": MinMaxScaler(),\n",
    "    \"robust\": RobustScaler(),\n",
    "    \"quantile\": QuantileTransformer(),\n",
    "}\n",
    "\n",
    "search_params = {\n",
    "    \"cv\": 5,\n",
    "    \"n_jobs\": -1,  # -1,\n",
    "    \"n_trials\": 5, # 1\n",
    "    # \"n_trials\": None,\n",
    "    \"random_state\": 42,\n",
    "    \"return_train_score\": True,\n",
    "    \"scoring\": \"neg_mean_squared_error\",  # r2\n",
    "    # \"timeout\": 10,\n",
    "    # \"timeout\": None,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "def search(\n",
    "    model,\n",
    "    param_grid={},\n",
    "    categorical_preprocessor=\"drop\",\n",
    "    scaler=\"identity\",\n",
    "    search_params=search_params,\n",
    "):\n",
    "    search_params = search_params.copy()\n",
    "    # search_params[\"n_trials\"] = int(4**len(param_grid))\n",
    "\n",
    "    numerical_features = X.select_dtypes(include=[\"number\"]).columns\n",
    "    categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"numerical\", \"passthrough\", numerical_features),\n",
    "            (\n",
    "                \"categorical\",\n",
    "                CATEGORICAL_PREPROCESSORS[categorical_preprocessor],\n",
    "                categorical_features,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return OptunaSearchCV(\n",
    "        Pipeline(\n",
    "            [\n",
    "                (\"categorical_preprocessor\", preprocessor),\n",
    "                (\"scaler\", SCALERS[scaler]),\n",
    "                (\"model\", model),\n",
    "            ]\n",
    "        ),\n",
    "        {f\"model__{k}\": v for k, v in param_grid.items()},\n",
    "        **search_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search grids\n",
    "from optuna.distributions import (\n",
    "    CategoricalDistribution,\n",
    "    FloatDistribution,\n",
    "    IntDistribution,\n",
    ")\n",
    "\n",
    "mlp_sk_param_grid = {\n",
    "    \"learning_rate_init\": FloatDistribution(1e-2, 1e-1, log=True),\n",
    "    \"max_iter\": IntDistribution(400, 400, log=True),\n",
    "    \"early_stopping\": CategoricalDistribution([True]),\n",
    "    \"n_iter_no_change\": IntDistribution(20,20)\n",
    "}\n",
    "\n",
    "# fonn1_sk_param_grid = {\n",
    "#     \"learning_rate_init\": FloatDistribution(1e-2, 1e-1, log=True),\n",
    "#     \"max_iter\": IntDistribution(400, 400, log=True),\n",
    "# }\n",
    "\n",
    "# fonn2_sk_param_grid = {\n",
    "#     \"learning_rate_init\": FloatDistribution(1e-2, 1e-1, log=True),\n",
    "#     \"max_iter\": IntDistribution(400, 400, log=True),\n",
    "# }\n",
    "\n",
    "mlp_TrANN_param_grid = {\n",
    "    \"learning_rate\": FloatDistribution(1e-2, 1e-1, log=True),\n",
    "    \"max_iter\": IntDistribution(400, 400, log=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahteh/miniconda3/envs/research/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1093/2285695493.py:69: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(\n",
      "/tmp/ipykernel_1093/2285695493.py:69: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(\n",
      "/tmp/ipykernel_1093/2285695493.py:69: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(\n",
      "/tmp/ipykernel_1093/2285695493.py:69: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(\n",
      "/tmp/ipykernel_1093/2285695493.py:69: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(\n",
      "/tmp/ipykernel_1093/2285695493.py:69: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(\n",
      "/tmp/ipykernel_1093/2285695493.py:69: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(\n",
      "/tmp/ipykernel_1093/2285695493.py:69: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(\n",
      "/tmp/ipykernel_1093/2285695493.py:69: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(\n",
      "/tmp/ipykernel_1093/2285695493.py:69: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Tree': OptunaSearchCV(cv=5,\n",
       "                estimator=Pipeline(steps=[('categorical_preprocessor',\n",
       "                                           ColumnTransformer(transformers=[('numerical',\n",
       "                                                                            'passthrough',\n",
       "                                                                            Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object')),\n",
       "                                                                           ('categorical',\n",
       "                                                                            'drop',\n",
       "                                                                            Index([], dtype='object'))])),\n",
       "                                          ('scaler', None), ('model', Tree())]),\n",
       "                n_jobs=-1, n_trials=5, param_distributions={}, random_state=42,\n",
       "                return_train_score=True, scoring='neg_mean_squared_error'),\n",
       " 'Ensemble_sk 10': OptunaSearchCV(cv=5,\n",
       "                estimator=Pipeline(steps=[('categorical_preprocessor',\n",
       "                                           ColumnTransformer(transformers=[('numerical',\n",
       "                                                                            'passthrough',\n",
       "                                                                            Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object')),\n",
       "                                                                           ('categorical',\n",
       "                                                                            'drop',\n",
       "                                                                            Index([], dtype='object'))])),\n",
       "                                          ('scaler', None),\n",
       "                                          ('model', Ensemble(n_estimators=10))]),\n",
       "                n_jobs=-1, n_trials=5, param_distributions={}, random_state=42,\n",
       "                return_train_score=True, scoring='neg_mean_squared_error'),\n",
       " 'MLP_sk 10': OptunaSearchCV(cv=5,\n",
       "                estimator=Pipeline(steps=[('categorical_preprocessor',\n",
       "                                           ColumnTransformer(transformers=[('numerical',\n",
       "                                                                            'passthrough',\n",
       "                                                                            Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object')),\n",
       "                                                                           ('categorical',\n",
       "                                                                            'drop',\n",
       "                                                                            Index([], dtype='object'))])),\n",
       "                                          ('scaler', None),\n",
       "                                          ('model',\n",
       "                                           MLP(hidden_layer_sizes=10))]),\n",
       "                n_...\n",
       "                param_distributions={'model__early_stopping': CategoricalDistribution(choices=(True,)),\n",
       "                                     'model__learning_rate_init': FloatDistribution(high=0.1, log=True, low=0.01, step=None),\n",
       "                                     'model__max_iter': IntDistribution(high=400, log=True, low=400, step=1),\n",
       "                                     'model__n_iter_no_change': IntDistribution(high=20, log=False, low=20, step=1)},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_mean_squared_error'),\n",
       " 'FONN1_sk 5 10': OptunaSearchCV(cv=5,\n",
       "                estimator=Pipeline(steps=[('categorical_preprocessor',\n",
       "                                           ColumnTransformer(transformers=[('numerical',\n",
       "                                                                            'passthrough',\n",
       "                                                                            Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object')),\n",
       "                                                                           ('categorical',\n",
       "                                                                            'drop',\n",
       "                                                                            Index([], dtype='object'))])),\n",
       "                                          ('scaler', None),\n",
       "                                          ('model',\n",
       "                                           FONN1(hidden_layer_sizes=15,\n",
       "                                                 num_...\n",
       "                param_distributions={'model__early_stopping': CategoricalDistribution(choices=(True,)),\n",
       "                                     'model__learning_rate_init': FloatDistribution(high=0.1, log=True, low=0.01, step=None),\n",
       "                                     'model__max_iter': IntDistribution(high=400, log=True, low=400, step=1),\n",
       "                                     'model__n_iter_no_change': IntDistribution(high=20, log=False, low=20, step=1)},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_mean_squared_error'),\n",
       " 'FONN2_sk 5 10': OptunaSearchCV(cv=5,\n",
       "                estimator=Pipeline(steps=[('categorical_preprocessor',\n",
       "                                           ColumnTransformer(transformers=[('numerical',\n",
       "                                                                            'passthrough',\n",
       "                                                                            Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object')),\n",
       "                                                                           ('categorical',\n",
       "                                                                            'drop',\n",
       "                                                                            Index([], dtype='object'))])),\n",
       "                                          ('scaler', None),\n",
       "                                          ('model',\n",
       "                                           FONN2(hidden_layer_sizes=15))]),...\n",
       "                param_distributions={'model__early_stopping': CategoricalDistribution(choices=(True,)),\n",
       "                                     'model__learning_rate_init': FloatDistribution(high=0.1, log=True, low=0.01, step=None),\n",
       "                                     'model__max_iter': IntDistribution(high=400, log=True, low=400, step=1),\n",
       "                                     'model__n_iter_no_change': IntDistribution(high=20, log=False, low=20, step=1)},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_mean_squared_error'),\n",
       " 'MLP_sk_target_standard 10': OptunaSearchCV(cv=5,\n",
       "                estimator=Pipeline(steps=[('categorical_preprocessor',\n",
       "                                           ColumnTransformer(transformers=[('numerical',\n",
       "                                                                            'passthrough',\n",
       "                                                                            Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object')),\n",
       "                                                                           ('categorical',\n",
       "                                                                            TargetEncoder(target_type='continuous'),\n",
       "                                                                            Index([], dtype='object'))])),\n",
       "                                          ('scaler', StandardScaler...\n",
       "                param_distributions={'model__early_stopping': CategoricalDistribution(choices=(True,)),\n",
       "                                     'model__learning_rate_init': FloatDistribution(high=0.1, log=True, low=0.01, step=None),\n",
       "                                     'model__max_iter': IntDistribution(high=400, log=True, low=400, step=1),\n",
       "                                     'model__n_iter_no_change': IntDistribution(high=20, log=False, low=20, step=1)},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_mean_squared_error'),\n",
       " 'FONN1_sk_target_standard 5 10': OptunaSearchCV(cv=5,\n",
       "                estimator=Pipeline(steps=[('categorical_preprocessor',\n",
       "                                           ColumnTransformer(transformers=[('numerical',\n",
       "                                                                            'passthrough',\n",
       "                                                                            Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object')),\n",
       "                                                                           ('categorical',\n",
       "                                                                            TargetEncoder(target_type='continuous'),\n",
       "                                                                            Index([], dtype='object'))])),\n",
       "                                          ('scaler', StandardScaler...\n",
       "                param_distributions={'model__early_stopping': CategoricalDistribution(choices=(True,)),\n",
       "                                     'model__learning_rate_init': FloatDistribution(high=0.1, log=True, low=0.01, step=None),\n",
       "                                     'model__max_iter': IntDistribution(high=400, log=True, low=400, step=1),\n",
       "                                     'model__n_iter_no_change': IntDistribution(high=20, log=False, low=20, step=1)},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_mean_squared_error'),\n",
       " 'FONN2_sk_target_standard 5 10': OptunaSearchCV(cv=5,\n",
       "                estimator=Pipeline(steps=[('categorical_preprocessor',\n",
       "                                           ColumnTransformer(transformers=[('numerical',\n",
       "                                                                            'passthrough',\n",
       "                                                                            Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object')),\n",
       "                                                                           ('categorical',\n",
       "                                                                            TargetEncoder(target_type='continuous'),\n",
       "                                                                            Index([], dtype='object'))])),\n",
       "                                          ('scaler', StandardScaler...\n",
       "                param_distributions={'model__early_stopping': CategoricalDistribution(choices=(True,)),\n",
       "                                     'model__learning_rate_init': FloatDistribution(high=0.1, log=True, low=0.01, step=None),\n",
       "                                     'model__max_iter': IntDistribution(high=400, log=True, low=400, step=1),\n",
       "                                     'model__n_iter_no_change': IntDistribution(high=20, log=False, low=20, step=1)},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_mean_squared_error'),\n",
       " 'FONN1_TrANN 5 10': OptunaSearchCV(cv=5,\n",
       "                estimator=Pipeline(steps=[('categorical_preprocessor',\n",
       "                                           ColumnTransformer(transformers=[('numerical',\n",
       "                                                                            'passthrough',\n",
       "                                                                            Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object')),\n",
       "                                                                           ('categorical',\n",
       "                                                                            'drop',\n",
       "                                                                            Index([], dtype='object'))])),\n",
       "                                          ('scaler', None),\n",
       "                                          ('model',\n",
       "                                           <models_TrANN.FONN1 object at 0x7fcdf553a180>)]),\n",
       "                n_jobs=-1, n_trials=5,\n",
       "                param_distributions={'model__learning_rate': FloatDistribution(high=0.1, log=True, low=0.01, step=None),\n",
       "                                     'model__max_iter': IntDistribution(high=400, log=True, low=400, step=1)},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_mean_squared_error'),\n",
       " 'FONN2_TrANN 5 10': OptunaSearchCV(cv=5,\n",
       "                estimator=Pipeline(steps=[('categorical_preprocessor',\n",
       "                                           ColumnTransformer(transformers=[('numerical',\n",
       "                                                                            'passthrough',\n",
       "                                                                            Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "        'PTRATIO', 'B', 'LSTAT'],\n",
       "       dtype='object')),\n",
       "                                                                           ('categorical',\n",
       "                                                                            'drop',\n",
       "                                                                            Index([], dtype='object'))])),\n",
       "                                          ('scaler', None),\n",
       "                                          ('model',\n",
       "                                           <models_TrANN.FONN2 object at 0x7fcdf4640e00>)]),\n",
       "                n_jobs=-1, n_trials=5,\n",
       "                param_distributions={'model__learning_rate': FloatDistribution(high=0.1, log=True, low=0.01, step=None),\n",
       "                                     'model__max_iter': IntDistribution(high=400, log=True, low=400, step=1)},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_mean_squared_error')}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from models import MLP, FONN1, FONN2, TREENN1, TREENN2\n",
    "from models_sklearn import (\n",
    "    Tree,\n",
    "    Ensemble,\n",
    "    MLP as MLP_sk,\n",
    "    FONN1 as FONN1_sk,\n",
    "    FONN2 as FONN2_sk,\n",
    "    TREENN1 as TREENN1_sk,\n",
    "    TREENN2 as TREENN2_sk,\n",
    ")\n",
    "from models_TrANN import (\n",
    "    FONN1 as FONN1_TrANN,\n",
    "    FONN2 as FONN2_TrANN,\n",
    "    FONN3 as FONN3_TrANN,\n",
    "    TREENN1 as TREENN1_TrANN,\n",
    "    TREENN2 as TREENN2_TrANN,\n",
    "    TREENN3 as TREENN3_TrANN,\n",
    ")\n",
    "\n",
    "models = {}\n",
    "\n",
    "num_trees_input = 5\n",
    "num_trees_hidden = 5\n",
    "hidden_nodes = [10]\n",
    "# hidden_nodes = [5, 10]\n",
    "\n",
    "scalers = [\"standard\"]\n",
    "categorical_preprocessor = [\"target\"]\n",
    "\n",
    "models[\"Tree\"] = search(Tree())\n",
    "for hn in hidden_nodes:\n",
    "    models[f\"Ensemble_sk {hn}\"] = search(Ensemble(hn))\n",
    "    models[f\"MLP_sk {hn}\"] = search(MLP_sk(hn), mlp_sk_param_grid)\n",
    "    models[f\"FONN1_sk {num_trees_input} {hn}\"] = search(\n",
    "        FONN1_sk(num_trees_input, num_trees_input + hn), mlp_sk_param_grid\n",
    "    )\n",
    "    models[f\"FONN2_sk {num_trees_hidden} {hn}\"] = search(\n",
    "        FONN2_sk(hn, num_trees_hidden + hn), mlp_sk_param_grid\n",
    "    )\n",
    "for hn in hidden_nodes:\n",
    "    for c in categorical_preprocessor:\n",
    "        for s in scalers:\n",
    "            models[f\"MLP_sk_{c}_{s} {hn}\"] = search(MLP_sk(hn), mlp_sk_param_grid, c, s)\n",
    "            models[f\"FONN1_sk_{c}_{s} {num_trees_input} {hn}\"] = search(\n",
    "                FONN1_sk(num_trees_input, num_trees_input + hn),\n",
    "                mlp_sk_param_grid,\n",
    "                c,\n",
    "                s,\n",
    "            )\n",
    "            models[f\"FONN2_sk_{c}_{s} {num_trees_hidden} {hn}\"] = search(\n",
    "                FONN2_sk(hn, num_trees_hidden + hn), mlp_sk_param_grid, c, s\n",
    "            )\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = 1\n",
    "\n",
    "for hn in hidden_nodes:\n",
    "    models[f\"FONN1_TrANN {num_trees_input} {hn}\"] = search(\n",
    "        FONN1_TrANN(input_dim, hn, output_dim, num_trees_input), mlp_TrANN_param_grid\n",
    "    )\n",
    "    models[f\"FONN2_TrANN {num_trees_hidden} {hn}\"] = search(\n",
    "        FONN2_TrANN(input_dim, hn, output_dim, num_trees_hidden), mlp_TrANN_param_grid\n",
    "    )\n",
    "    # models[f\"FONN3_TrANN {num_trees_hidden} {hn}\"] = search(\n",
    "    #     FONN3_TrANN(input_dim, hn, output_dim, num_trees_hidden), mlp_TrANN_param_grid\n",
    "    # )\n",
    "    # models[f\"TREENN1_TrANN {hn}\"] = search(\n",
    "    #     TREENN1_TrANN(input_dim, hn, output_dim, 1), mlp_TrANN_param_grid\n",
    "    # )\n",
    "    # models[f\"TREENN2_TrANN {hn}\"] = search(\n",
    "    #     TREENN2_TrANN(input_dim, hn, output_dim, 1), mlp_TrANN_param_grid\n",
    "    # )\n",
    "    # models[f\"TREENN2_TrANN {hn}\"] = search(\n",
    "    #     TREENN3_TrANN(input_dim, hn, output_dim, 1), mlp_TrANN_param_grid\n",
    "    # )\n",
    "\n",
    "display(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-23 08:15:02,337] A new study created in memory with name: no-name-a6c61a6d-6ecc-48a3-a00a-ae1baea5a138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-23 08:15:02,730] Trial 1 finished with value: -40.977425548437196 and parameters: {}. Best is trial 1 with value: -40.977425548437196.\n",
      "[I 2025-01-23 08:15:02,748] Trial 2 finished with value: -35.205288099398174 and parameters: {}. Best is trial 2 with value: -35.205288099398174.\n",
      "[I 2025-01-23 08:15:02,752] Trial 0 finished with value: -44.7346862745098 and parameters: {}. Best is trial 2 with value: -35.205288099398174.\n",
      "[I 2025-01-23 08:15:02,810] Trial 3 finished with value: -31.995243642011264 and parameters: {}. Best is trial 3 with value: -31.995243642011264.\n",
      "[I 2025-01-23 08:15:02,840] Trial 4 finished with value: -33.18731935546496 and parameters: {}. Best is trial 3 with value: -31.995243642011264.\n",
      "[I 2025-01-23 08:15:02,864] A new study created in memory with name: no-name-7f0ba8b4-a250-4766-af8f-0f797e363921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Ensemble_sk 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-23 08:15:04,106] Trial 0 finished with value: -24.037054806833627 and parameters: {}. Best is trial 0 with value: -24.037054806833627.\n",
      "[I 2025-01-23 08:15:04,155] Trial 2 finished with value: -21.273807260726066 and parameters: {}. Best is trial 2 with value: -21.273807260726066.\n",
      "[I 2025-01-23 08:15:04,184] Trial 1 finished with value: -24.869754434090463 and parameters: {}. Best is trial 2 with value: -21.273807260726066.\n",
      "[I 2025-01-23 08:15:04,235] Trial 4 finished with value: -24.641349487478163 and parameters: {}. Best is trial 2 with value: -21.273807260726066.\n",
      "[I 2025-01-23 08:15:04,249] Trial 3 finished with value: -23.921928542030678 and parameters: {}. Best is trial 2 with value: -21.273807260726066.\n",
      "[I 2025-01-23 08:15:04,357] A new study created in memory with name: no-name-deea292c-4282-4a99-944f-cd5273bc37cf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting MLP_sk 10...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train and evaluate models\n",
    "cv_results = {}\n",
    "results = []\n",
    "\n",
    "result_columns = [\n",
    "    \"model\", \"mean_fit_time\", \"mean_score_time\", \"mean_train_score\", \"mean_test_score\"\n",
    "]\n",
    "\n",
    "\n",
    "def fit_model(name, model, X, y):\n",
    "    print(f\"Fitting {name}...\")\n",
    "    model.fit(X, y.to_numpy().ravel())\n",
    "    result = model.cv_results_\n",
    "    cv_results[name] = result\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        **{col: result[col][model.best_index_] for col in result}\n",
    "    }\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    results.append(fit_model(name, model, X, y))\n",
    "\n",
    "raw_df = pd.DataFrame(results)\n",
    "# results_df.set_index(\"model\", inplace=True)\n",
    "raw_df = raw_df[result_columns]\n",
    "raw_df.to_csv(f\"output/model_results_{dataset}_{time.strftime('%F_%T')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_df,\n",
    "        raw_df.sort_values(by=\"mean_test_score\", ascending=False),\n",
    "        raw_df.sort_values(by=\"mean_train_score\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({name: model.best_params_ for name,\n",
    "             model in models.items()}).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "predictions = pd.DataFrame(\n",
    "    {name: model.best_estimator_.predict(X).ravel() for name, model in models.items()})\n",
    "display(pd.concat([y, predictions], axis=1),\n",
    "        pd.concat([y, predictions], axis=1).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Predictions\n",
    "plot_data(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all models\n",
    "\n",
    "def plot_loss(model, ax1, ax2, label):\n",
    "    ax1.plot(model.loss_curve_, label=label)\n",
    "    ax1.set_xlabel(\"Iterations\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(model.loss_curve_, label=label)\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.set_xlabel(\"Iterations\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_[\"model\"], \"loss_curve_\"):\n",
    "        plot_loss(model.best_estimator_[\"model\"], ax1, ax2, label=model_name)\n",
    "\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"All models\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_xlabel(\"Iterations\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"All models\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model groups\n",
    "\n",
    "plot_groups = {}\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_[\"model\"], \"loss_curve_\"):\n",
    "        key = model_name.split(\"_\" if \"_\" in model_name else \" \")[0]\n",
    "        if key not in plot_groups:\n",
    "            plot_groups[key] = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig, (ax1, ax2) = plot_groups[key]\n",
    "        plot_loss(model.best_estimator_[\"model\"], ax1, ax2, model_name)\n",
    "\n",
    "for group, plot in plot_groups.items():\n",
    "    fig, (ax1, ax2) = plot\n",
    "    ax1.set_xlabel(\"Iterations\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(group)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.set_xlabel(\"Iterations\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.set_title(group)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual models\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_[\"model\"], \"loss_curve_\"):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        plot_loss(model.best_estimator_[\"model\"], ax1, ax2, model_name)\n",
    "        ax1.set_title(model_name)\n",
    "        ax2.set_title(model_name)\n",
    "        fig.tight_layout()\n",
    "        fig.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
