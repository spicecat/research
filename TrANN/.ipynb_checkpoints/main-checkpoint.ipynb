{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNSyFZvf0Leo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Check if the environment is activated\n",
    "if \"CONDA_DEFAULT_ENV\" in os.environ:\n",
    "    print(f\"Environment '{os.environ['CONDA_DEFAULT_ENV']}' is activated.\")\n",
    "else:\n",
    "    print(\"No specific environment is activated.\")\n",
    "\n",
    "RANDOM_SEED = 42 # For final model reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "cv = None\n",
    "scorer = None\n",
    "X_test = None\n",
    "y_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the Boston dataset\n",
    "# dataset = \"boston\"\n",
    "# raw_df = pd.read_csv(\"data/boston.csv\")\n",
    "# target = [\"MEDV\"]\n",
    "# features = raw_df.columns.drop(target)\n",
    "# target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the store sales dataset\n",
    "# dataset = \"store_sales\"\n",
    "# excel_file = pd.ExcelFile(\"data/store_sales.xlsx\")\n",
    "# sheet_names = excel_file.sheet_names\n",
    "\n",
    "# # Read the data\n",
    "# raw_df = pd.read_excel(excel_file, sheet_name=sheet_names[2])  # 2, 9\n",
    "# iri_key_counts = raw_df[\"IRI_KEY\"].value_counts()\n",
    "# iri_keys = iri_key_counts[iri_key_counts > 300].index\n",
    "\n",
    "\n",
    "# target = [\"Total.Volume\"]\n",
    "# features = [\"F\", \"D\", \"Unit.Price\"]\n",
    "\n",
    "# raw_df = raw_df[raw_df[\"IRI_KEY\"] == iri_keys[0]]\n",
    "\n",
    "# sheet_names, iri_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the California housing dataset\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# dataset = \"california\"\n",
    "# housing = fetch_california_housing()\n",
    "# target = housing.target_names\n",
    "# features = housing.feature_names\n",
    "# raw_df = pd.concat(\n",
    "#     [\n",
    "#         pd.DataFrame(housing.data, columns=housing.feature_names),\n",
    "#         pd.DataFrame(housing.target, columns=housing.target_names),\n",
    "#     ],\n",
    "#     axis=1,\n",
    "# )\n",
    "# target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the soybean dataset\n",
    "# dataset = \"soybean\"\n",
    "# raw_df = pd.read_excel(\"data/soybean.xlsx\")\n",
    "# # X = raw_df.values[:-1, [5, 6, 15, 16, 17, 26,\n",
    "# #                         34, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]]  # 9 check yield 12 rm band\n",
    "# # y = raw_df.values[:-1, 11]\n",
    "# X = raw_df.iloc[:-1, [5, 6, 15, 16, 17, 26,\n",
    "#                   34, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]]\n",
    "# y = raw_df.iloc[:-1, [11]]\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load LengthOfStay\n",
    "# dataset = \"LengthOfStay\"\n",
    "# raw_df = pd.read_csv(\"data/LengthOfStay.csv\")\n",
    "# raw_df = raw_df.drop(columns=[\"eid\", \"vdate\", \"discharged\"])\n",
    "# target = [\"lengthofstay\"]\n",
    "# features = raw_df.columns.drop(target)\n",
    "# target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load HospitalStay\n",
    "# dataset = \"HospitalStay\"\n",
    "# raw_df = pd.read_csv(\"data/Healthcare_Investments_and_Hospital_Stay.csv\")\n",
    "# target = [\"Hospital_Stay\"]\n",
    "# features = raw_df.columns.drop(target)\n",
    "# target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AssetPricing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "dataset = \"AssetPricing\"\n",
    "\n",
    "################################################################################\n",
    "# Characteristics\n",
    "# Variables are constructed using Green, Hand, and Zhang (2017)'s sas code\n",
    "# absacc  : Absolute accruals\n",
    "# beta    : Beta\n",
    "# cfp     : Cash flow to price ratio\n",
    "# chpmia  : Industry-adjusted change in profit margin\n",
    "# ep      : Earnings to price\n",
    "# gma     : Gross profitability\n",
    "# herf    : Industry sales concentration\n",
    "# idiovol : Idiosyncratic return volatility\n",
    "# lev     : Leverage\n",
    "# mom12m  : 12-month momentum\n",
    "# mom6m   : 6-month momentum\n",
    "# nincr   : Number of earnings increases\n",
    "# pchdepr : % change in depreciation\n",
    "# ps      : Financial statements score\n",
    "# roavol  : Earnings volatility\n",
    "# roeq    : Return on equity\n",
    "# salecash: Sales to cash\n",
    "# stdcf   : Cash flow volatility\n",
    "# sue     : Unexpected quarterly earnings\n",
    "# tang    : Debt capacity/firm tangibility\n",
    "features = [\n",
    "    \"absacc\",\n",
    "    \"beta\",\n",
    "    \"cfp\",\n",
    "    \"chpmia\",\n",
    "    \"ep\",\n",
    "    \"gma\",\n",
    "    \"herf\",\n",
    "    \"idiovol\",\n",
    "    \"lev\",\n",
    "    \"mom12m\",\n",
    "    \"mom6m\",\n",
    "    \"nincr\",\n",
    "    \"pchdepr\",\n",
    "    \"ps\",\n",
    "    \"roavol\",\n",
    "    \"roeq\",\n",
    "    \"salecash\",\n",
    "    \"stdcf\",\n",
    "    \"sue\",\n",
    "    \"tang\",\n",
    "]\n",
    "target = [\"exret\"]\n",
    "\n",
    "\n",
    "def winsorize(X, q):\n",
    "    \"\"\"Winsorizes a pandas Series or numpy array.\"\"\"\n",
    "    X = X.copy()\n",
    "    q_l = np.nanquantile(X, q)\n",
    "    q_u = np.nanquantile(X, 1 - q)\n",
    "    if pd.isna(q_l):\n",
    "        q_l = -np.inf\n",
    "    if pd.isna(q_u):\n",
    "        q_u = np.inf\n",
    "    X[X < q_l] = q_l\n",
    "    X[X > q_u] = q_u\n",
    "    return X\n",
    "\n",
    "\n",
    "def cal_r2(y_true, y_pred):\n",
    "    \"\"\"Calculates R2 according to Gu, Kelly, and Xiu (2020).\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    sse = np.sum(np.square(y_true - y_pred))\n",
    "    sst = np.sum(np.square(y_true))  # Denominator without demeaning\n",
    "    if sst == 0:\n",
    "        return 100.0 if sse == 0 else -np.inf\n",
    "    r2 = 100 * (1 - sse / sst)\n",
    "    return r2\n",
    "\n",
    "\n",
    "# Sample split\n",
    "# Training/Validation sample  : 2000 ~ 2019 (15 years)\n",
    "# ym = (year - 1960) * 12 + (month - 1)\n",
    "train_val_start_year = 2000\n",
    "train_val_end_year = 2014\n",
    "test_start_year = 2015\n",
    "test_end_year = 2019\n",
    "\n",
    "# Calculate ym ranges\n",
    "ym_train_val_st = (train_val_start_year - 1960) * 12\n",
    "ym_train_val_ed = (train_val_end_year - 1960) * 12 + 11\n",
    "ym_test_st = (test_start_year - 1960) * 12\n",
    "ym_test_ed = (test_end_year - 1960) * 12 + 11\n",
    "\n",
    "################################################################################\n",
    "# Load data\n",
    "# Remove microcap stocks whose market capitalization fall below the 20th NYSE cutpoint\n",
    "# lme   : 1-month-lagged market equity\n",
    "# retadj: stock monthly return (adjust for delisting)\n",
    "# exret : excess return (= retadj - rf)\n",
    "# rf    : risk-free rate\n",
    "raw_df = pd.read_stata(\"data/ML_sample.dta\")\n",
    "raw_df[\"ym\"] = (raw_df[\"year\"] - 1960) * 12 + (raw_df[\"month\"] - 1)\n",
    "raw_df = raw_df.astype({\"permno\": \"int\", \"year\": \"int\", \"month\": \"int\", \"ym\": \"int\"})\n",
    "raw_df = raw_df[[\"permno\", \"year\", \"month\", \"ym\", \"lme\", \"retadj\", \"exret\"] + features]\n",
    "raw_df = raw_df.sort_values(by=[\"ym\", \"permno\"], ascending=True).reset_index(drop=True)\n",
    "assert not raw_df.duplicated(subset=[\"ym\", \"permno\"]).any()\n",
    "\n",
    "# Data Preparation\n",
    "print(\"Preparing data (winsorizing, standardizing)...\")\n",
    "train_val_mask = (raw_df[\"ym\"] >= ym_train_val_st) & (raw_df[\"ym\"] <= ym_train_val_ed)\n",
    "test_mask = (raw_df[\"ym\"] >= ym_test_st) & (raw_df[\"ym\"] <= ym_test_ed)\n",
    "\n",
    "# Winsorize target only in train/val period\n",
    "raw_df[\"exret_winsor\"] = raw_df[\"exret\"]\n",
    "raw_df.loc[train_val_mask, \"exret_winsor\"] = (\n",
    "    raw_df.loc[train_val_mask].groupby(\"ym\")[\"exret\"].transform(winsorize, q=0.01)\n",
    ")\n",
    "\n",
    "# Cross-sectional standardization (monthly)\n",
    "CMat = raw_df[features].values.copy()\n",
    "unique_yms = raw_df[\"ym\"].unique()\n",
    "for ym_i in unique_yms:\n",
    "    idx_i = raw_df.index[raw_df[\"ym\"] == ym_i].tolist()\n",
    "    if len(idx_i) > 1:\n",
    "        month_data = CMat[idx_i, :]\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(month_data)\n",
    "        np.nan_to_num(scaled_data, copy=False, nan=0.0)\n",
    "        CMat[idx_i, :] = scaled_data\n",
    "    elif len(idx_i) == 1:\n",
    "        CMat[idx_i, :] = 0  # Set features to 0 if only one stock\n",
    "CMat[np.isnan(CMat)] = 0  # Fill original NaNs\n",
    "raw_df[features] = CMat\n",
    "del CMat\n",
    "\n",
    "\n",
    "raw_df.loc[train_val_mask, \"exret\"] = raw_df.loc[train_val_mask, \"exret_winsor\"]\n",
    "X_test = raw_df.loc[test_mask, features]\n",
    "y_test = raw_df.loc[test_mask, target]\n",
    "raw_df=raw_df.loc[train_val_mask]\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "scorer = make_scorer(cal_r2, greater_is_better=True)\n",
    "\n",
    "target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "output_folder = f\"output/{dataset}_{time.strftime('%F_%T')}\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "if not os.path.exists(f\"{output_folder}/models\"):\n",
    "    os.makedirs(f\"{output_folder}/models\")\n",
    "\n",
    "X = raw_df[features]\n",
    "y = raw_df[target]\n",
    "\n",
    "display(f\"output: {output_folder}\", X.describe(), y.describe(), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Outliers\n",
    "# def remove_outliers(df, threshold=3):\n",
    "#     z_scores = np.abs((df - df.mean()) / df.std())\n",
    "#     return df[(z_scores < threshold).all(axis=1)]\n",
    "# filtered_train_data = train_data\n",
    "# for col in train_data.columns:\n",
    "#     value_counts = train_data[col].value_counts().sort(by=\"count\")\n",
    "#     valid = value_counts.filter(pl.col(\"count\") > value_counts[\"count\"].max()/len(value_counts))[col]\n",
    "#     filtered_train_data = filtered_train_data.filter(pl.col(col).is_in(valid))\n",
    "# display(filtered_train_data, filtered_train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_data(X, y, cols=4):\n",
    "    fig, axs = plt.subplots(\n",
    "        (X.shape[1] + X.shape[1] * y.shape[1] + y.shape[1] + cols - 1) // cols,\n",
    "        cols,\n",
    "        figsize=(20, 15),\n",
    "    )\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    a = 0\n",
    "    for i, col in enumerate(X.columns):\n",
    "        data = X[col].to_numpy()\n",
    "        ax = axs[a + i]\n",
    "        sns.histplot(data, kde=True, ax=ax)\n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel(col)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\")\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\")\n",
    "    a += i + 1\n",
    "    for i, col in enumerate(X.columns):\n",
    "        data = X[col].to_numpy()\n",
    "        for j, ycol in enumerate(y.columns):\n",
    "            ax = axs[a + i + j * y.shape[1]]\n",
    "            sns.scatterplot(x=data, y=y[ycol], ax=ax)\n",
    "            ax.set_title(f\"{col} vs {ycol}\")\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel(ycol)\n",
    "    a += i + 1\n",
    "    for i, col in enumerate(y.columns):\n",
    "        data = y[col].to_numpy()\n",
    "        ax = axs[a + i]\n",
    "        sns.histplot(data, kde=True, ax=ax)\n",
    "        ax.set_title(col)\n",
    "    a += i + 1\n",
    "    for j in range(a, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    # fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_data(X, y)\n",
    "fig.savefig(f\"{output_folder}/data.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search\n",
    "from optuna.integration.sklearn import OptunaSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    TargetEncoder,\n",
    ")\n",
    "\n",
    "CATEGORICAL_PREPROCESSORS = {\n",
    "    \"drop\": \"drop\",\n",
    "    \"ordinal\": OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "    \"one_hot\": OneHotEncoder(\n",
    "        handle_unknown=\"ignore\", max_categories=20, sparse_output=False\n",
    "    ),\n",
    "    \"target\": TargetEncoder(target_type=\"continuous\"),\n",
    "}\n",
    "\n",
    "SCALERS = {\n",
    "    \"identity\": None,\n",
    "    \"standard\": StandardScaler(),\n",
    "    \"minmax\": MinMaxScaler(),\n",
    "    \"robust\": RobustScaler(),\n",
    "    \"quantile\": QuantileTransformer(),\n",
    "}\n",
    "\n",
    "search_params = {\n",
    "    \"cv\": cv or 5,\n",
    "    \"n_jobs\": -1,  # -1,\n",
    "    \"n_trials\": 10,  # 1\n",
    "    # \"n_trials\": None,\n",
    "    \"random_state\": 42,\n",
    "    \"return_train_score\": True,\n",
    "    # \"scoring\": \"neg_mean_squared_error\",\n",
    "    \"scoring\": scorer or \"r2\",\n",
    "    # \"timeout\": 10,\n",
    "    # \"timeout\": None,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "def search(\n",
    "    model,\n",
    "    param_grid={},\n",
    "    categorical_preprocessor=\"drop\",\n",
    "    scaler=\"identity\",\n",
    "    search_params=search_params,\n",
    "):\n",
    "    search_params = search_params.copy()\n",
    "    # search_params[\"n_trials\"] = int(4**len(param_grid))\n",
    "\n",
    "    numerical_features = X.select_dtypes(include=[\"number\"]).columns\n",
    "    categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"numerical\", \"passthrough\", numerical_features),\n",
    "            (\n",
    "                \"categorical\",\n",
    "                CATEGORICAL_PREPROCESSORS[categorical_preprocessor],\n",
    "                categorical_features,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return OptunaSearchCV(\n",
    "        Pipeline(\n",
    "            [\n",
    "                (\"categorical_preprocessor\", preprocessor),\n",
    "                (\"scaler\", SCALERS[scaler]),\n",
    "                (\"model\", model),\n",
    "            ]\n",
    "        ),\n",
    "        {f\"model__{k}\": v for k, v in param_grid.items()},\n",
    "        **search_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search grids\n",
    "from optuna.distributions import (\n",
    "    CategoricalDistribution,\n",
    "    FloatDistribution,\n",
    "    IntDistribution,\n",
    ")\n",
    "\n",
    "mlp_sk_param_grid = {\n",
    "    \"learning_rate_init\": FloatDistribution(1e-2, 1e-1, log=True),\n",
    "    \"max_iter\": IntDistribution(100, 100, log=True),\n",
    "    \"early_stopping\": CategoricalDistribution([False]),\n",
    "    \"n_iter_no_change\": IntDistribution(100, 100),\n",
    "}\n",
    "\n",
    "mlp_TrANN_param_grid = {\n",
    "    \"learning_rate_init\": FloatDistribution(1e-2, 1e-1, log=True),\n",
    "    \"max_iter\": IntDistribution(200, 200, log=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import MLP, FONN1, FONN2, TREENN1, TREENN2\n",
    "from models.models_sklearn import (\n",
    "    Tree,\n",
    "    Ensemble,\n",
    "    MLP as MLP_sk,\n",
    "    FONN1 as FONN1_sk,\n",
    "    FONN2 as FONN2_sk,\n",
    "    TREENN1 as TREENN1_sk,\n",
    "    TREENN2 as TREENN2_sk,\n",
    ")\n",
    "from models.models_TrANN import (\n",
    "    FONN1 as FONN1_TrANN,\n",
    "    FONN2 as FONN2_TrANN,\n",
    "    FONN3 as FONN3_TrANN,\n",
    "    TREENN1 as TREENN1_TrANN,\n",
    "    TREENN2 as TREENN2_TrANN,\n",
    "    TREENN3 as TREENN3_TrANN,\n",
    ")\n",
    "\n",
    "models = {}\n",
    "\n",
    "num_trees_input = 5\n",
    "num_trees_hidden = 5\n",
    "hidden_nodes = [5]\n",
    "# hidden_nodes = [5, 10]\n",
    "\n",
    "# categorical_preprocessor = [\"drop\", \"target\"]\n",
    "# scalers = [\"identity\", \"standard\"]\n",
    "scalers = [\"identity\"]\n",
    "categorical_preprocessor = [\"drop\"]\n",
    "\n",
    "models[\"Tree\"] = search(Tree())\n",
    "for hn in hidden_nodes:\n",
    "    models[f\"Ensemble_sk {hn}\"] = search(Ensemble(hn))\n",
    "    for c in categorical_preprocessor:\n",
    "        for s in scalers:\n",
    "            sk_args = (mlp_sk_param_grid, c, s)\n",
    "            trann_args = (mlp_TrANN_param_grid, c, s)\n",
    "            models[f\"MLP_sk_{c}_{s} {hn}\"] = search(MLP_sk(hn), *sk_args)\n",
    "            models[f\"FONN1_sk_{c}_{s} {num_trees_input} {hn}\"] = search(\n",
    "                FONN1_sk(num_trees_input, num_trees_input + hn), *sk_args\n",
    "            )\n",
    "            models[f\"FONN2_sk_{c}_{s} {num_trees_hidden} {hn}\"] = search(\n",
    "                FONN2_sk(num_trees_hidden, num_trees_hidden + hn), *sk_args\n",
    "            )\n",
    "            models[f\"TREENN1_sk_{c}_{s} {hn}\"] = search(TREENN1_sk(1 + hn), *sk_args)\n",
    "            models[f\"TREENN2_sk_{c}_{s} {hn}\"] = search(TREENN2_sk(1 + hn), *sk_args)\n",
    "            # models[f\"FONN1_TrANN_{c}_{s} {num_trees_input} {hn}\"] = search(\n",
    "            #     FONN1_TrANN(hn, num_trees_input), *trann_args\n",
    "            # )\n",
    "            # models[f\"FONN2_TrANN_{c}_{s} {num_trees_hidden} {hn}\"] = search(\n",
    "            #     FONN2_TrANN(hn, num_trees_hidden), *trann_args\n",
    "            # )\n",
    "            # models[f\"FONN3_TrANN_{c}_{s} {num_trees_hidden} {hn}\"] = search(\n",
    "            #     FONN3_TrANN(hn, num_trees_hidden), *trann_args\n",
    "            # )\n",
    "            # models[f\"TREENN1_TrANN_{c}_{s} {hn}\"] = search(\n",
    "            #     TREENN1_TrANN(hn), *trann_args\n",
    "            # )\n",
    "            # models[f\"TREENN2_TrANN_{c}_{s} {hn}\"] = search(\n",
    "            #     TREENN2_TrANN(hn), *trann_args\n",
    "            # )\n",
    "            # models[f\"TREENN3_TrANN_{c}_{s} {hn}\"] = search(\n",
    "            #     TREENN3_TrANN(hn), *trann_args\n",
    "            # )\n",
    "\n",
    "display(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train and evaluate models\n",
    "cv_results = {}\n",
    "results = []\n",
    "\n",
    "result_columns = [\n",
    "    \"model\",\n",
    "    \"mean_fit_time\",\n",
    "    \"mean_score_time\",\n",
    "    \"mean_train_score\",\n",
    "    \"mean_test_score\",\n",
    "    \"score\",\n",
    "]\n",
    "\n",
    "\n",
    "def fit_model(name: str, model:RegressorMixin, X: pd.DataFrame, y: pd.DataFrame):\n",
    "    print(f\"Fitting {name}...\")\n",
    "    model.fit(X, y.to_numpy().ravel())\n",
    "    result = model.cv_results_\n",
    "    cv_results[name] = result\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        **{col: result[col][model.best_index_] for col in result},\n",
    "        \"score\": scorer._score_func(X_test,y_test) if y_test is not None and scorer else mean_squared_error(y, model.predict(X)),\n",
    "        \"params\": str(model.best_params_),\n",
    "    }\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    results.append(fit_model(name, model, X, y))\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results = results[result_columns]\n",
    "results.to_csv(f\"{output_folder}/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    results,\n",
    "    results.sort_values(by=\"mean_test_score\", ascending=False),\n",
    "    results.sort_values(by=\"mean_train_score\", ascending=False),\n",
    "    results.sort_values(by=\"score\", ascending=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({name: model.best_params_ for name, model in models.items()}).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "predictions = pd.DataFrame(\n",
    "    {name: model.best_estimator_.predict(X).ravel() for name, model in models.items()}\n",
    ")\n",
    "predictions = pd.concat([y, predictions], axis=1)\n",
    "predictions.to_csv(f\"{output_folder}/predictions.csv\", index=False)\n",
    "predictions.describe().to_csv(f\"{output_folder}/predictions_stats.csv\", index=True)\n",
    "\n",
    "display(predictions, predictions.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Predictions\n",
    "fig = plot_data(predictions, y)\n",
    "fig.savefig(f\"{output_folder}/predictions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all models\n",
    "\n",
    "\n",
    "def plot_loss(model, ax1, ax2, label):\n",
    "    ax1.plot(model.loss_curve_, label=label)\n",
    "    ax1.set_xlabel(\"Iterations\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(model.loss_curve_, label=label)\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.set_xlabel(\"Iterations\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_[\"model\"], \"loss_curve_\"):\n",
    "        plot_loss(model.best_estimator_[\"model\"], ax1, ax2, label=model_name)\n",
    "\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"All models\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_xlabel(\"Iterations\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"All models\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{output_folder}/models/models.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model groups\n",
    "\n",
    "plot_groups = {}\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_[\"model\"], \"loss_curve_\"):\n",
    "        keys = model_name.split(\" \")[0].split(\"_\") + list(\n",
    "            enumerate(model_name.split(\" \")[-1:0:-1])\n",
    "        )\n",
    "        for key in keys:\n",
    "            if key not in plot_groups:\n",
    "                plot_groups[key] = plt.subplots(1, 2, figsize=(14, 6))\n",
    "            fig, (ax1, ax2) = plot_groups[key]\n",
    "            plot_loss(model.best_estimator_[\"model\"], ax1, ax2, model_name)\n",
    "\n",
    "for group, plot in plot_groups.items():\n",
    "    fig, (ax1, ax2) = plot\n",
    "    ax1.set_xlabel(\"Iterations\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(group)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.set_xlabel(\"Iterations\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.set_title(group)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{output_folder}/models/group_{group}.png\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual models\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_[\"model\"], \"loss_curve_\"):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        plot_loss(model.best_estimator_[\"model\"], ax1, ax2, model_name)\n",
    "        ax1.set_title(model_name)\n",
    "        ax2.set_title(model_name)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{output_folder}/models/{model_name}.png\")\n",
    "        fig.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
