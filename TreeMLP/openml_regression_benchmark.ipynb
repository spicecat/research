{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0355f62c",
   "metadata": {},
   "source": [
    "# OpenML-CTR23 Regression Benchmark\n",
    "\n",
    "This notebook benchmarks a variety of regression models on the `OpenML-CTR23 - A curated tabular regression benchmarking suite` benchmark suite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06c211b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q pip openml scikit-learn pandas nbformat setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e29ed75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import openml\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f26187da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def make_preprocessor(dataset: openml.OpenMLDataset) -> ColumnTransformer:\n",
    "    target = dataset.default_target_attribute\n",
    "    numeric_features = dataset.get_features_by_type(\"numeric\", exclude=[target])\n",
    "    nominal_features = dataset.get_features_by_type(\"nominal\", exclude=[target])\n",
    "\n",
    "    numeric_transformer = Pipeline(\n",
    "        [(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "    nominal_transformer = Pipeline(\n",
    "        [(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]\n",
    "    )\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        [\n",
    "            (\"numeric_preprocessor\", numeric_transformer, numeric_features),\n",
    "            (\"nominal_preprocessor\", nominal_transformer, nominal_features),\n",
    "        ],\n",
    "        sparse_threshold=0.0,  # Ensure dense output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30895dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Models\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Baseline models\n",
    "models = {\n",
    "    \"DummyRegressor\": DummyRegressor(strategy=\"mean\"),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(),\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"MLPRegressor\": MLPRegressor(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Optimization\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "\n",
    "search_params = {\n",
    "    \"cv\": 5,\n",
    "    \"n_jobs\": -1,\n",
    "    \"n_trials\": 20,\n",
    "    \"return_train_score\": False,\n",
    "    # \"scoring\": \"neg_mean_squared_error\",\n",
    "    \"timeout\": None,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "hpo_grid = {\n",
    "    \"RandomForestRegressor\": {\n",
    "        \"n_estimators\": [5, 10],\n",
    "        \"max_depth\": [2, 5],\n",
    "    },\n",
    "    \"MLPRegressor\": {\n",
    "        \"hidden_layer_sizes\": [(5,), (10,)],\n",
    "        \"learning_rate_init\": [1e-3, 1e-1],\n",
    "        \"max_iter\": [100, 200],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def make_hpo_model(\n",
    "    preprocessor: ColumnTransformer, model_name: str\n",
    ") -> RandomizedSearchCV | Pipeline:\n",
    "    pipe = Pipeline([(\"preprocessor\", preprocessor), (\"model\", models[model_name])])\n",
    "    return (\n",
    "        RandomizedSearchCV(\n",
    "            pipe,\n",
    "            {f\"model__{k}\": v for k, v in hpo_grid[model_name].items()},\n",
    "            cv=KFold(n_splits=4, shuffle=True),\n",
    "        )\n",
    "        if model_name in hpo_grid\n",
    "        else pipe\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4773c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def evaluate_model(task: openml.OpenMLTask, model_name: str, seed: int = 42):\n",
    "    dataset = task.get_dataset()\n",
    "    preprocessor = make_preprocessor(dataset)\n",
    "    model = make_hpo_model(preprocessor, model_name)\n",
    "    run = openml.runs.run_model_on_task(model, task, seed=seed, n_jobs=-1)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63771953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenML Benchmark Suite\n",
       "======================\n",
       "ID..............: 353\n",
       "Name............: OpenML-CTR23 - A curated tabular regression benchmarking suite\n",
       "Status..........: active\n",
       "Main Entity Type: task\n",
       "Study URL.......: https://www.openml.org/s/353\n",
       "# of Data.......: 35\n",
       "# of Tasks......: 35\n",
       "Creator.........: https://www.openml.org/u/30127\n",
       "Upload Time.....: 2023-05-31 16:39:49"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Benchmark Suite\n",
    "SUITE_ID = \"8f0ea660163b436bbd4abd49665c7b1d\"  # OpenML-CTR23 - A curated tabular regression benchmarking suite\n",
    "suite = openml.study.get_suite(SUITE_ID)\n",
    "display(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Models\n",
    "N_TASKS = 999\n",
    "task_results = {}\n",
    "\n",
    "for task_id in suite.tasks[:N_TASKS]:\n",
    "    model_results = {}\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    for model_name in models.keys():\n",
    "        print(f\"Running {model_name} on task {task.target_name} id {task_id}...\")\n",
    "        run = evaluate_model(task, model_name)\n",
    "        model_results[model_name] = run\n",
    "    task_results[task_id] = model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd01b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Results\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "results_df = pd.DataFrame(task_results)\n",
    "mse_df = results_df.map(lambda run: run.get_metric_fn(mean_squared_error).mean())\n",
    "r2_df = results_df.map(lambda run: run.get_metric_fn(r2_score).mean())\n",
    "rank_df = mse_df.rank(axis=0, method=\"min\", ascending=True)\n",
    "average_rank = rank_df.mean(axis=1)\n",
    "display(mse_df, r2_df, rank_df, average_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a6e811d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:openml.extensions.sklearn.extension:'Read more' not found in descriptions. Trying to trim till 'Parameters' if available in docstring.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenML Run\n",
       "==========\n",
       "Uploader Name..............: None\n",
       "Metric.....................: None\n",
       "Local Result - MAE (+- STD): 1.5850 +- 0.0716\n",
       "Local Runtime - ms (+- STD): 24.1282 +- 7.6310\n",
       "Run ID.....................: None\n",
       "Task ID....................: 361234\n",
       "Task Type..................: None\n",
       "Task URL...................: https://www.openml.org/t/361234\n",
       "Flow ID....................: None\n",
       "Flow Name..................: sklearn.pipeline.Pipeline(preprocessor=sklearn.compose._column_transformer.ColumnTransformer(numeric_preprocessor=sklearn.pipeline.Pipeline(imputer=sklearn.impute._base.SimpleImputer,scaler=sklearn.preprocessing._data.StandardScaler),nominal_preprocessor=sklearn.pipeline.Pipeline(onehot=sklearn.preprocessing._encoders.OneHotEncoder)),model=sklearn.linear_model._base.LinearRegression)\n",
       "Flow URL...................: None\n",
       "Setup ID...................: None\n",
       "Setup String...............: Python_3.13.3. Sklearn_1.6.1. NumPy_2.2.5. SciPy_1.15.2.\n",
       "Dataset ID.................: 44956\n",
       "Dataset URL................: https://www.openml.org/d/44956"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Single Model Evaluation\n",
    "task_id = suite.tasks[0]\n",
    "task = openml.tasks.get_task(task_id)\n",
    "dataset = task.get_dataset()\n",
    "preprocessor = make_preprocessor(dataset)\n",
    "\n",
    "model_name = \"LinearRegression\"\n",
    "model = make_hpo_model(preprocessor, model_name)\n",
    "run = openml.runs.run_model_on_task(model, task, n_jobs=-1)\n",
    "display(run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
