{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment 'research' is activated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "\n",
    "# List of required packages\n",
    "required_packages = ['ipywidgets', 'jupyter', 'matplotlib', 'numpy', 'pandas',\n",
    "                     'optuna', 'optuna-integration[sklearn]', 'scikit-learn']\n",
    "\n",
    "# Check if packages are installed, and install if not\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        pkg_resources.get_distribution(package)\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        install(package)\n",
    "\n",
    "# Check if the environment is activated\n",
    "if 'CONDA_DEFAULT_ENV' in os.environ:\n",
    "    print(f\"Environment '{os.environ['CONDA_DEFAULT_ENV']}' is activated.\")\n",
    "else:\n",
    "    print(\"No specific environment is activated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yNSyFZvf0Leo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the store sales dataset\n",
    "# excel_file = pd.ExcelFile('data/store_sales.xlsx')\n",
    "# sheet_names = excel_file.sheet_names\n",
    "\n",
    "# # Read the data\n",
    "# results_df = pd.read_excel(excel_file, sheet_name=sheet_names[2])  # 2, 9\n",
    "# iri_key_counts = results_df['IRI_KEY'].value_counts()\n",
    "# iri_keys = iri_key_counts[iri_key_counts > 300].index\n",
    "\n",
    "\n",
    "# features = ['F', 'D', 'Unit.Price']\n",
    "# target = 'Total.Volume'\n",
    "\n",
    "# results_df = results_df[results_df['IRI_KEY'] == iri_keys[0]]\n",
    "# X = results_df[features].values\n",
    "# y = results_df[target].values\n",
    "\n",
    "# sheet_names, iri_keys, X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the Boston dataset\n",
    "# data_url = 'http://lib.stat.cmu.edu/datasets/boston'\n",
    "# raw_df = pd.read_csv(data_url, sep='\\s+', skiprows=22,  # type: ignore\n",
    "#                      header=None)  # type: ignore\n",
    "# X = np.hstack([raw_df.values[::2, :-1], raw_df.values[1::2, :2]])\n",
    "# y = raw_df.values[1::2, 2].reshape(-1, 1).ravel()\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20640, 8),\n",
       " (20640,),\n",
       " ['MedInc',\n",
       "  'HouseAge',\n",
       "  'AveRooms',\n",
       "  'AveBedrms',\n",
       "  'Population',\n",
       "  'AveOccup',\n",
       "  'Latitude',\n",
       "  'Longitude'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the California housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data  # type: ignore\n",
    "y = housing.target  # type: ignore\n",
    "X.shape, y.shape, housing.feature_names  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the soybean dataset\n",
    "# raw_df = pd.read_excel(\"data/soybean.xlsx\")\n",
    "# # print(raw_df.head())\n",
    "# X = raw_df.values[:-1, [5, 6, 15, 16, 17, 26,\n",
    "#                         34, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]]  # 9 check yield 12 rm band\n",
    "# y = raw_df.values[:-1, 11]\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the data\n",
    "# data = {\n",
    "#     \"Size\": [850, 900, 1200, 1400, 1600, 1700, 1800, 2000, 2200, 2500],\n",
    "#     \"Bedrooms\": [2, 3, 3, 3, 3, 4, 4, 4, 5, 5],\n",
    "#     \"Price\": [300, 340, 400, 500, 520, 580, 600, 620, 720, 790]\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# X = df[[\"Size\", \"Bedrooms\"]].values\n",
    "# y = df[\"Price\"].values\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "\n",
    "X = X.astype(np.float64)\n",
    "y = y.astype(np.float64)\n",
    "\n",
    "# scaler_X = StandardScaler()\n",
    "# X = scaler_X.fit_transform(X)\n",
    "# scaler_y = StandardScaler()\n",
    "# y = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()  # type: ignore\n",
    "# scaler_X = QuantileTransformer()\n",
    "# X = scaler_X.fit_transform(X)\n",
    "# scaler_y = QuantileTransformer()\n",
    "# y = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chess/Developer/research/models_sklearn.py:350: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22,  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "from optuna.integration.sklearn import OptunaSearchCV\n",
    "from optuna import distributions\n",
    "from models_sklearn import Ensemble, MLP_sk, FONN1_sk, FONN2_sk, TREENN1_sk, TREENN2_sk\n",
    "from models import MLP, FONN1, FONN2, TREENN1, TREENN2\n",
    "\n",
    "search_params = {\n",
    "    'cv': 5,\n",
    "    'n_jobs': -1,\n",
    "    'n_trials': 10,\n",
    "    'random_state': 42,\n",
    "    'return_train_score': True,\n",
    "    'scoring': 'neg_root_mean_squared_error',\n",
    "    'timeout': 30,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "mlp_sk_param_grid = {\n",
    "    'learning_rate_init': distributions.FloatDistribution(1e-3, 1e-1, log=True),\n",
    "    'max_iter': distributions.CategoricalDistribution([200]),\n",
    "    # 'max_iter': distributions.IntDistribution(200, 1000, log=True),\n",
    "}\n",
    "mlp_param_grid = {\n",
    "    'learning_rate': distributions.FloatDistribution(1e-3, 1e-1, log=True),\n",
    "    'epochs': distributions.CategoricalDistribution([200]),\n",
    "    # 'epochs': distributions.IntDistribution(200, 1000, log=True),\n",
    "}\n",
    "\n",
    "\n",
    "def search(model, param_grid={}):\n",
    "    return OptunaSearchCV(model, param_grid, **search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101378/2991922431.py:30: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  return OptunaSearchCV(model, param_grid, **search_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Tree': OptunaSearchCV(cv=5, estimator=Ensemble(n_estimators=1), n_jobs=-1,\n",
       "                param_distributions={}, random_state=42, return_train_score=True,\n",
       "                scoring='neg_root_mean_squared_error', timeout=30, verbose=1),\n",
       " 'MLP_sk 10': OptunaSearchCV(cv=5, estimator=MLP_sk(hidden_layer_sizes=10), n_jobs=-1,\n",
       "                param_distributions={'learning_rate_init': FloatDistribution(high=0.1, log=True, low=0.001, step=None),\n",
       "                                     'max_iter': CategoricalDistribution(choices=(200,))},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_root_mean_squared_error', timeout=30, verbose=1),\n",
       " 'MLP_sk_tanh 10': OptunaSearchCV(cv=5, estimator=MLP_sk(activation='tanh', hidden_layer_sizes=10),\n",
       "                n_jobs=-1,\n",
       "                param_distributions={'learning_rate_init': FloatDistribution(high=0.1, log=True, low=0.001, step=None),\n",
       "                                     'max_iter': CategoricalDistribution(choices=(200,))},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_root_mean_squared_error', timeout=30, verbose=1),\n",
       " 'Ensemble_sk 10': OptunaSearchCV(cv=5, estimator=Ensemble(n_estimators=10), n_jobs=-1,\n",
       "                param_distributions={}, random_state=42, return_train_score=True,\n",
       "                scoring='neg_root_mean_squared_error', timeout=30, verbose=1),\n",
       " 'FONN1_sk 5 10': OptunaSearchCV(cv=5, estimator=FONN1_sk(hidden_layer_sizes=15, num_trees=5),\n",
       "                n_jobs=-1,\n",
       "                param_distributions={'learning_rate_init': FloatDistribution(high=0.1, log=True, low=0.001, step=None),\n",
       "                                     'max_iter': CategoricalDistribution(choices=(200,))},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_root_mean_squared_error', timeout=30, verbose=1),\n",
       " 'FONN2_sk 5 10': OptunaSearchCV(cv=5, estimator=FONN2_sk(hidden_layer_sizes=15), n_jobs=-1,\n",
       "                param_distributions={'learning_rate_init': FloatDistribution(high=0.1, log=True, low=0.001, step=None),\n",
       "                                     'max_iter': CategoricalDistribution(choices=(200,))},\n",
       "                random_state=42, return_train_score=True,\n",
       "                scoring='neg_root_mean_squared_error', timeout=30, verbose=1)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "num_trees_input = 5\n",
    "num_trees_hidden = 5\n",
    "hidden_nodes = [10]\n",
    "\n",
    "models['Tree'] = search(Ensemble(1))\n",
    "for hn in hidden_nodes:\n",
    "    models[f'MLP_sk {hn}'] = search(\n",
    "        MLP_sk(hn), mlp_sk_param_grid)\n",
    "    models[f'MLP_sk_tanh {hn}'] = search(\n",
    "        MLP_sk(hn, activation='tanh'), mlp_sk_param_grid)\n",
    "    if isinstance(hn, tuple):\n",
    "        models[f'Ensemble {sum(hn)}'] = search(Ensemble(sum(hn)))\n",
    "        models[f'FONN1_sk {num_trees_input} {hn}'] = search(\n",
    "            FONN1_sk(num_trees_input, (num_trees_input+hn[0], *hn[1:])), mlp_sk_param_grid)\n",
    "        models[f'FONN2_sk {num_trees_hidden} {hn}'] = search(\n",
    "            FONN2_sk(num_trees_hidden, (*hn[:-1], num_trees_hidden+hn[-1])), mlp_sk_param_grid)\n",
    "        models[f'TREENN1_sk {hn}'] = search(\n",
    "            TREENN1_sk((1+hn[0], *hn[1:])), mlp_sk_param_grid)\n",
    "        models[f'TREENN2_sk {hn}'] = search(\n",
    "            TREENN2_sk((*hn[:-1], 1+hn[-1])), mlp_sk_param_grid)\n",
    "    else:\n",
    "        models[f'Ensemble_sk {hn}'] = search(Ensemble(hn))\n",
    "        models[f'FONN1_sk {num_trees_input} {hn}'] = search(\n",
    "            FONN1_sk(num_trees_input, num_trees_input+hn), mlp_sk_param_grid)\n",
    "        models[f'FONN2_sk {num_trees_hidden} {hn}'] = search(\n",
    "            FONN2_sk(hn, num_trees_hidden+hn), mlp_sk_param_grid)\n",
    "        # models[f'TREENN1_sk {hn}'] = search(\n",
    "        #     TREENN1_sk(1+hn), mlp_sk_param_grid)\n",
    "        # models[f'TREENN2_sk {hn}'] = search(\n",
    "        #     TREENN2_sk(1+hn), mlp_sk_param_grid)\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = 1\n",
    "\n",
    "# for hn in hidden_nodes:\n",
    "#     models[f'MLP {hn}'] = search(\n",
    "#         MLP(input_dim, hn, output_dim), mlp_param_grid)\n",
    "#     models[f'MLP_tanh {hn}'] = search(\n",
    "#         MLP(input_dim, hn, output_dim, activation='tanh'), mlp_param_grid)\n",
    "#     models[f'FONN1 {num_trees_input} {hn}'] = search(\n",
    "#         FONN1(input_dim, hn, output_dim, num_trees_input), mlp_param_grid)\n",
    "#     models[f'FONN2 {num_trees_hidden} {hn}'] = search(\n",
    "#         FONN2(input_dim, hn, output_dim, num_trees_hidden), mlp_param_grid)\n",
    "#     models[f'TREENN1 {hn}'] = search(\n",
    "#         FONN1(input_dim, hn, output_dim, 1), mlp_param_grid)\n",
    "#     models[f'TREENN2 {hn}'] = search(\n",
    "#         FONN2(input_dim, hn, output_dim, 1), mlp_param_grid)\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 01:59:10,192] A new study created in memory with name: no-name-5ee021a6-2403-4e19-af1b-db865a5719a1\n",
      "[I 2024-11-14 01:59:10,198] A new study created in memory with name: no-name-fd0eb02b-99a9-47f1-ad2e-778859bb42c7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-14 01:59:10,199] A new study created in memory with name: no-name-8d9f25aa-bf29-4a0d-975e-ae6b19a9d940\n",
      "[I 2024-11-14 01:59:10,201] A new study created in memory with name: no-name-13ddb3c5-d23b-4611-b100-c97729f867ec\n",
      "[I 2024-11-14 01:59:10,206] A new study created in memory with name: no-name-f07b4370-c06f-48a2-8db8-e46f11d6f560\n",
      "[I 2024-11-14 01:59:10,206] A new study created in memory with name: no-name-4ed40937-7712-4041-a0a2-9b2fb2934ef6\n",
      "[I 2024-11-14 01:59:12,255] Trial 6 finished with value: -0.9401539868300061 and parameters: {}. Best is trial 6 with value: -0.9401539868300061.\n",
      "[I 2024-11-14 01:59:12,310] Trial 7 finished with value: -0.9205819172104915 and parameters: {}. Best is trial 7 with value: -0.9205819172104915.\n",
      "[I 2024-11-14 01:59:12,331] Trial 3 finished with value: -0.9238866637197287 and parameters: {}. Best is trial 7 with value: -0.9205819172104915.\n",
      "[I 2024-11-14 01:59:12,346] Trial 0 finished with value: -0.9452869825535697 and parameters: {}. Best is trial 7 with value: -0.9205819172104915.\n",
      "[I 2024-11-14 01:59:12,419] Trial 9 finished with value: -0.9520485424965998 and parameters: {}. Best is trial 7 with value: -0.9205819172104915.\n",
      "[I 2024-11-14 01:59:12,458] Trial 4 finished with value: -0.9343929669582918 and parameters: {}. Best is trial 7 with value: -0.9205819172104915.\n",
      "[I 2024-11-14 01:59:12,515] Trial 8 finished with value: -0.9315089409104162 and parameters: {}. Best is trial 7 with value: -0.9205819172104915.\n",
      "[I 2024-11-14 01:59:12,553] Trial 1 finished with value: -0.8892768327759413 and parameters: {}. Best is trial 1 with value: -0.8892768327759413.\n",
      "[I 2024-11-14 01:59:12,575] Trial 2 finished with value: -0.8988874223622453 and parameters: {}. Best is trial 1 with value: -0.8892768327759413.\n",
      "[I 2024-11-14 01:59:12,742] Trial 5 finished with value: -0.890707846329853 and parameters: {}. Best is trial 1 with value: -0.8892768327759413.\n",
      "[I 2024-11-14 01:59:22,798] Trial 0 finished with value: -0.6765147996778866 and parameters: {}. Best is trial 0 with value: -0.6765147996778866.\n",
      "[I 2024-11-14 01:59:23,130] Trial 6 finished with value: -0.6746514774258517 and parameters: {}. Best is trial 6 with value: -0.6746514774258517.\n",
      "[I 2024-11-14 01:59:23,185] Trial 1 finished with value: -0.6893977879387734 and parameters: {}. Best is trial 6 with value: -0.6746514774258517.\n",
      "[I 2024-11-14 01:59:23,751] Trial 9 finished with value: -0.6768182287863423 and parameters: {}. Best is trial 6 with value: -0.6746514774258517.\n",
      "[I 2024-11-14 01:59:23,770] Trial 7 finished with value: -0.6847058496699603 and parameters: {}. Best is trial 6 with value: -0.6746514774258517.\n",
      "[I 2024-11-14 01:59:23,893] Trial 2 finished with value: -0.6816310291706082 and parameters: {}. Best is trial 6 with value: -0.6746514774258517.\n",
      "[I 2024-11-14 01:59:23,927] Trial 8 finished with value: -0.6984579039429057 and parameters: {}. Best is trial 6 with value: -0.6746514774258517.\n",
      "[I 2024-11-14 01:59:23,936] Trial 5 finished with value: -0.6802713468433353 and parameters: {}. Best is trial 6 with value: -0.6746514774258517.\n",
      "[I 2024-11-14 01:59:24,019] Trial 3 finished with value: -0.6881382447993414 and parameters: {}. Best is trial 6 with value: -0.6746514774258517.\n",
      "[I 2024-11-14 01:59:24,041] Trial 4 finished with value: -0.6921772811158495 and parameters: {}. Best is trial 6 with value: -0.6746514774258517.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import sys\n",
    "from warnings import simplefilter\n",
    "if not sys.warnoptions:\n",
    "    simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# Train and evaluate models\n",
    "cv_results = {}\n",
    "results = []\n",
    "\n",
    "result_columns = [\n",
    "    'mean_fit_time', 'mean_score_time', 'mean_train_score', 'mean_test_score'\n",
    "]\n",
    "\n",
    "def fit_model(name, model, X, y):\n",
    "    model.fit(X, y)\n",
    "    result = model.cv_results_\n",
    "    return {\n",
    "        'model': name,\n",
    "        **{col: result[col][model.best_index_] for col in result}\n",
    "    }\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=None) as executor:\n",
    "    future_to_model = {executor.submit(fit_model, name, model, X, y): name for name, model in models.items()}\n",
    "    for future in as_completed(future_to_model):\n",
    "        name = future_to_model[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "            cv_results[name] = result\n",
    "        except Exception as e:\n",
    "            print(f\"Exception in {name}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('model', inplace=True)\n",
    "results_df = results_df[result_columns]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(name, model.best_params_) for name, model in models.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_, 'loss_curve_'):\n",
    "        ax1.plot(model.best_estimator_.loss_curve_, label=model_name)\n",
    "        ax2.plot(model.best_estimator_.loss_curve_, label=model_name)\n",
    "\n",
    "ax1.set_xlabel('Iterations')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('All models')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel('Iterations')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('All models')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_groups = {}\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_, 'loss_curve_'):\n",
    "        key = model_name.split('_' if '_' in model_name else ' ')[0]\n",
    "        if key not in plot_groups:\n",
    "            plot_groups[key] = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig, (ax1, ax2) = plot_groups[key]\n",
    "        ax1.plot(model.best_estimator_.loss_curve_, label=model_name)\n",
    "        ax2.plot(model.best_estimator_.loss_curve_, label=model_name)\n",
    "\n",
    "for group, plot in plot_groups.items():\n",
    "    fig, (ax1, ax2) = plot\n",
    "    ax1.set_xlabel('Iterations')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(group)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xlabel('Iterations')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title(group)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_loss(model, title='Loss Curve'):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    ax1.plot(model.loss_curve_)\n",
    "    ax1.set_xlabel('Iterations')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(title)\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(model.loss_curve_)\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xlabel('Iterations')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title(title)\n",
    "    ax2.grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_, 'loss_curve_'):\n",
    "        plot_loss(model.best_estimator_, model_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
