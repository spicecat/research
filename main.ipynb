{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yNSyFZvf0Leo"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import time\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, ARDRegression, SGDRegressor, PassiveAggressiveRegressor\n","from sklearn.svm import SVR\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor, StackingRegressor, VotingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from xgboost import XGBRegressor\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n","\n","from models import FONN1, FONN2, TREENN1, TREENN2"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["((506, 12), (506,))"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Load the Boston dataset\n","data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22,  # type: ignore\n","                     header=None)  # type: ignore\n","X = np.hstack([raw_df.values[::2, :-1], raw_df.values[1::2, :2]])\n","y = raw_df.values[1::2, 2]\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","X.shape, y.shape"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["((404, 12), (102, 12), (404,), (102,))"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42)\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Function to train and evaluate a model\n","def train_evaluate_model(model, X_train, X_test, y_train, y_test):\n","    start_time = time.time()\n","    model.fit(X_train, y_train)\n","    end_time = time.time()\n","    train_time = end_time - start_time\n","\n","    start_time = time.time()\n","    predictions = model.predict(X_test)\n","    end_time = time.time()\n","    comp_time = end_time - start_time\n","\n","    r2 = r2_score(y_test, predictions)\n","    mae = mean_absolute_error(y_test, predictions)\n","    mse = mean_squared_error(y_test, predictions)\n","\n","    return r2, mae, mse, train_time, comp_time\n","\n","\n","# Initialize models\n","models = {\n","    \"Linear Regression\": LinearRegression(),\n","    \"Ridge Regression\": Ridge(),\n","    \"Lasso Regression\": Lasso(),\n","    \"ElasticNet Regression\": ElasticNet(),\n","    \"Bayesian Ridge Regression\": BayesianRidge(),\n","    \"ARD Regression\": ARDRegression(),\n","    \"SGD Regressor\": SGDRegressor(),\n","    \"Passive Aggressive Regressor\": PassiveAggressiveRegressor(),\n","    \"Support Vector Regression\": SVR(),\n","    \"MLP Regressor\": MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n","    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n","    \"Gradient Boosting Regressor\": GradientBoostingRegressor(random_state=42),\n","    \"XGBoost Regressor\": XGBRegressor(random_state=42),\n","    \"AdaBoost Regressor\": AdaBoostRegressor(random_state=42),\n","    \"Bagging Regressor\": BaggingRegressor(random_state=42),\n","    \"ExtraTrees Regressor\": ExtraTreesRegressor(random_state=42),\n","    \"HistGradientBoosting Regressor\": HistGradientBoostingRegressor(random_state=42),\n","    \"Stacking Regressor\": StackingRegressor(estimators=[\n","        ('lr', LinearRegression()),\n","        ('rf', RandomForestRegressor(n_estimators=10, random_state=42))\n","    ], final_estimator=Ridge()),\n","    \"Voting Regressor\": VotingRegressor(estimators=[\n","        ('lr', LinearRegression()),\n","        ('rf', RandomForestRegressor(n_estimators=10, random_state=42)),\n","        ('gb', GradientBoostingRegressor(random_state=42))\n","    ])\n","}\n","\n","# Train and evaluate models\n","results = {}\n","for name, model in models.items():\n","    r2, mae, mse, fit_time, comp_time = train_evaluate_model(\n","        model, X_train, X_test, y_train, y_test)\n","    results[name] = {\"RÂ² Score\": r2, \"MAE\": mae, \"MSE\": mse,\n","                     \"Train Time (s)\": fit_time, \"Comp Time (s)\": comp_time}"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# # Initialize and train MLP\n","# input_dim = X_train.shape[1]\n","# hidden_dim = 10  # Increased hidden layer size\n","# output_dim = 1\n","# num_trees_input = 0\n","# epochs = 40000  # Increased number of epochs\n","# learning_rate = 0.0001  # Decreased learning rate\n","\n","# start_time = time.time()\n","# mlp = FONN1(input_dim, hidden_dim, output_dim, num_trees_input)\n","# mlp.train(X_train, y_train, epochs, learning_rate)\n","# end_time = time.time()\n","# mlp_train_time = end_time - start_time"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"operands could not be broadcast together with shapes (22,10) (12,10) (22,10) ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m fonn1 \u001b[38;5;241m=\u001b[39m FONN1(input_dim, hidden_dim, output_dim, num_trees_input)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mfonn1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     13\u001b[0m fonn1_train_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n","File \u001b[0;32m~/Developer/research/models.py:200\u001b[0m, in \u001b[0;36mFONN1.train\u001b[0;34m(self, X, y, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    199\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((output \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m~/Developer/research/models.py:187\u001b[0m, in \u001b[0;36mFONN1.backward\u001b[0;34m(self, X, y, output, learning_rate)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_output \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m d_weights_output\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_output \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m d_bias_output\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights_hidden\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md_weights_hidden\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_hidden \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m d_bias_hidden\n","\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (22,10) (12,10) (22,10) "]}],"source":["# Initialize and train FONN1\n","input_dim = X_train.shape[1]\n","hidden_dim = 10  # Increased hidden layer size\n","output_dim = 1\n","num_trees_input = 10\n","epochs = 40000  # Increased number of epochs\n","learning_rate = 0.0001  # Decreased learning rate\n","\n","start_time = time.time()\n","fonn1 = FONN1(input_dim, hidden_dim, output_dim, num_trees_input)\n","fonn1.train(X_train, y_train, epochs, learning_rate)\n","end_time = time.time()\n","fonn1_train_time = end_time - start_time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 571.1344741100389\n","Epoch 100, Loss: 7.101467013221472\n","Epoch 200, Loss: 7.100472119059717\n","Epoch 300, Loss: 7.099451389994043\n","Epoch 400, Loss: 7.098385955564067\n","Epoch 500, Loss: 7.097256095763151\n","Epoch 600, Loss: 7.096040986387926\n","Epoch 700, Loss: 7.094718483252138\n","Epoch 800, Loss: 7.093264957567157\n","Epoch 900, Loss: 7.091655199500762\n"]}],"source":["# Initialize and train FONN2\n","input_dim = X_train.shape[1]\n","hidden_dim = 10\n","output_dim = 1\n","num_trees_hidden = 10\n","epochs = 1000\n","learning_rate = 0.001\n","\n","start_time = time.time()\n","fonn2 = FONN2(input_dim, hidden_dim, output_dim, num_trees_hidden)\n","fonn2.train(X_train, y_train, epochs, learning_rate)\n","end_time = time.time()\n","fonn2_train_time = end_time - start_time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 606.8179580540033\n","Epoch 100, Loss: 604.5908502112225\n","Epoch 200, Loss: 600.9720240880093\n","Epoch 300, Loss: 596.2356149515359\n","Epoch 400, Loss: 590.9508026089832\n","Epoch 500, Loss: 585.4664913482309\n","Epoch 600, Loss: 579.9657306568773\n","Epoch 700, Loss: 574.5343455833752\n","Epoch 800, Loss: 569.2055455380231\n","Epoch 900, Loss: 563.985664398479\n","Epoch 1000, Loss: 558.8685206340114\n","Epoch 1100, Loss: 553.8431564395822\n","Epoch 1200, Loss: 548.90028508685\n","Epoch 1300, Loss: 544.034070681884\n","Epoch 1400, Loss: 539.237205277297\n","Epoch 1500, Loss: 534.5000352003989\n","Epoch 1600, Loss: 529.8084769818113\n","Epoch 1700, Loss: 525.1537677078577\n","Epoch 1800, Loss: 520.5314389902076\n","Epoch 1900, Loss: 515.9389474267089\n","Epoch 2000, Loss: 511.3747283569037\n","Epoch 2100, Loss: 506.8377614552665\n","Epoch 2200, Loss: 502.32735032489927\n","Epoch 2300, Loss: 497.84300185587097\n","Epoch 2400, Loss: 493.38435611925604\n","Epoch 2500, Loss: 488.95114360018\n","Epoch 2600, Loss: 484.5431580348803\n","Epoch 2700, Loss: 480.16023856020934\n","Epoch 2800, Loss: 475.80225763485157\n","Epoch 2900, Loss: 471.46911265586397\n","Epoch 3000, Loss: 467.16072000892683\n","Epoch 3100, Loss: 462.8770107617\n","Epoch 3200, Loss: 458.61792749118786\n","Epoch 3300, Loss: 454.38342190928216\n","Epoch 3400, Loss: 450.1734530601186\n","Epoch 3500, Loss: 445.98798593367815\n","Epoch 3600, Loss: 441.826990386819\n","Epoch 3700, Loss: 437.69044029440397\n","Epoch 3800, Loss: 433.5783128747509\n","Epoch 3900, Loss: 429.49058814863866\n","Epoch 4000, Loss: 425.4272485017004\n","Epoch 4100, Loss: 421.3882783276222\n","Epoch 4200, Loss: 417.3736637350619\n","Epoch 4300, Loss: 413.38339230523536\n","Epoch 4400, Loss: 409.4174528901041\n","Epoch 4500, Loss: 405.47583544333634\n","Epoch 4600, Loss: 401.55853087790155\n","Epoch 4700, Loss: 397.66553094544906\n","Epoch 4800, Loss: 393.79682813361114\n","Epoch 4900, Loss: 389.9524155781379\n","Epoch 5000, Loss: 386.13228698737146\n","Epoch 5100, Loss: 382.3364365770361\n","Epoch 5200, Loss: 378.5648590136957\n","Epoch 5300, Loss: 374.81754936552323\n","Epoch 5400, Loss: 371.09450305926623\n","Epoch 5500, Loss: 367.3957158424851\n","Epoch 5600, Loss: 363.7211837502897\n","Epoch 5700, Loss: 360.0709030759342\n","Epoch 5800, Loss: 356.44487034472496\n","Epoch 5900, Loss: 352.8430822907852\n","Epoch 6000, Loss: 349.2655358362908\n","Epoch 6100, Loss: 345.7122280728433\n","Epoch 6200, Loss: 342.183156244702\n","Epoch 6300, Loss: 338.678317733631\n","Epoch 6400, Loss: 335.1977100451537\n","Epoch 6500, Loss: 331.7413307960336\n","Epoch 6600, Loss: 328.3091777028242\n","Epoch 6700, Loss: 324.9012485713527\n","Epoch 6800, Loss: 321.51754128701623\n","Epoch 6900, Loss: 318.15805380578377\n","Epoch 7000, Loss: 314.82278414581185\n","Epoch 7100, Loss: 311.5117303795888\n","Epoch 7200, Loss: 308.2248906265327\n","Epoch 7300, Loss: 304.9622630459757\n","Epoch 7400, Loss: 301.7238458304706\n","Epoch 7500, Loss: 298.5096371993662\n","Epoch 7600, Loss: 295.31963539259425\n","Epoch 7700, Loss: 292.153838664622\n","Epoch 7800, Loss: 289.0122452785208\n","Epoch 7900, Loss: 285.89485350010585\n","Epoch 8000, Loss: 282.8016615921045\n","Epoch 8100, Loss: 279.7326678083068\n","Epoch 8200, Loss: 276.68787038765805\n","Epoch 8300, Loss: 273.6672675482465\n","Epoch 8400, Loss: 270.67085748114346\n","Epoch 8500, Loss: 267.698638344047\n","Epoch 8600, Loss: 264.7506082546829\n","Epoch 8700, Loss: 261.8267652839067\n","Epoch 8800, Loss: 258.92710744845624\n","Epoch 8900, Loss: 256.05163270328995\n","Epoch 9000, Loss: 253.20033893344768\n","Epoch 9100, Loss: 250.3732239453621\n","Epoch 9200, Loss: 247.57028545754022\n","Epoch 9300, Loss: 244.7915210905276\n","Epoch 9400, Loss: 242.0369283560572\n","Epoch 9500, Loss: 239.3065046452714\n","Epoch 9600, Loss: 236.600247215895\n","Epoch 9700, Loss: 233.9181531782164\n","Epoch 9800, Loss: 231.26021947972086\n","Epoch 9900, Loss: 228.62644288819146\n","Epoch 10000, Loss: 226.01681997307418\n","Epoch 10100, Loss: 223.4313470848676\n","Epoch 10200, Loss: 220.87002033226628\n","Epoch 10300, Loss: 218.3328355567438\n","Epoch 10400, Loss: 215.81978830421238\n","Epoch 10500, Loss: 213.33087379333836\n","Epoch 10600, Loss: 210.86608688002363\n","Epoch 10700, Loss: 208.42542201748228\n","Epoch 10800, Loss: 206.0088732112429\n","Epoch 10900, Loss: 203.61643396829018\n","Epoch 11000, Loss: 201.248097239421\n","Epoch 11100, Loss: 198.90385535371735\n","Epoch 11200, Loss: 196.58369994383693\n","Epoch 11300, Loss: 194.28762186056971\n","Epoch 11400, Loss: 192.01561107480765\n","Epoch 11500, Loss: 189.767656564698\n","Epoch 11600, Loss: 187.54374618529434\n","Epoch 11700, Loss: 185.3438665174474\n","Epoch 11800, Loss: 183.16800269197176\n","Epoch 11900, Loss: 181.01613818423743\n","Epoch 12000, Loss: 178.8882545732218\n","Epoch 12100, Loss: 176.78433125764775\n","Epoch 12200, Loss: 174.70434512003877\n","Epoch 12300, Loss: 172.64827012722245\n","Epoch 12400, Loss: 170.61607685284642\n","Epoch 12500, Loss: 168.60773190361402\n","Epoch 12600, Loss: 166.6231972258965\n","Epoch 12700, Loss: 164.6624292627096\n","Epoch 12800, Loss: 162.72537792216082\n","Epoch 12900, Loss: 160.81198530653518\n","Epoch 13000, Loss: 158.92218413498603\n","Epoch 13100, Loss: 157.05589577057597\n","Epoch 13200, Loss: 155.21302773159778\n","Epoch 13300, Loss: 153.39347052385884\n","Epoch 13400, Loss: 151.5970935691436\n","Epoch 13500, Loss: 149.82373991648487\n","Epoch 13600, Loss: 148.07321929332178\n","Epoch 13700, Loss: 146.3452988610984\n","Epoch 13800, Loss: 144.6396907487772\n","Epoch 13900, Loss: 142.95603498934253\n","Epoch 14000, Loss: 141.29387577947233\n","Epoch 14100, Loss: 139.6526278497619\n","Epoch 14200, Loss: 138.0315278684639\n","Epoch 14300, Loss: 136.4295626529572\n","Epoch 14400, Loss: 134.84536049646317\n","Epoch 14500, Loss: 133.27702214732133\n","Epoch 14600, Loss: 131.7218500005742\n","Epoch 14700, Loss: 130.17590010931917\n","Epoch 14800, Loss: 128.63321656443762\n","Epoch 14900, Loss: 127.08448588802815\n","Epoch 15000, Loss: 125.5152662122853\n","Epoch 15100, Loss: 123.91019282640754\n","Epoch 15200, Loss: 122.25489607190772\n","Epoch 15300, Loss: 120.53649495440416\n","Epoch 15400, Loss: 118.74543140498245\n","Epoch 15500, Loss: 116.89114121594757\n","Epoch 15600, Loss: 115.03265845652875\n","Epoch 15700, Loss: 113.3105210440305\n","Epoch 15800, Loss: 111.71763270041687\n","Epoch 15900, Loss: 110.1837324754174\n","Epoch 16000, Loss: 108.6948559421283\n","Epoch 16100, Loss: 107.24537548209872\n","Epoch 16200, Loss: 105.83114645453503\n","Epoch 16300, Loss: 104.44874679308414\n","Epoch 16400, Loss: 103.09533991270035\n","Epoch 16500, Loss: 101.76859141706277\n","Epoch 16600, Loss: 100.46658412075459\n","Epoch 16700, Loss: 99.18773678237619\n","Epoch 16800, Loss: 97.9307335336345\n","Epoch 16900, Loss: 96.6944666287093\n","Epoch 17000, Loss: 95.47799220498374\n","Epoch 17100, Loss: 94.28049737078649\n","Epoch 17200, Loss: 93.10127646037924\n","Epoch 17300, Loss: 91.939714274786\n","Epoch 17400, Loss: 90.79527433864146\n","Epoch 17500, Loss: 89.66749053557646\n","Epoch 17600, Loss: 88.55596085546178\n","Epoch 17700, Loss: 87.4603423286066\n","Epoch 17800, Loss: 86.38034649778314\n","Epoch 17900, Loss: 85.31573498718372\n","Epoch 18000, Loss: 84.26631488544412\n","Epoch 18100, Loss: 83.23193378482198\n","Epoch 18200, Loss: 82.21247442001705\n","Epoch 18300, Loss: 81.20784893123142\n","Epoch 18400, Loss: 80.21799283729626\n","Epoch 18500, Loss: 79.24285884659166\n","Epoch 18600, Loss: 78.28241065814701\n","Epoch 18700, Loss: 77.33661691561079\n","Epoch 18800, Loss: 76.40544547486653\n","Epoch 18900, Loss: 75.48885813298341\n","Epoch 19000, Loss: 74.58680594282063\n","Epoch 19100, Loss: 73.69922520580626\n","Epoch 19200, Loss: 72.82603419824495\n","Epoch 19300, Loss: 71.96713064773803\n","Epoch 19400, Loss: 71.12238993960685\n","Epoch 19500, Loss: 70.29166400162889\n","Epoch 19600, Loss: 69.47478079087401\n","Epoch 19700, Loss: 68.67154428953904\n","Epoch 19800, Loss: 67.88173490634132\n","Epoch 19900, Loss: 67.10511017347442\n","Epoch 20000, Loss: 66.34140562225228\n","Epoch 20100, Loss: 65.5903357087813\n","Epoch 20200, Loss: 64.85159464029725\n","Epoch 20300, Loss: 64.12485692077388\n","Epoch 20400, Loss: 63.40977739129836\n","Epoch 20500, Loss: 62.70599049124907\n","Epoch 20600, Loss: 62.01310842543879\n","Epoch 20700, Loss: 61.33071793016213\n","Epoch 20800, Loss: 60.65837548986293\n","Epoch 20900, Loss: 59.995601415501106\n","Epoch 21000, Loss: 59.341874727074696\n","Epoch 21100, Loss: 58.69663445535804\n","Epoch 21200, Loss: 58.05930060793981\n","Epoch 21300, Loss: 57.429340025702736\n","Epoch 21400, Loss: 56.80640761713608\n","Epoch 21500, Loss: 56.1905486455422\n","Epoch 21600, Loss: 55.58230464206408\n","Epoch 21700, Loss: 54.98247932640804\n","Epoch 21800, Loss: 54.391625962053\n","Epoch 21900, Loss: 53.80966780965345\n","Epoch 22000, Loss: 53.235915552152136\n","Epoch 22100, Loss: 52.66937171881467\n","Epoch 22200, Loss: 52.10893935933728\n","Epoch 22300, Loss: 51.553412368043396\n","Epoch 22400, Loss: 51.001487117926345\n","Epoch 22500, Loss: 50.452315510098735\n","Epoch 22600, Loss: 49.90735827185536\n","Epoch 22700, Loss: 49.37103795390359\n","Epoch 22800, Loss: 48.84445251119736\n","Epoch 22900, Loss: 48.324116150264054\n","Epoch 23000, Loss: 47.80735074303549\n","Epoch 23100, Loss: 47.29245572519433\n","Epoch 23200, Loss: 46.77814602124947\n","Epoch 23300, Loss: 46.264631498094275\n","Epoch 23400, Loss: 45.754903365899935\n","Epoch 23500, Loss: 45.250717938005714\n","Epoch 23600, Loss: 44.749403377949655\n","Epoch 23700, Loss: 44.25118194039453\n","Epoch 23800, Loss: 43.76062787822829\n","Epoch 23900, Loss: 43.27804169746525\n","Epoch 24000, Loss: 42.80395222148577\n","Epoch 24100, Loss: 42.34070339794362\n","Epoch 24200, Loss: 41.88578551143608\n","Epoch 24300, Loss: 41.43656184986062\n","Epoch 24400, Loss: 40.99214487672913\n","Epoch 24500, Loss: 40.552219181852934\n","Epoch 24600, Loss: 40.116619751673866\n","Epoch 24700, Loss: 39.685243732298886\n","Epoch 24800, Loss: 39.258022359751685\n","Epoch 24900, Loss: 38.83490800521826\n","Epoch 25000, Loss: 38.41586724328876\n","Epoch 25100, Loss: 38.00087675555586\n","Epoch 25200, Loss: 37.58992066227132\n","Epoch 25300, Loss: 37.182988615202554\n","Epoch 25400, Loss: 36.780074322527696\n","Epoch 25500, Loss: 36.38117433890669\n","Epoch 25600, Loss: 35.9862870354779\n","Epoch 25700, Loss: 35.595411706837666\n","Epoch 25800, Loss: 35.20854779408258\n","Epoch 25900, Loss: 34.82569421405862\n","Epoch 26000, Loss: 34.44684878995452\n","Epoch 26100, Loss: 34.07200778001929\n","Epoch 26200, Loss: 33.701165501047306\n","Epoch 26300, Loss: 33.33431404229837\n","Epoch 26400, Loss: 32.97144306426243\n","Epoch 26500, Loss: 32.612539675473755\n","Epoch 26600, Loss: 32.25758837960826\n","Epoch 26700, Loss: 31.906571084452743\n","Epoch 26800, Loss: 31.55946716403756\n","Epoch 26900, Loss: 31.21625356525908\n","Epoch 27000, Loss: 30.876904950637776\n","Epoch 27100, Loss: 30.541393869403628\n","Epoch 27200, Loss: 30.20969094980437\n","Epoch 27300, Loss: 29.88176510633051\n","Epoch 27400, Loss: 29.55758375638482\n","Epoch 27500, Loss: 29.23711304174491\n","Epoch 27600, Loss: 28.920318050939052\n","Epoch 27700, Loss: 28.607163039352294\n","Epoch 27800, Loss: 28.297611644487414\n","Epoch 27900, Loss: 27.99162709431943\n","Epoch 28000, Loss: 27.68917240710195\n","Epoch 28100, Loss: 27.39021058131789\n","Epoch 28200, Loss: 27.094704774721755\n","Epoch 28300, Loss: 26.802618471608607\n","Epoch 28400, Loss: 26.513915637575618\n","Epoch 28500, Loss: 26.22856086112792\n","Epoch 28600, Loss: 25.946519481531826\n","Epoch 28700, Loss: 25.66775770234809\n","Epoch 28800, Loss: 25.39224269009509\n","Epoch 28900, Loss: 25.119942657510762\n","Epoch 29000, Loss: 24.85082693091063\n","Epoch 29100, Loss: 24.584866001190278\n","Epoch 29200, Loss: 24.322031558100257\n","Epoch 29300, Loss: 24.062296507538832\n","Epoch 29400, Loss: 23.805634971764093\n","Epoch 29500, Loss: 23.55202246058073\n","Epoch 29600, Loss: 23.30157267986177\n","Epoch 29700, Loss: 23.054402829169465\n","Epoch 29800, Loss: 22.81153881443193\n","Epoch 29900, Loss: 22.574021291895857\n","Epoch 30000, Loss: 22.34180892396364\n","Epoch 30100, Loss: 22.114883738879044\n","Epoch 30200, Loss: 21.893082760987262\n","Epoch 30300, Loss: 21.676248970159858\n","Epoch 30400, Loss: 21.464233028344974\n","Epoch 30500, Loss: 21.256892725835062\n","Epoch 30600, Loss: 21.05409245849823\n","Epoch 30700, Loss: 20.855702736128983\n","Epoch 30800, Loss: 20.661599722090283\n","Epoch 30900, Loss: 20.471664804275385\n","Epoch 31000, Loss: 20.285784197212315\n","Epoch 31100, Loss: 20.103848574897548\n","Epoch 31200, Loss: 19.92575273369557\n","Epoch 31300, Loss: 19.751395284396775\n","Epoch 31400, Loss: 19.580678372296155\n","Epoch 31500, Loss: 19.413507423955632\n","Epoch 31600, Loss: 19.249790919145546\n","Epoch 31700, Loss: 19.089440186333576\n","Epoch 31800, Loss: 18.93236922000235\n","Epoch 31900, Loss: 18.77849451802881\n","Epoch 32000, Loss: 18.627734937348595\n","Epoch 32100, Loss: 18.48001156615074\n","Epoch 32200, Loss: 18.33524761089951\n","Epoch 32300, Loss: 18.193368296554272\n","Epoch 32400, Loss: 18.054300778450852\n","Epoch 32500, Loss: 17.917974064413\n","Epoch 32600, Loss: 17.784318945776146\n","Epoch 32700, Loss: 17.65326793612327\n","Epoch 32800, Loss: 17.524755216650515\n","Epoch 32900, Loss: 17.398716587196226\n","Epoch 33000, Loss: 17.275089422077787\n","Epoch 33100, Loss: 17.15381262998513\n","Epoch 33200, Loss: 17.034826617277254\n","Epoch 33300, Loss: 16.918073254115928\n","Epoch 33400, Loss: 16.8034958429519\n","Epoch 33500, Loss: 16.691039088949548\n","Epoch 33600, Loss: 16.580649071999655\n","Epoch 33700, Loss: 16.472273220024995\n","Epoch 33800, Loss: 16.365860283331305\n","Epoch 33900, Loss: 16.26136030979771\n","Epoch 34000, Loss: 16.158724620735445\n","Epoch 34100, Loss: 16.05790578727388\n","Epoch 34200, Loss: 15.958857607158027\n","Epoch 34300, Loss: 15.861535081862744\n","Epoch 34400, Loss: 15.76589439394691\n","Epoch 34500, Loss: 15.671892884585512\n","Epoch 34600, Loss: 15.57948903123043\n","Epoch 34700, Loss: 15.488642425360878\n","Epoch 34800, Loss: 15.399313750293828\n","Epoch 34900, Loss: 15.311464759032317\n","Epoch 35000, Loss: 15.22505825213587\n","Epoch 35100, Loss: 15.14005805560347\n","Epoch 35200, Loss: 15.056428998764277\n","Epoch 35300, Loss: 14.974136892175858\n","Epoch 35400, Loss: 14.893148505533997\n","Epoch 35500, Loss: 14.813431545601267\n","Epoch 35600, Loss: 14.73495463416535\n","Epoch 35700, Loss: 14.657687286040662\n","Epoch 35800, Loss: 14.581599887129515\n","Epoch 35900, Loss: 14.506663672561613\n","Epoch 36000, Loss: 14.432850704932047\n","Epoch 36100, Loss: 14.36013385266035\n","Epoch 36200, Loss: 14.28848676849413\n","Epoch 36300, Loss: 14.217883868181596\n","Epoch 36400, Loss: 14.148300309338653\n","Epoch 36500, Loss: 14.079711970536035\n","Epoch 36600, Loss: 14.012095430632455\n","Epoch 36700, Loss: 13.945427948379212\n","Epoch 36800, Loss: 13.879687442321497\n","Epoch 36900, Loss: 13.814852471020705\n","Epoch 37000, Loss: 13.750902213621128\n","Epoch 37100, Loss: 13.687816450783197\n","Epoch 37200, Loss: 13.625575546003986\n","Epoch 37300, Loss: 13.564160427344046\n","Epoch 37400, Loss: 13.50355256957819\n","Epoch 37500, Loss: 13.44373397678549\n","Epoch 37600, Loss: 13.384687165392467\n","Epoch 37700, Loss: 13.326395147680831\n","Epoch 37800, Loss: 13.268841415769604\n","Epoch 37900, Loss: 13.2120099260792\n","Epoch 38000, Loss: 13.155885084282964\n","Epoch 38100, Loss: 13.100451730749993\n","Epoch 38200, Loss: 13.045695126480673\n","Epoch 38300, Loss: 12.991600939535108\n","Epoch 38400, Loss: 12.938155231952138\n","Epoch 38500, Loss: 12.885344447155763\n","Epoch 38600, Loss: 12.8331553978434\n","Epoch 38700, Loss: 12.781575254349585\n","Epoch 38800, Loss: 12.730591533476971\n","Epoch 38900, Loss: 12.680192087785361\n","Epoch 39000, Loss: 12.630365095328385\n","Epoch 39100, Loss: 12.581099049826406\n","Epoch 39200, Loss: 12.532382751263205\n","Epoch 39300, Loss: 12.484205296893384\n","Epoch 39400, Loss: 12.436556072646498\n","Epoch 39500, Loss: 12.38942474491357\n","Epoch 39600, Loss: 12.342801252700795\n","Epoch 39700, Loss: 12.29667580013513\n","Epoch 39800, Loss: 12.251038849306044\n","Epoch 39900, Loss: 12.205881113427166\n"]}],"source":["# Initialize and train TREENN1\n","input_dim = X_train.shape[1]\n","hidden_dim = 10  # Hidden layer size\n","output_dim = 1\n","epochs = 40000  # Number of epochs\n","learning_rate = 0.0001  # Learning rate\n","\n","start_time = time.time()\n","treenn1 = TREENN1(input_dim, hidden_dim, output_dim)\n","treenn1.train(X_train, y_train, epochs, learning_rate)\n","end_time = time.time()\n","treenn1_train_time = end_time - start_time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 611.125623691866\n","Epoch 100, Loss: 491.5937288530418\n","Epoch 200, Loss: 377.85587003634635\n","Epoch 300, Loss: 272.1391759341896\n","Epoch 400, Loss: 181.1179967962341\n","Epoch 500, Loss: 107.19329599451923\n","Epoch 600, Loss: 52.130442861603726\n","Epoch 700, Loss: 18.244999695272178\n","Epoch 800, Loss: 7.494098217896716\n","Epoch 900, Loss: 7.2982836711762475\n"]}],"source":["# Initialize and train TREENN2\n","input_dim = X_train.shape[1]\n","hidden_dim = 10\n","output_dim = 1\n","epochs = 1000\n","learning_rate = 0.001\n","\n","start_time = time.time()\n","treenn2 = TREENN2(input_dim, hidden_dim, output_dim)\n","treenn2.train(X_train, y_train, epochs, learning_rate)\n","end_time = time.time()\n","treenn2_train_time = end_time - start_time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"shapes (102,12) and (22,10) not aligned: 12 (dim 1) != 22 (dim 0)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Measure computational time and evaluate the FONN1 model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m fonn1_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mfonn1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m fonn1_comp_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n","File \u001b[0;32m~/Developer/research/models.py:150\u001b[0m, in \u001b[0;36mFONN1.forward\u001b[0;34m(self, combined_input)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, combined_input):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Compute hidden layer activations\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombined_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights_hidden\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_hidden\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_hidden \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_hidden)  \u001b[38;5;66;03m# Tanh activation\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# Compute output layer activations\u001b[39;00m\n","File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: shapes (102,12) and (22,10) not aligned: 12 (dim 1) != 22 (dim 0)"]}],"source":["# Measure computational time and evaluate the FONN1 model\n","start_time = time.time()\n","fonn1_predictions = fonn1.forward(X_test)\n","end_time = time.time()\n","fonn1_comp_time = end_time - start_time\n","\n","fonn1_r2 = r2_score(y_test, fonn1_predictions)\n","fonn1_mae = mean_absolute_error(y_test, fonn1_predictions)\n","fonn1_mse = mean_squared_error(y_test, fonn1_predictions)\n","\n","results[\"FONN1\"] = {\"RÂ² Score\": fonn1_r2, \"MAE\": fonn1_mae, \"MSE\": fonn1_mse,\n","                    \"Train Time (s)\": fonn1_train_time, \"Comp Time (s)\": fonn1_comp_time}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Measure computational time and evaluate the custom MLP model\n","start_time = time.time()\n","fonn2_predictions = fonn2.forward(X_test)\n","end_time = time.time()\n","fonn2_comp_time = end_time - start_time\n","\n","fonn2_r2 = r2_score(y_test, fonn2_predictions)\n","fonn2_mae = mean_absolute_error(y_test, fonn2_predictions)\n","fonn2_mse = mean_squared_error(y_test, fonn2_predictions)\n","\n","results[\"FONN2\"] = {\"RÂ² Score\": fonn2_r2, \"MAE\": fonn2_mae, \"MSE\": fonn2_mse,\n","                    \"Train Time (s)\": fonn2_train_time, \"Comp Time (s)\": fonn2_comp_time}\n","\n","# Measure computational time and predict house prices using the decision trees in the hidden layer\n","start_time = time.time()\n","fonn2_tree_predictions = fonn2.tree_predict(X_test)\n","end_time = time.time()\n","fonn2_tree_comp_time = end_time - start_time\n","\n","fonn2_tree_r2 = r2_score(y_test, fonn2_tree_predictions)\n","fonn2_tree_mae = mean_absolute_error(y_test, fonn2_tree_predictions)\n","fonn2_tree_mse = mean_squared_error(y_test, fonn2_tree_predictions)\n","\n","results[\"Tree-based Predictions (FONN2)\"] = {\"RÂ² Score\": fonn2_tree_r2, \"MAE\": fonn2_tree_mae,\n","                                             \"MSE\": fonn2_tree_mse, \"Train Time (s)\": fonn2_train_time, \"Comp Time (s)\": fonn2_tree_comp_time}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine 10 decision trees and evaluate the ensemble model\n","start_time = time.time()\n","trees = [DecisionTreeRegressor(max_depth=5, random_state=i).fit(\n","    X_train, y_train) for i in range(10)]\n","end_time = time.time()\n","ensemble_train_time = end_time - start_time\n","\n","start_time = time.time()\n","ensemble_predictions = np.mean(\n","    [tree.predict(X_test) for tree in trees], axis=0)\n","end_time = time.time()\n","ensemble_comp_time = end_time - start_time\n","\n","ensemble_r2 = r2_score(y_test, ensemble_predictions)\n","ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)\n","ensemble_mse = mean_squared_error(y_test, ensemble_predictions)\n","\n","results[\"Ensemble of 10 Trees\"] = {\"RÂ² Score\": ensemble_r2, \"MAE\": ensemble_mae,\n","                                   \"MSE\": ensemble_mse, \"Train Time (s)\": ensemble_train_time, \"Comp Time (s)\": ensemble_comp_time}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Measure computational time and evaluate the TREENN1 model\n","start_time = time.time()\n","treenn1_predictions = treenn1.forward(X_test)\n","end_time = time.time()\n","treenn1_comp_time = end_time - start_time\n","\n","treenn1_r2 = r2_score(y_test, treenn1_predictions)\n","treenn1_mae = mean_absolute_error(y_test, treenn1_predictions)\n","treenn1_mse = mean_squared_error(y_test, treenn1_predictions)\n","\n","results[\"TREENN1\"] = {\"RÂ² Score\": treenn1_r2, \"MAE\": treenn1_mae, \"MSE\": treenn1_mse,\n","                      \"Train Time (s)\": treenn1_train_time, \"Comp Time (s)\": treenn1_comp_time}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Measure computational time and evaluate the custom MLP model\n","start_time = time.time()\n","treenn2_predictions = treenn2.forward(X_test)\n","end_time = time.time()\n","treenn2_comp_time = end_time - start_time\n","\n","treenn2_r2 = r2_score(y_test, treenn2_predictions)\n","treenn2_mae = mean_absolute_error(y_test, treenn2_predictions)\n","treenn2_mse = mean_squared_error(y_test, treenn2_predictions)\n","\n","results[\"TREENN2\"] = {\"RÂ² Score\": treenn2_r2, \"MAE\": treenn2_mae,\n","                      \"MSE\": treenn2_mse, \"Train Time (s)\": treenn2_train_time, \"Comp Time (s)\": treenn2_comp_time}\n","\n","# Measure computational time and predict house prices using the decision tree in the hidden layer\n","start_time = time.time()\n","treenn2_tree_predictions = treenn2.tree_hidden.predict(treenn2.a1)\n","end_time = time.time()\n","treenn2_tree_comp_time = end_time - start_time\n","\n","treenn2_tree_r2 = r2_score(y_test, treenn2_tree_predictions)\n","treenn2_tree_mae = mean_absolute_error(y_test, treenn2_tree_predictions)\n","treenn2_tree_mse = mean_squared_error(y_test, treenn2_tree_predictions)\n","\n","results[\"Tree-based Predictions (TREENN2)\"] = {\"RÂ² Score\": treenn2_tree_r2, \"MAE\": treenn2_tree_mae,\n","                                               \"MSE\": treenn2_tree_mse, \"Train Time (s)\": treenn2_train_time, \"Comp Time (s)\": treenn2_tree_comp_time}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert results to a DataFrame for better visualization\n","results_df = pd.DataFrame(results).T\n","print(results_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get and print tree importances\n","tree_importances = fonn2.get_tree_importances()"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
