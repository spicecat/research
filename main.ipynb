{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yNSyFZvf0Leo"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import time\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, ARDRegression, SGDRegressor, PassiveAggressiveRegressor\n","from sklearn.svm import SVR\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor, StackingRegressor, VotingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from xgboost import XGBRegressor\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n","\n","from models import FONN1, FONN2, TREENN1, TREENN2"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["((506, 12), (506,))"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Load the Boston dataset\n","data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22,  # type: ignore\n","                     header=None)  # type: ignore\n","X = np.hstack([raw_df.values[::2, :-1], raw_df.values[1::2, :2]])\n","y = raw_df.values[1::2, 2]\n","\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","X.shape, y.shape"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["((404, 12), (102, 12), (404,), (102,))"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42)\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Function to train and evaluate a model\n","def train_evaluate_model(model, X_train, X_test, y_train, y_test):\n","    start_time = time.time()\n","    model.fit(X_train, y_train)\n","    end_time = time.time()\n","    train_time = end_time - start_time\n","\n","    start_time = time.time()\n","    predictions = model.predict(X_test)\n","    end_time = time.time()\n","    comp_time = end_time - start_time\n","\n","    r2 = r2_score(y_test, predictions)\n","    mae = mean_absolute_error(y_test, predictions)\n","    mse = mean_squared_error(y_test, predictions)\n","\n","    return r2, mae, mse, train_time, comp_time\n","\n","\n","# Initialize models\n","models = {\n","    \"Linear Regression\": LinearRegression(),\n","    \"Ridge Regression\": Ridge(),\n","    \"Lasso Regression\": Lasso(),\n","    \"ElasticNet Regression\": ElasticNet(),\n","    \"Bayesian Ridge Regression\": BayesianRidge(),\n","    \"ARD Regression\": ARDRegression(),\n","    \"SGD Regressor\": SGDRegressor(),\n","    \"Passive Aggressive Regressor\": PassiveAggressiveRegressor(),\n","    \"Support Vector Regression\": SVR(),\n","    \"MLP Regressor\": MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n","    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n","    \"Gradient Boosting Regressor\": GradientBoostingRegressor(random_state=42),\n","    \"XGBoost Regressor\": XGBRegressor(random_state=42),\n","    \"AdaBoost Regressor\": AdaBoostRegressor(random_state=42),\n","    \"Bagging Regressor\": BaggingRegressor(random_state=42),\n","    \"ExtraTrees Regressor\": ExtraTreesRegressor(random_state=42),\n","    \"HistGradientBoosting Regressor\": HistGradientBoostingRegressor(random_state=42),\n","    \"Stacking Regressor\": StackingRegressor(estimators=[\n","        ('lr', LinearRegression()),\n","        ('rf', RandomForestRegressor(n_estimators=10, random_state=42))\n","    ], final_estimator=Ridge()),\n","    \"Voting Regressor\": VotingRegressor(estimators=[\n","        ('lr', LinearRegression()),\n","        ('rf', RandomForestRegressor(n_estimators=10, random_state=42)),\n","        ('gb', GradientBoostingRegressor(random_state=42))\n","    ])\n","}\n","\n","# Train and evaluate models\n","results = {}\n","for name, model in models.items():\n","    r2, mae, mse, fit_time, comp_time = train_evaluate_model(\n","        model, X_train, X_test, y_train, y_test)\n","    results[name] = {\"R² Score\": r2, \"MAE\": mae, \"MSE\": mse,\n","                     \"Train Time (s)\": fit_time, \"Comp Time (s)\": comp_time}"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# # Initialize and train MLP\n","# input_dim = X_train.shape[1]\n","# hidden_dim = 10  # Increased hidden layer size\n","# output_dim = 1\n","# num_trees_input = 0\n","# epochs = 40000  # Increased number of epochs\n","# learning_rate = 0.0001  # Decreased learning rate\n","\n","# start_time = time.time()\n","# mlp = FONN1(input_dim, hidden_dim, output_dim, num_trees_input)\n","# mlp.train(X_train, y_train, epochs, learning_rate)\n","# end_time = time.time()\n","# mlp_train_time = end_time - start_time"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 606.5360967734182\n","Epoch 100, Loss: 599.2096840195194\n","Epoch 200, Loss: 594.0552385721818\n","Epoch 300, Loss: 589.0655737916654\n","Epoch 400, Loss: 584.1237543426554\n","Epoch 500, Loss: 579.2150262345203\n","Epoch 600, Loss: 574.3349791755749\n","Epoch 700, Loss: 569.4817738100883\n","Epoch 800, Loss: 564.6544869575226\n","Epoch 900, Loss: 559.8525962484895\n","Epoch 1000, Loss: 555.0757800738841\n","Epoch 1100, Loss: 550.3238276073417\n","Epoch 1200, Loss: 545.5965937836214\n","Epoch 1300, Loss: 540.8939748668694\n","Epoch 1400, Loss: 536.2158943155102\n","Epoch 1500, Loss: 531.5622941686872\n","Epoch 1600, Loss: 526.9331295670908\n","Epoch 1700, Loss: 522.3283651393835\n","Epoch 1800, Loss: 517.7479725444251\n","Epoch 1900, Loss: 513.1919287545691\n","Epoch 2000, Loss: 508.6602148284715\n","Epoch 2100, Loss: 504.15281501578283\n","Epoch 2200, Loss: 499.66971609207565\n","Epoch 2300, Loss: 495.2109068567978\n","Epoch 2400, Loss: 490.7763777487826\n","Epoch 2500, Loss: 486.3661205479375\n","Epoch 2600, Loss: 481.9801281410508\n","Epoch 2700, Loss: 477.6183943359545\n","Epoch 2800, Loss: 473.2809137126063\n","Epoch 2900, Loss: 468.96768150267684\n","Epoch 3000, Loss: 464.6786934913777\n","Epoch 3100, Loss: 460.41394593680695\n","Epoch 3200, Loss: 456.1734355032149\n","Epoch 3300, Loss: 451.95715920542125\n","Epoch 3400, Loss: 447.76511436223706\n","Epoch 3500, Loss: 443.5972985572039\n","Epoch 3600, Loss: 439.4537096053254\n","Epoch 3700, Loss: 435.3343455247301\n","Epoch 3800, Loss: 431.2392045124202\n","Epoch 3900, Loss: 427.1682849234229\n","Epoch 4000, Loss: 423.1215852527881\n","Epoch 4100, Loss: 419.09910411998214\n","Epoch 4200, Loss: 415.1008402553036\n","Epoch 4300, Loss: 411.1267924880153\n","Epoch 4400, Loss: 407.176959735939\n","Epoch 4500, Loss: 403.2513409962977\n","Epoch 4600, Loss: 399.34993533763145\n","Epoch 4700, Loss: 395.4727418926348\n","Epoch 4800, Loss: 391.6197598517902\n","Epoch 4900, Loss: 387.79098845769073\n","Epoch 5000, Loss: 383.9864269999609\n","Epoch 5100, Loss: 380.2060748106962\n","Epoch 5200, Loss: 376.4499312603571\n","Epoch 5300, Loss: 372.71799575405663\n","Epoch 5400, Loss: 369.01026772819426\n","Epoch 5500, Loss: 365.32674664739085\n","Epoch 5600, Loss: 361.66743200168753\n","Epoch 5700, Loss: 358.032323303976\n","Epoch 5800, Loss: 354.4214200876288\n","Epoch 5900, Loss: 350.83472190430655\n","Epoch 6000, Loss: 347.2722283219162\n","Epoch 6100, Loss: 343.7339389227019\n","Epoch 6200, Loss: 340.2198533014483\n","Epoch 6300, Loss: 336.7299710637803\n","Epoch 6400, Loss: 333.264291824542\n","Epoch 6500, Loss: 329.8228152062434\n","Epoch 6600, Loss: 326.4055408375571\n","Epoch 6700, Loss: 323.0124683518564\n","Epoch 6800, Loss: 319.643597385779\n","Epoch 6900, Loss: 316.2989275778062\n","Epoch 7000, Loss: 312.97845856684506\n","Epoch 7100, Loss: 309.6821899908017\n","Epoch 7200, Loss: 306.41012148513306\n","Epoch 7300, Loss: 303.1622526813652\n","Epoch 7400, Loss: 299.9385832055651\n","Epoch 7500, Loss: 296.7391126767509\n","Epoch 7600, Loss: 293.56384070522705\n","Epoch 7700, Loss: 290.4127668908273\n","Epoch 7800, Loss: 287.2858908210477\n","Epoch 7900, Loss: 284.1832120690508\n","Epoch 8000, Loss: 281.1047301915177\n","Epoch 8100, Loss: 278.0504447263253\n","Epoch 8200, Loss: 275.02035519001913\n","Epoch 8300, Loss: 272.01446107505177\n","Epoch 8400, Loss: 269.03276184675013\n","Epoch 8500, Loss: 266.0752569399699\n","Epoch 8600, Loss: 263.14194575539125\n","Epoch 8700, Loss: 260.23282765539875\n","Epoch 8800, Loss: 257.3479019594833\n","Epoch 8900, Loss: 254.4871679390897\n","Epoch 9000, Loss: 251.65062481182434\n","Epoch 9100, Loss: 248.83827173491832\n","Epoch 9200, Loss: 246.05010779782597\n","Epoch 9300, Loss: 243.28613201381384\n","Epoch 9400, Loss: 240.54634331036937\n","Epoch 9500, Loss: 237.83074051822413\n","Epoch 9600, Loss: 235.1393223587461\n","Epoch 9700, Loss: 232.47208742940393\n","Epoch 9800, Loss: 229.82903418694502\n","Epoch 9900, Loss: 227.21016092784873\n","Epoch 10000, Loss: 224.6154657655225\n","Epoch 10100, Loss: 222.0449466035802\n","Epoch 10200, Loss: 219.49860110439218\n","Epoch 10300, Loss: 216.9764266518932\n","Epoch 10400, Loss: 214.47842030738386\n","Epoch 10500, Loss: 212.00457875672802\n","Epoch 10600, Loss: 209.5548982469255\n","Epoch 10700, Loss: 207.12937450947078\n","Epoch 10800, Loss: 204.7280026671701\n","Epoch 10900, Loss: 202.35077712009\n","Epoch 11000, Loss: 199.99769140497182\n","Epoch 11100, Loss: 197.66873802061727\n","Epoch 11200, Loss: 195.3639082092289\n","Epoch 11300, Loss: 193.08319168017653\n","Epoch 11400, Loss: 190.82657625769264\n","Epoch 11500, Loss: 188.59404742688304\n","Epoch 11600, Loss: 186.3855877420903\n","Epoch 11700, Loss: 184.20117604633575\n","Epoch 11800, Loss: 182.04078642751708\n","Epoch 11900, Loss: 179.90438680162802\n","Epoch 12000, Loss: 177.79193695767955\n","Epoch 12100, Loss: 175.7033858095713\n","Epoch 12200, Loss: 173.6386674523464\n","Epoch 12300, Loss: 171.5976953683316\n","Epoch 12400, Loss: 169.58035368418237\n","Epoch 12500, Loss: 167.5864835639011\n","Epoch 12600, Loss: 165.6158612548299\n","Epoch 12700, Loss: 163.6681611240897\n","Epoch 12800, Loss: 161.742890152022\n","Epoch 12900, Loss: 159.83926431403333\n","Epoch 13000, Loss: 157.95595613786594\n","Epoch 13100, Loss: 156.09052373604382\n","Epoch 13200, Loss: 154.23793015641493\n","Epoch 13300, Loss: 152.38591552578768\n","Epoch 13400, Loss: 150.49737076180898\n","Epoch 13500, Loss: 148.51212442418844\n","Epoch 13600, Loss: 146.58101103582453\n","Epoch 13700, Loss: 144.69036082052307\n","Epoch 13800, Loss: 142.84368111988803\n","Epoch 13900, Loss: 141.03009946078677\n","Epoch 14000, Loss: 139.23907275803353\n","Epoch 14100, Loss: 137.4603008308539\n","Epoch 14200, Loss: 135.69114308524382\n","Epoch 14300, Loss: 133.9332542504155\n","Epoch 14400, Loss: 132.18919935351224\n","Epoch 14500, Loss: 130.46187810804776\n","Epoch 14600, Loss: 128.7541104228807\n","Epoch 14700, Loss: 127.06829798492065\n","Epoch 14800, Loss: 125.40622959640018\n","Epoch 14900, Loss: 123.76903760114494\n","Epoch 15000, Loss: 122.15726470455508\n","Epoch 15100, Loss: 120.57098657424831\n","Epoch 15200, Loss: 119.00994571253754\n","Epoch 15300, Loss: 117.47367016275312\n","Epoch 15400, Loss: 115.9615663488598\n","Epoch 15500, Loss: 114.47298534736606\n","Epoch 15600, Loss: 113.00726670526973\n","Epoch 15700, Loss: 111.56376533476677\n","Epoch 15800, Loss: 110.14186666293496\n","Epoch 15900, Loss: 108.74099416732277\n","Epoch 16000, Loss: 107.36061226842078\n","Epoch 16100, Loss: 106.00022654060437\n","Epoch 16200, Loss: 104.6593824340926\n","Epoch 16300, Loss: 103.33766317626063\n","Epoch 16400, Loss: 102.03468720536726\n","Epoch 16500, Loss: 100.75010533000987\n","Epoch 16600, Loss: 99.48359774693341\n","Epoch 16700, Loss: 98.23487103911664\n","Epoch 16800, Loss: 97.00365527964607\n","Epoch 16900, Loss: 95.7897013636247\n","Epoch 17000, Loss: 94.59277867132893\n","Epoch 17100, Loss: 93.41267313063328\n","Epoch 17200, Loss: 92.24918570007016\n","Epoch 17300, Loss: 91.10213124293283\n","Epoch 17400, Loss: 89.97133771519682\n","Epoch 17500, Loss: 88.85664555236903\n","Epoch 17600, Loss: 87.7579071172759\n","Epoch 17700, Loss: 86.67498606433301\n","Epoch 17800, Loss: 85.60775648556566\n","Epoch 17900, Loss: 84.55610172711008\n","Epoch 18000, Loss: 83.5199127978977\n","Epoch 18100, Loss: 82.49908632816903\n","Epoch 18200, Loss: 81.49352206406367\n","Epoch 18300, Loss: 80.503119892561\n","Epoch 18400, Loss: 79.52777637397203\n","Epoch 18500, Loss: 78.56738075474485\n","Epoch 18600, Loss: 77.62181057331965\n","Epoch 18700, Loss: 76.69092749940906\n","Epoch 18800, Loss: 75.77457516923882\n","Epoch 18900, Loss: 74.87258218062442\n","Epoch 19000, Loss: 73.98477347903093\n","Epoch 19100, Loss: 73.11098911441796\n","Epoch 19200, Loss: 72.25109948943646\n","Epoch 19300, Loss: 71.40499832572097\n","Epoch 19400, Loss: 70.57256694212288\n","Epoch 19500, Loss: 69.75363756897168\n","Epoch 19600, Loss: 68.94798965413474\n","Epoch 19700, Loss: 68.15537033981883\n","Epoch 19800, Loss: 67.37551162810607\n","Epoch 19900, Loss: 66.60813853112293\n","Epoch 20000, Loss: 65.8529738573314\n","Epoch 20100, Loss: 65.10974203635914\n","Epoch 20200, Loss: 64.37817206934227\n","Epoch 20300, Loss: 63.65799963625658\n","Epoch 20400, Loss: 62.948968473379\n","Epoch 20500, Loss: 62.250831117951186\n","Epoch 20600, Loss: 61.563349102057785\n","Epoch 20700, Loss: 60.88629266663601\n","Epoch 20800, Loss: 60.219440064288015\n","Epoch 20900, Loss: 59.56257652628029\n","Epoch 21000, Loss: 58.91549297829574\n","Epoch 21100, Loss: 58.2779845954876\n","Epoch 21200, Loss: 57.64984928736508\n","Epoch 21300, Loss: 57.03088619655639\n","Epoch 21400, Loss: 56.420894283610465\n","Epoch 21500, Loss: 55.819671054634526\n","Epoch 21600, Loss: 55.22701147188888\n","Epoch 21700, Loss: 54.64270707146979\n","Epoch 21800, Loss: 54.06654529841989\n","Epoch 21900, Loss: 53.498309058928555\n","Epoch 22000, Loss: 52.93777648200432\n","Epoch 22100, Loss: 52.38472087886826\n","Epoch 22200, Loss: 51.83891088668409\n","Epoch 22300, Loss: 51.30011078323676\n","Epoch 22400, Loss: 50.76808095983827\n","Epoch 22500, Loss: 50.24257854013639\n","Epoch 22600, Loss: 49.72335813172306\n","Epoch 22700, Loss: 49.21017269456454\n","Epoch 22800, Loss: 48.70277445546958\n","Epoch 22900, Loss: 48.20309496720738\n","Epoch 23000, Loss: 47.70681268111292\n","Epoch 23100, Loss: 47.21564123584726\n","Epoch 23200, Loss: 46.72933949849098\n","Epoch 23300, Loss: 46.24750821552722\n","Epoch 23400, Loss: 45.76988587291294\n","Epoch 23500, Loss: 45.29627620595402\n","Epoch 23600, Loss: 44.826491593662155\n","Epoch 23700, Loss: 44.36021444057442\n","Epoch 23800, Loss: 43.897102522929046\n","Epoch 23900, Loss: 43.43679056718219\n","Epoch 24000, Loss: 42.978359572716066\n","Epoch 24100, Loss: 42.51860055711289\n","Epoch 24200, Loss: 42.03397289765685\n","Epoch 24300, Loss: 41.48032389944321\n","Epoch 24400, Loss: 41.01921588730463\n","Epoch 24500, Loss: 40.572709164031686\n","Epoch 24600, Loss: 40.1339133175107\n","Epoch 24700, Loss: 39.699830129290625\n","Epoch 24800, Loss: 39.2696645178811\n","Epoch 24900, Loss: 38.84272869627614\n","Epoch 25000, Loss: 38.4193052128857\n","Epoch 25100, Loss: 37.99921573361933\n","Epoch 25200, Loss: 37.582459271298205\n","Epoch 25300, Loss: 37.169118771130776\n","Epoch 25400, Loss: 36.7589913477395\n","Epoch 25500, Loss: 36.35234398752485\n","Epoch 25600, Loss: 35.94920219235464\n","Epoch 25700, Loss: 35.54954994180055\n","Epoch 25800, Loss: 35.15353365493822\n","Epoch 25900, Loss: 34.761049400142824\n","Epoch 26000, Loss: 34.372329443922524\n","Epoch 26100, Loss: 33.987379647609814\n","Epoch 26200, Loss: 33.60604188157754\n","Epoch 26300, Loss: 33.22862318638219\n","Epoch 26400, Loss: 32.8550042922058\n","Epoch 26500, Loss: 32.48525247706741\n","Epoch 26600, Loss: 32.119294626384\n","Epoch 26700, Loss: 31.757276572600645\n","Epoch 26800, Loss: 31.399162917396715\n","Epoch 26900, Loss: 31.04493129182717\n","Epoch 27000, Loss: 30.694521474912527\n","Epoch 27100, Loss: 30.348034421880044\n","Epoch 27200, Loss: 30.00538190401252\n","Epoch 27300, Loss: 29.666612770587644\n","Epoch 27400, Loss: 29.331602295225903\n","Epoch 27500, Loss: 29.00046951248787\n","Epoch 27600, Loss: 28.67310281726195\n","Epoch 27700, Loss: 28.349567996285344\n","Epoch 27800, Loss: 28.029762692561366\n","Epoch 27900, Loss: 27.713661020667868\n","Epoch 28000, Loss: 27.40133395466818\n","Epoch 28100, Loss: 27.092927500796975\n","Epoch 28200, Loss: 26.78891358313536\n","Epoch 28300, Loss: 26.489248943017937\n","Epoch 28400, Loss: 26.193927262816654\n","Epoch 28500, Loss: 25.90288907512777\n","Epoch 28600, Loss: 25.616073976404774\n","Epoch 28700, Loss: 25.33350323525376\n","Epoch 28800, Loss: 25.055323449314024\n","Epoch 28900, Loss: 24.781669066719346\n","Epoch 29000, Loss: 24.512628980232048\n","Epoch 29100, Loss: 24.248203517599723\n","Epoch 29200, Loss: 23.98845270874008\n","Epoch 29300, Loss: 23.733368396871388\n","Epoch 29400, Loss: 23.482856143764742\n","Epoch 29500, Loss: 23.236826890605535\n","Epoch 29600, Loss: 22.995240573263537\n","Epoch 29700, Loss: 22.7583545750717\n","Epoch 29800, Loss: 22.526186912078867\n","Epoch 29900, Loss: 22.29873705268982\n","Epoch 30000, Loss: 22.076095714187435\n","Epoch 30100, Loss: 21.858210008463416\n","Epoch 30200, Loss: 21.6450755225865\n","Epoch 30300, Loss: 21.436567726324117\n","Epoch 30400, Loss: 21.232583655845755\n","Epoch 30500, Loss: 21.03303522989811\n","Epoch 30600, Loss: 20.837783615227288\n","Epoch 30700, Loss: 20.646695771848282\n","Epoch 30800, Loss: 20.459644045152075\n","Epoch 30900, Loss: 20.276505729369763\n","Epoch 31000, Loss: 20.097162735091793\n","Epoch 31100, Loss: 19.921501332856394\n","Epoch 31200, Loss: 19.749411948332835\n","Epoch 31300, Loss: 19.58078899117479\n","Epoch 31400, Loss: 19.415530705216575\n","Epoch 31500, Loss: 19.253539031830257\n","Epoch 31600, Loss: 19.09471948118729\n","Epoch 31700, Loss: 18.938981008186378\n","Epoch 31800, Loss: 18.78623589117468\n","Epoch 31900, Loss: 18.63639961249546\n","Epoch 32000, Loss: 18.489390740479337\n","Epoch 32100, Loss: 18.345130812857285\n","Epoch 32200, Loss: 18.203544221783158\n","Epoch 32300, Loss: 18.064558100760145\n","Epoch 32400, Loss: 17.928102213806206\n","Epoch 32500, Loss: 17.79410884719002\n","Epoch 32600, Loss: 17.662512704041248\n","Epoch 32700, Loss: 17.53325080209573\n","Epoch 32800, Loss: 17.40626237478743\n","Epoch 32900, Loss: 17.28148877584801\n","Epoch 33000, Loss: 17.15887338752637\n","Epoch 33100, Loss: 17.038361532494545\n","Epoch 33200, Loss: 16.919900389466758\n","Epoch 33300, Loss: 16.8034389125219\n","Epoch 33400, Loss: 16.688927754090404\n","Epoch 33500, Loss: 16.57631919154026\n","Epoch 33600, Loss: 16.465567057277124\n","Epoch 33700, Loss: 16.35662667225633\n","Epoch 33800, Loss: 16.249454782792586\n","Epoch 33900, Loss: 16.14400950054328\n","Epoch 34000, Loss: 16.04025024553549\n","Epoch 34100, Loss: 15.938137692102314\n","Epoch 34200, Loss: 15.83763371759269\n","Epoch 34300, Loss: 15.738701353718675\n","Epoch 34400, Loss: 15.641304740405241\n","Epoch 34500, Loss: 15.545409082010742\n","Epoch 34600, Loss: 15.4509806057889\n","Epoch 34700, Loss: 15.357986522468043\n","Epoch 34800, Loss: 15.266394988827567\n","Epoch 34900, Loss: 15.176175072156807\n","Epoch 35000, Loss: 15.087296716486849\n","Epoch 35100, Loss: 14.999730710491542\n","Epoch 35200, Loss: 14.913448656958964\n","Epoch 35300, Loss: 14.828422943740811\n","Epoch 35400, Loss: 14.744626716092078\n","Epoch 35500, Loss: 14.662033850319032\n","Epoch 35600, Loss: 14.580618928658573\n","Epoch 35700, Loss: 14.500357215316939\n","Epoch 35800, Loss: 14.421224633600552\n","Epoch 35900, Loss: 14.34319774407624\n","Epoch 36000, Loss: 14.266253723702336\n","Epoch 36100, Loss: 14.190370345876193\n","Epoch 36200, Loss: 14.115525961347192\n","Epoch 36300, Loss: 14.041699479948251\n","Epoch 36400, Loss: 13.968870353101465\n","Epoch 36500, Loss: 13.897018557057049\n","Epoch 36600, Loss: 13.826124576827294\n","Epoch 36700, Loss: 13.756169390779531\n","Epoch 36800, Loss: 13.687134455855146\n","Epoch 36900, Loss: 13.619001693382879\n","Epoch 37000, Loss: 13.551753475457216\n","Epoch 37100, Loss: 13.485372611854444\n","Epoch 37200, Loss: 13.419842337460048\n","Epoch 37300, Loss: 13.355146300183048\n","Epoch 37400, Loss: 13.29126854933391\n","Epoch 37500, Loss: 13.228193524444116\n","Epoch 37600, Loss: 13.16590604450608\n","Epoch 37700, Loss: 13.1043912976136\n","Epoch 37800, Loss: 13.043634830983532\n","Epoch 37900, Loss: 12.983622541340274\n","Epoch 38000, Loss: 12.924340665645484\n","Epoch 38100, Loss: 12.865775772155978\n","Epoch 38200, Loss: 12.807914751793472\n","Epoch 38300, Loss: 12.75074480981038\n","Epoch 38400, Loss: 12.694253457736464\n","Epoch 38500, Loss: 12.638428505591568\n","Epoch 38600, Loss: 12.583258054350555\n","Epoch 38700, Loss: 12.528730488646321\n","Epoch 38800, Loss: 12.474834469698191\n","Epoch 38900, Loss: 12.421558928452836\n","Epoch 39000, Loss: 12.368893058925591\n","Epoch 39100, Loss: 12.31682631173066\n","Epoch 39200, Loss: 12.265348387788963\n","Epoch 39300, Loss: 12.214449232203272\n","Epoch 39400, Loss: 12.16411902829065\n","Epoch 39500, Loss: 12.11434819176243\n","Epoch 39600, Loss: 12.065127365043415\n","Epoch 39700, Loss: 12.016447411721622\n","Epoch 39800, Loss: 11.968299411121079\n","Epoch 39900, Loss: 11.920674652990575\n"]}],"source":["# Initialize and train FONN1\n","input_dim = X_train.shape[1]\n","hidden_dim = 10  # Increased hidden layer size\n","output_dim = 1\n","num_trees_input = 10\n","epochs = 40000  # Increased number of epochs\n","learning_rate = 0.0001  # Decreased learning rate\n","\n","start_time = time.time()\n","fonn1 = FONN1(input_dim, hidden_dim, output_dim, num_trees_input)\n","fonn1.train(X_train, y_train, epochs, learning_rate)\n","end_time = time.time()\n","fonn1_train_time = end_time - start_time"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 569.7653854462766\n","Epoch 100, Loss: 7.090767707879386\n","Epoch 200, Loss: 7.090059135189161\n","Epoch 300, Loss: 7.089212969746764\n","Epoch 400, Loss: 7.0882124026171835\n","Epoch 500, Loss: 7.0870391470611045\n","Epoch 600, Loss: 7.085673492068558\n","Epoch 700, Loss: 7.084094447906731\n","Epoch 800, Loss: 7.082280002014757\n","Epoch 900, Loss: 7.080207502535303\n"]}],"source":["# Initialize and train FONN2\n","input_dim = X_train.shape[1]\n","hidden_dim = 10\n","output_dim = 1\n","num_trees_hidden = 10\n","epochs = 1000\n","learning_rate = 0.001\n","\n","start_time = time.time()\n","fonn2 = FONN2(input_dim, hidden_dim, output_dim, num_trees_hidden)\n","fonn2.train(X_train, y_train, epochs, learning_rate)\n","end_time = time.time()\n","fonn2_train_time = end_time - start_time"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 605.8179568116229\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 100, Loss: 602.954071461698\n","Epoch 200, Loss: 598.86144304153\n","Epoch 300, Loss: 593.9249777615461\n","Epoch 400, Loss: 588.592885091757\n","Epoch 500, Loss: 583.1331516621993\n","Epoch 600, Loss: 577.6857251983316\n","Epoch 700, Loss: 572.315001787408\n","Epoch 800, Loss: 567.0442450314109\n","Epoch 900, Loss: 561.8758468508678\n","Epoch 1000, Loss: 556.8027064282724\n","Epoch 1100, Loss: 551.8143244370971\n","Epoch 1200, Loss: 546.902730587956\n","Epoch 1300, Loss: 542.0669655505415\n","Epoch 1400, Loss: 537.2979243508485\n","Epoch 1500, Loss: 532.5813310922357\n","Epoch 1600, Loss: 527.9056193177004\n","Epoch 1700, Loss: 523.2644547252622\n","Epoch 1800, Loss: 518.6544130981636\n","Epoch 1900, Loss: 514.0734630699509\n","Epoch 2000, Loss: 509.52031571801615\n","Epoch 2100, Loss: 504.9941100640798\n","Epoch 2200, Loss: 500.4942470361692\n","Epoch 2300, Loss: 496.0202956828367\n","Epoch 2400, Loss: 491.5719372648188\n","Epoch 2500, Loss: 487.14893042068616\n","Epoch 2600, Loss: 482.7510886449305\n","Epoch 2700, Loss: 478.3782652606932\n","Epoch 2800, Loss: 474.0303431168711\n","Epoch 2900, Loss: 469.707227354706\n","Epoch 3000, Loss: 465.40884022204517\n","Epoch 3100, Loss: 461.13511728583893\n","Epoch 3200, Loss: 456.8860046194333\n","Epoch 3300, Loss: 452.66145668220315\n","Epoch 3400, Loss: 448.46143469923044\n","Epoch 3500, Loss: 444.28590540767885\n","Epoch 3600, Loss: 440.1348400758281\n","Epoch 3700, Loss: 436.008213727435\n","Epoch 3800, Loss: 431.906004522532\n","Epoch 3900, Loss: 427.82819325870344\n","Epoch 4000, Loss: 423.774762966073\n","Epoch 4100, Loss: 419.7456985758612\n","Epoch 4200, Loss: 415.7409866471976\n","Epoch 4300, Loss: 411.7606151404353\n","Epoch 4400, Loss: 407.80457322786157\n","Epoch 4500, Loss: 403.87285113469756\n","Epoch 4600, Loss: 399.96544000479156\n","Epoch 4700, Loss: 396.0823317865671\n","Epoch 4800, Loss: 392.22351913568724\n","Epoch 4900, Loss: 388.3889953315839\n","Epoch 5000, Loss: 384.57875420555223\n","Epoch 5100, Loss: 380.79279007853734\n","Epoch 5200, Loss: 377.031097707079\n","Epoch 5300, Loss: 373.29367223615606\n","Epoch 5400, Loss: 369.5805091578862\n","Epoch 5500, Loss: 365.8916042752195\n","Epoch 5600, Loss: 362.226953669898\n","Epoch 5700, Loss: 358.5865536740803\n","Epoch 5800, Loss: 354.9704008451177\n","Epoch 5900, Loss: 351.37849194305153\n","Epoch 6000, Loss: 347.8108239104659\n","Epoch 6100, Loss: 344.26739385438094\n","Epoch 6200, Loss: 340.7481990299215\n","Epoch 6300, Loss: 337.25323682552863\n","Epoch 6400, Loss: 333.78250474951625\n","Epoch 6500, Loss: 330.3360004178004\n","Epoch 6600, Loss: 326.91372154265014\n","Epoch 6700, Loss: 323.5156659223291\n","Epoch 6800, Loss: 320.14183143151223\n","Epoch 6900, Loss: 316.7922160123737\n","Epoch 7000, Loss: 313.46681766625727\n","Epoch 7100, Loss: 310.16563444584574\n","Epoch 7200, Loss: 306.8886644477567\n","Epoch 7300, Loss: 303.63590580549766\n","Epoch 7400, Loss: 300.40735668271907\n","Epoch 7500, Loss: 297.2030152667091\n","Epoch 7600, Loss: 294.0228797620765\n","Epoch 7700, Loss: 290.86694838457237\n","Epoch 7800, Loss: 287.7352193550012\n","Epoch 7900, Loss: 284.6276908931775\n","Epoch 8000, Loss: 281.5443612118793\n","Epoch 8100, Loss: 278.48522851075563\n","Epoch 8200, Loss: 275.4502909701417\n","Epoch 8300, Loss: 272.4395467447332\n","Epoch 8400, Loss: 269.4529939570753\n","Epoch 8500, Loss: 266.4906306908117\n","Epoch 8600, Loss: 263.5524549836438\n","Epoch 8700, Loss: 260.63846481994034\n","Epoch 8800, Loss: 257.748658122938\n","Epoch 8900, Loss: 254.8830327464648\n","Epoch 9000, Loss: 252.0415864661136\n","Epoch 9100, Loss: 249.22431696978487\n","Epoch 9200, Loss: 246.4312218475095\n","Epoch 9300, Loss: 243.66229858045077\n","Epoch 9400, Loss: 240.91754452897518\n","Epoch 9500, Loss: 238.19695691966447\n","Epoch 9600, Loss: 235.5005328311285\n","Epoch 9700, Loss: 232.82826917845549\n","Epoch 9800, Loss: 230.1801626961182\n","Epoch 9900, Loss: 227.55620991912443\n","Epoch 10000, Loss: 224.9564071621732\n","Epoch 10100, Loss: 222.38075049653918\n","Epoch 10200, Loss: 219.8292357243682\n","Epoch 10300, Loss: 217.30185835001453\n","Epoch 10400, Loss: 214.7986135479941\n","Epoch 10500, Loss: 212.31949612705554\n","Epoch 10600, Loss: 209.86450048979003\n","Epoch 10700, Loss: 207.43362058709894\n","Epoch 10800, Loss: 205.02684986672222\n","Epoch 10900, Loss: 202.64418121488487\n","Epoch 11000, Loss: 200.28560688994767\n","Epoch 11100, Loss: 197.9511184467381\n","Epoch 11200, Loss: 195.64070664998417\n","Epoch 11300, Loss: 193.35436137496143\n","Epoch 11400, Loss: 191.09207149308386\n","Epoch 11500, Loss: 188.85382473969634\n","Epoch 11600, Loss: 186.63960756074718\n","Epoch 11700, Loss: 184.44940493429044\n","Epoch 11800, Loss: 182.28320016186103\n","Epoch 11900, Loss: 180.14097462362105\n","Epoch 12000, Loss: 178.022707489727\n","Epoch 12100, Loss: 175.9283753785203\n","Epoch 12200, Loss: 173.8579519497732\n","Epoch 12300, Loss: 171.81140741816014\n","Epoch 12400, Loss: 169.78870796813717\n","Epoch 12500, Loss: 167.78981504618164\n","Epoch 12600, Loss: 165.81468449942437\n","Epoch 12700, Loss: 163.86326552046955\n","Epoch 12800, Loss: 161.93549934575398\n","Epoch 12900, Loss: 160.0313176378596\n","Epoch 13000, Loss: 158.15064045889625\n","Epoch 13100, Loss: 156.2933737096503\n","Epoch 13200, Loss: 154.45940586351742\n","Epoch 13300, Loss: 152.64860375902236\n","Epoch 13400, Loss: 150.86080712026597\n","Epoch 13500, Loss: 149.095821335663\n","Epoch 13600, Loss: 147.3534078173822\n","Epoch 13700, Loss: 145.63327094692255\n","Epoch 13800, Loss: 143.93504011925177\n","Epoch 13900, Loss: 142.25824461390906\n","Epoch 14000, Loss: 140.6022777438062\n","Epoch 14100, Loss: 138.96634459326077\n","Epoch 14200, Loss: 137.3493839662947\n","Epoch 14300, Loss: 135.74994858486176\n","Epoch 14400, Loss: 134.16601540397593\n","Epoch 14500, Loss: 132.59467449384445\n","Epoch 14600, Loss: 131.0315980617216\n","Epoch 14700, Loss: 129.4700942880153\n","Epoch 14800, Loss: 127.900282136568\n","Epoch 14900, Loss: 126.31081858520803\n","Epoch 15000, Loss: 124.68401992158793\n","Epoch 15100, Loss: 123.00535414056507\n","Epoch 15200, Loss: 121.26290612516304\n","Epoch 15300, Loss: 119.44918723217272\n","Epoch 15400, Loss: 117.58096997965136\n","Epoch 15500, Loss: 115.73043104659867\n","Epoch 15600, Loss: 114.03055099101383\n","Epoch 15700, Loss: 112.43771785357632\n","Epoch 15800, Loss: 110.90059354764378\n","Epoch 15900, Loss: 109.40760043519772\n","Epoch 16000, Loss: 107.95307227404003\n","Epoch 16100, Loss: 106.5327412973667\n","Epoch 16200, Loss: 105.14315283059875\n","Epoch 16300, Loss: 103.78151439482211\n","Epoch 16400, Loss: 102.44557819587139\n","Epoch 16500, Loss: 101.13353476392477\n","Epoch 16600, Loss: 99.84392386335891\n","Epoch 16700, Loss: 98.57556309223497\n","Epoch 16800, Loss: 97.32749129102659\n","Epoch 16900, Loss: 96.09892348320425\n","Epoch 17000, Loss: 94.88921472368207\n","Epoch 17100, Loss: 93.69783098880512\n","Epoch 17200, Loss: 92.5243257880892\n","Epoch 17300, Loss: 91.36832149400728\n","Epoch 17400, Loss: 90.22949455337717\n","Epoch 17500, Loss: 89.10756384399947\n","Epoch 17600, Loss: 88.00228152269771\n","Epoch 17700, Loss: 86.9134257973895\n","Epoch 17800, Loss: 85.84079515147846\n","Epoch 17900, Loss: 84.78420365042548\n","Epoch 18000, Loss: 83.74347706015791\n","Epoch 18100, Loss: 82.71844959560016\n","Epoch 18200, Loss: 81.70896118635747\n","Epoch 18300, Loss: 80.71485518986663\n","Epoch 18400, Loss: 79.7359764995603\n","Epoch 18500, Loss: 78.77216999175816\n","Epoch 18600, Loss: 77.82327923937264\n","Epoch 18700, Loss: 76.88914540396831\n","Epoch 18800, Loss: 75.96960620966027\n","Epoch 18900, Loss: 75.06449490853937\n","Epoch 19000, Loss: 74.17363916902536\n","Epoch 19100, Loss: 73.29685985303755\n","Epoch 19200, Loss: 72.43396968980457\n","Epoch 19300, Loss: 71.58477189725564\n","Epoch 19400, Loss: 70.74905884038142\n","Epoch 19500, Loss: 69.92661084489478\n","Epoch 19600, Loss: 69.1171952999646\n","Epoch 19700, Loss: 68.32056618163995\n","Epoch 19800, Loss: 67.53646410324751\n","Epoch 19900, Loss: 66.76461694270934\n","Epoch 20000, Loss: 66.00474100147271\n","Epoch 20100, Loss: 65.25654251646594\n","Epoch 20200, Loss: 64.51971920350753\n","Epoch 20300, Loss: 63.79396143619722\n","Epoch 20400, Loss: 63.07895278750141\n","Epoch 20500, Loss: 62.37437009744984\n","Epoch 20600, Loss: 61.67988394141132\n","Epoch 20700, Loss: 60.99516105140615\n","Epoch 20800, Loss: 60.31987039331141\n","Epoch 20900, Loss: 59.653693830120666\n","Epoch 21000, Loss: 58.996340578357206\n","Epoch 21100, Loss: 58.347562845913465\n","Epoch 21200, Loss: 57.707171185496954\n","Epoch 21300, Loss: 57.07505516612107\n","Epoch 21400, Loss: 56.4512136699915\n","Epoch 21500, Loss: 55.835750986559006\n","Epoch 21600, Loss: 55.2287388906552\n","Epoch 21700, Loss: 54.62997956917275\n","Epoch 21800, Loss: 54.03894376127819\n","Epoch 21900, Loss: 53.45500824108613\n","Epoch 22000, Loss: 52.877841789033575\n","Epoch 22100, Loss: 52.307807252362146\n","Epoch 22200, Loss: 51.74592543580402\n","Epoch 22300, Loss: 51.192822471087084\n","Epoch 22400, Loss: 50.64796260626829\n","Epoch 22500, Loss: 50.110406377196554\n","Epoch 22600, Loss: 49.57947859201866\n","Epoch 22700, Loss: 49.05472240542299\n","Epoch 22800, Loss: 48.5357500335391\n","Epoch 22900, Loss: 48.02217251902069\n","Epoch 23000, Loss: 47.51356158910409\n","Epoch 23100, Loss: 47.009409759375146\n","Epoch 23200, Loss: 46.5090720163578\n","Epoch 23300, Loss: 46.011673109812286\n","Epoch 23400, Loss: 45.51596442898524\n","Epoch 23500, Loss: 45.02015270672604\n","Epoch 23600, Loss: 44.52197929739847\n","Epoch 23700, Loss: 44.02042012475046\n","Epoch 23800, Loss: 43.52161643992177\n","Epoch 23900, Loss: 43.03909008023085\n","Epoch 24000, Loss: 42.57335396250654\n","Epoch 24100, Loss: 42.11650576491774\n","Epoch 24200, Loss: 41.66530000441135\n","Epoch 24300, Loss: 41.21888192948779\n","Epoch 24400, Loss: 40.77692638211941\n","Epoch 24500, Loss: 40.339257147486386\n","Epoch 24600, Loss: 39.90576100853216\n","Epoch 24700, Loss: 39.47635756932723\n","Epoch 24800, Loss: 39.05098582825804\n","Epoch 24900, Loss: 38.6295974678067\n","Epoch 25000, Loss: 38.21215316215506\n","Epoch 25100, Loss: 37.79862034179527\n","Epoch 25200, Loss: 37.38897170933514\n","Epoch 25300, Loss: 36.983184170002886\n","Epoch 25400, Loss: 36.581238010308354\n","Epoch 25500, Loss: 36.183116240824695\n","Epoch 25600, Loss: 35.78880406038035\n","Epoch 25700, Loss: 35.39828841956342\n","Epoch 25800, Loss: 35.01155767111544\n","Epoch 25900, Loss: 34.628601298571866\n","Epoch 26000, Loss: 34.24940971515509\n","Epoch 26100, Loss: 33.87397412404169\n","Epoch 26200, Loss: 33.502286429667365\n","Epoch 26300, Loss: 33.13433918829918\n","Epoch 26400, Loss: 32.770125585071696\n","Epoch 26500, Loss: 32.40963942428899\n","Epoch 26600, Loss: 32.05287512016744\n","Epoch 26700, Loss: 31.6998276763809\n","Epoch 26800, Loss: 31.35049264473259\n","Epoch 26900, Loss: 31.004866055908742\n","Epoch 27000, Loss: 30.662944318381662\n","Epoch 27100, Loss: 30.324724084889397\n","Epoch 27200, Loss: 29.99020208924129\n","Epoch 27300, Loss: 29.659374959194622\n","Epoch 27400, Loss: 29.332239013543536\n","Epoch 27500, Loss: 29.008790053142423\n","Epoch 27600, Loss: 28.68902315621768\n","Epoch 27700, Loss: 28.372932487978808\n","Epoch 27800, Loss: 28.060511133299787\n","Epoch 27900, Loss: 27.75175095928585\n","Epoch 28000, Loss: 27.446642512114543\n","Epoch 28100, Loss: 27.145174949927906\n","Epoch 28200, Loss: 26.847336011030887\n","Epoch 28300, Loss: 26.55311201445674\n","Epoch 28400, Loss: 26.2624878882635\n","Epoch 28500, Loss: 25.975447219820417\n","Epoch 28600, Loss: 25.69197232184337\n","Epoch 28700, Loss: 25.412044307994368\n","Epoch 28800, Loss: 25.135643172373637\n","Epoch 28900, Loss: 24.862747868079047\n","Epoch 29000, Loss: 24.593336381054353\n","Epoch 29100, Loss: 24.327385796572866\n","Epoch 29200, Loss: 24.064872356801494\n","Epoch 29300, Loss: 23.80577150888443\n","Epoch 29400, Loss: 23.55005794382274\n","Epoch 29500, Loss: 23.29778125686368\n","Epoch 29600, Loss: 23.049085393988882\n","Epoch 29700, Loss: 22.804541755067024\n","Epoch 29800, Loss: 22.565246668991865\n","Epoch 29900, Loss: 22.33146642524034\n","Epoch 30000, Loss: 22.103136194719024\n","Epoch 30100, Loss: 21.880092194125965\n","Epoch 30200, Loss: 21.66232874212869\n","Epoch 30300, Loss: 21.449758810741912\n","Epoch 30400, Loss: 21.24221705413643\n","Epoch 30500, Loss: 21.039544359106422\n","Epoch 30600, Loss: 20.841587530086063\n","Epoch 30700, Loss: 20.648199000041632\n","Epoch 30800, Loss: 20.459236564396075\n","Epoch 30900, Loss: 20.27456313556253\n","Epoch 31000, Loss: 20.09404651584822\n","Epoch 31100, Loss: 19.917559186671443\n","Epoch 31200, Loss: 19.744978112227237\n","Epoch 31300, Loss: 19.5761845559355\n","Epoch 31400, Loss: 19.41106390819983\n","Epoch 31500, Loss: 19.249505524191182\n","Epoch 31600, Loss: 19.091402570544677\n","Epoch 31700, Loss: 18.936651880017454\n","Epoch 31800, Loss: 18.78515381329971\n","Epoch 31900, Loss: 18.636812127298565\n","Epoch 32000, Loss: 18.491533849326192\n","Epoch 32100, Loss: 18.34922915671889\n","Epoch 32200, Loss: 18.20981126149444\n","Epoch 32300, Loss: 18.073196299721864\n","Epoch 32400, Loss: 17.9393032253316\n","Epoch 32500, Loss: 17.80805370813753\n","Epoch 32600, Loss: 17.679372035875115\n","Epoch 32700, Loss: 17.55318502008551\n","Epoch 32800, Loss: 17.429421905693815\n","Epoch 32900, Loss: 17.30801428414203\n","Epoch 33000, Loss: 17.18889600994624\n","Epoch 33100, Loss: 17.07200312055227\n","Epoch 33200, Loss: 16.957273759366835\n","Epoch 33300, Loss: 16.844648101842314\n","Epoch 33400, Loss: 16.734068284493276\n","Epoch 33500, Loss: 16.62547833672246\n","Epoch 33600, Loss: 16.518824115333622\n","Epoch 33700, Loss: 16.414053241608613\n","Epoch 33800, Loss: 16.311115040826117\n","Epoch 33900, Loss: 16.20996048410138\n","Epoch 34000, Loss: 16.110542132427238\n","Epoch 34100, Loss: 16.012814082800418\n","Epoch 34200, Loss: 15.91673191631998\n","Epoch 34300, Loss: 15.822252648149464\n","Epoch 34400, Loss: 15.729334679239155\n","Epoch 34500, Loss: 15.63793774971054\n","Epoch 34600, Loss: 15.548022893810868\n","Epoch 34700, Loss: 15.459552396352557\n","Epoch 34800, Loss: 15.372489750558115\n","Epoch 34900, Loss: 15.286799617238715\n","Epoch 35000, Loss: 15.202447785240489\n","Epoch 35100, Loss: 15.119401133099826\n","Epoch 35200, Loss: 15.03762759185464\n","Epoch 35300, Loss: 14.957096108965109\n","Epoch 35400, Loss: 14.87777661330279\n","Epoch 35500, Loss: 14.799639981171905\n","Epoch 35600, Loss: 14.72265800333196\n","Epoch 35700, Loss: 14.64680335299435\n","Epoch 35800, Loss: 14.572049554769652\n","Epoch 35900, Loss: 14.498370954545372\n","Epoch 36000, Loss: 14.425742690276506\n","Epoch 36100, Loss: 14.35414066367325\n","Epoch 36200, Loss: 14.28354151277216\n","Epoch 36300, Loss: 14.213922585377816\n","Epoch 36400, Loss: 14.145261913363447\n","Epoch 36500, Loss: 14.077538187818705\n","Epoch 36600, Loss: 14.01073073503351\n","Epoch 36700, Loss: 13.944819493306472\n","Epoch 36800, Loss: 13.879784990566241\n","Epoch 36900, Loss: 13.815608322793237\n","Epoch 37000, Loss: 13.752271133229156\n","Epoch 37100, Loss: 13.68975559236032\n","Epoch 37200, Loss: 13.628044378660713\n","Epoch 37300, Loss: 13.56712066007915\n","Epoch 37400, Loss: 13.506968076254783\n","Epoch 37500, Loss: 13.447570721444064\n","Epoch 37600, Loss: 13.388913128141532\n","Epoch 37700, Loss: 13.330980251376431\n","Epoch 37800, Loss: 13.273757453666333\n","Epoch 37900, Loss: 13.217230490608557\n","Epoch 38000, Loss: 13.161385497089888\n","Epoch 38100, Loss: 13.106208974094638\n","Epoch 38200, Loss: 13.051687776091063\n","Epoch 38300, Loss: 12.997809098975956\n","Epoch 38400, Loss: 12.94456046855744\n","Epoch 38500, Loss: 12.891929729555612\n","Epoch 38600, Loss: 12.839905035101461\n","Epoch 38700, Loss: 12.788474836714206\n","Epoch 38800, Loss: 12.7376278747378\n","Epoch 38900, Loss: 12.687353169217392\n","Epoch 39000, Loss: 12.637640011197393\n","Epoch 39100, Loss: 12.588477954422638\n","Epoch 39200, Loss: 12.539856807424973\n","Epoch 39300, Loss: 12.491766625978002\n","Epoch 39400, Loss: 12.444197705903008\n","Epoch 39500, Loss: 12.397140576209772\n","Epoch 39600, Loss: 12.350585992556379\n","Epoch 39700, Loss: 12.30452493101249\n","Epoch 39800, Loss: 12.258948582111158\n","Epoch 39900, Loss: 12.21384834517465\n"]}],"source":["# Initialize and train TREENN1\n","input_dim = X_train.shape[1]\n","hidden_dim = 10  # Hidden layer size\n","output_dim = 1\n","epochs = 40000  # Number of epochs\n","learning_rate = 0.0001  # Learning rate\n","\n","start_time = time.time()\n","treenn1 = TREENN1(input_dim, hidden_dim, output_dim)\n","treenn1.train(X_train, y_train, epochs, learning_rate)\n","end_time = time.time()\n","treenn1_train_time = end_time - start_time"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Loss: 589.0452029724268\n","Epoch 100, Loss: 470.7939577039868\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 200, Loss: 358.2679207530062\n","Epoch 300, Loss: 255.81353295582977\n","Epoch 400, Loss: 168.077202715935\n","Epoch 500, Loss: 97.32971622165032\n","Epoch 600, Loss: 45.561803476522265\n","Epoch 700, Loss: 15.133814602572725\n","Epoch 800, Loss: 7.389116148230879\n","Epoch 900, Loss: 7.253237351153362\n"]}],"source":["# Initialize and train TREENN2\n","input_dim = X_train.shape[1]\n","hidden_dim = 10\n","output_dim = 1\n","epochs = 1000\n","learning_rate = 0.001\n","\n","start_time = time.time()\n","treenn2 = TREENN2(input_dim, hidden_dim, output_dim)\n","treenn2.train(X_train, y_train, epochs, learning_rate)\n","end_time = time.time()\n","treenn2_train_time = end_time - start_time"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 102 and the array at index 1 has size 404","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Measure computational time and evaluate the FONN1 model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m fonn1_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mfonn1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m fonn1_comp_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n","File \u001b[0;32m~/Developer/research/models.py:37\u001b[0m, in \u001b[0;36mFONN1.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 37\u001b[0m     combined_input \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tree_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Compute hidden layer activations\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_hidden \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\n\u001b[1;32m     41\u001b[0m         combined_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_hidden) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_hidden\n","File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n","File \u001b[0;32m~/anaconda3/envs/research/lib/python3.11/site-packages/numpy/core/shape_base.py:345\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 102 and the array at index 1 has size 404"]}],"source":["# Measure computational time and evaluate the FONN1 model\n","start_time = time.time()\n","fonn1_predictions = fonn1.forward(X_test)\n","end_time = time.time()\n","fonn1_comp_time = end_time - start_time\n","\n","fonn1_r2 = r2_score(y_test, fonn1_predictions)\n","fonn1_mae = mean_absolute_error(y_test, fonn1_predictions)\n","fonn1_mse = mean_squared_error(y_test, fonn1_predictions)\n","\n","results[\"FONN1\"] = {\"R² Score\": fonn1_r2, \"MAE\": fonn1_mae, \"MSE\": fonn1_mse,\n","                    \"Train Time (s)\": fonn1_train_time, \"Comp Time (s)\": fonn1_comp_time}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Measure computational time and evaluate the custom MLP model\n","start_time = time.time()\n","fonn2_predictions = fonn2.forward(X_test)\n","end_time = time.time()\n","fonn2_comp_time = end_time - start_time\n","\n","fonn2_r2 = r2_score(y_test, fonn2_predictions)\n","fonn2_mae = mean_absolute_error(y_test, fonn2_predictions)\n","fonn2_mse = mean_squared_error(y_test, fonn2_predictions)\n","\n","results[\"FONN2\"] = {\"R² Score\": fonn2_r2, \"MAE\": fonn2_mae, \"MSE\": fonn2_mse,\n","                    \"Train Time (s)\": fonn2_train_time, \"Comp Time (s)\": fonn2_comp_time}\n","\n","# Measure computational time and predict house prices using the decision trees in the hidden layer\n","start_time = time.time()\n","fonn2_tree_predictions = fonn2.tree_predict(X_test)\n","end_time = time.time()\n","fonn2_tree_comp_time = end_time - start_time\n","\n","fonn2_tree_r2 = r2_score(y_test, fonn2_tree_predictions)\n","fonn2_tree_mae = mean_absolute_error(y_test, fonn2_tree_predictions)\n","fonn2_tree_mse = mean_squared_error(y_test, fonn2_tree_predictions)\n","\n","results[\"Tree-based Predictions (FONN2)\"] = {\"R² Score\": fonn2_tree_r2, \"MAE\": fonn2_tree_mae,\n","                                             \"MSE\": fonn2_tree_mse, \"Train Time (s)\": fonn2_train_time, \"Comp Time (s)\": fonn2_tree_comp_time}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine 10 decision trees and evaluate the ensemble model\n","start_time = time.time()\n","trees = [DecisionTreeRegressor(max_depth=5, random_state=i).fit(\n","    X_train, y_train) for i in range(10)]\n","end_time = time.time()\n","ensemble_train_time = end_time - start_time\n","\n","start_time = time.time()\n","ensemble_predictions = np.mean(\n","    [tree.predict(X_test) for tree in trees], axis=0)\n","end_time = time.time()\n","ensemble_comp_time = end_time - start_time\n","\n","ensemble_r2 = r2_score(y_test, ensemble_predictions)\n","ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)\n","ensemble_mse = mean_squared_error(y_test, ensemble_predictions)\n","\n","results[\"Ensemble of 10 Trees\"] = {\"R² Score\": ensemble_r2, \"MAE\": ensemble_mae,\n","                                   \"MSE\": ensemble_mse, \"Train Time (s)\": ensemble_train_time, \"Comp Time (s)\": ensemble_comp_time}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Measure computational time and evaluate the TREENN1 model\n","start_time = time.time()\n","treenn1_predictions = treenn1.forward(X_test)\n","end_time = time.time()\n","treenn1_comp_time = end_time - start_time\n","\n","treenn1_r2 = r2_score(y_test, treenn1_predictions)\n","treenn1_mae = mean_absolute_error(y_test, treenn1_predictions)\n","treenn1_mse = mean_squared_error(y_test, treenn1_predictions)\n","\n","results[\"TREENN1\"] = {\"R² Score\": treenn1_r2, \"MAE\": treenn1_mae, \"MSE\": treenn1_mse,\n","                      \"Train Time (s)\": treenn1_train_time, \"Comp Time (s)\": treenn1_comp_time}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Measure computational time and evaluate the custom MLP model\n","start_time = time.time()\n","treenn2_predictions = treenn2.forward(X_test)\n","end_time = time.time()\n","treenn2_comp_time = end_time - start_time\n","\n","treenn2_r2 = r2_score(y_test, treenn2_predictions)\n","treenn2_mae = mean_absolute_error(y_test, treenn2_predictions)\n","treenn2_mse = mean_squared_error(y_test, treenn2_predictions)\n","\n","results[\"TREENN2\"] = {\"R² Score\": treenn2_r2, \"MAE\": treenn2_mae,\n","                      \"MSE\": treenn2_mse, \"Train Time (s)\": treenn2_train_time, \"Comp Time (s)\": treenn2_comp_time}\n","\n","# Measure computational time and predict house prices using the decision tree in the hidden layer\n","start_time = time.time()\n","treenn2_tree_predictions = treenn2.tree_hidden.predict(treenn2.a1)\n","end_time = time.time()\n","treenn2_tree_comp_time = end_time - start_time\n","\n","treenn2_tree_r2 = r2_score(y_test, treenn2_tree_predictions)\n","treenn2_tree_mae = mean_absolute_error(y_test, treenn2_tree_predictions)\n","treenn2_tree_mse = mean_squared_error(y_test, treenn2_tree_predictions)\n","\n","results[\"Tree-based Predictions (TREENN2)\"] = {\"R² Score\": treenn2_tree_r2, \"MAE\": treenn2_tree_mae,\n","                                               \"MSE\": treenn2_tree_mse, \"Train Time (s)\": treenn2_train_time, \"Comp Time (s)\": treenn2_tree_comp_time}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert results to a DataFrame for better visualization\n","results_df = pd.DataFrame(results).T\n","print(results_df)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get and print tree importances\n","tree_importances = fonn2.get_tree_importances()"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
