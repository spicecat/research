{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yNSyFZvf0Leo"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# # Load the store sales dataset\n","# excel_file = pd.ExcelFile('data/store_sales.xlsx')\n","# sheet_names = excel_file.sheet_names\n","\n","# # Read the data\n","# results_df = pd.read_excel(excel_file, sheet_name=sheet_names[2])  # 2, 9\n","# iri_key_counts = results_df['IRI_KEY'].value_counts()\n","# iri_keys = iri_key_counts[iri_key_counts > 300].index\n","\n","\n","# features = ['F', 'D', 'Unit.Price']\n","# target = 'Total.Volume'\n","\n","# results_df = results_df[results_df['IRI_KEY'] == iri_keys[0]]\n","# X = results_df[features].values\n","# y = results_df[target].values\n","\n","# sheet_names, iri_keys, X.shape, y.shape"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# # Load the Boston dataset\n","# data_url = 'http://lib.stat.cmu.edu/datasets/boston'\n","# raw_df = pd.read_csv(data_url, sep='\\s+', skiprows=22,  # type: ignore\n","#                      header=None)  # type: ignore\n","# X = np.hstack([raw_df.values[::2, :-1], raw_df.values[1::2, :2]])\n","# y = raw_df.values[1::2, 2].reshape(-1, 1).ravel()\n","# X.shape, y.shape"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# # Load the California housing dataset\n","# from sklearn.datasets import fetch_california_housing\n","# housing = fetch_california_housing()\n","# X = housing.data  # type: ignore\n","# y = housing.target  # type: ignore\n","# X.shape, y.shape, housing.feature_names  # type: ignore"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["((34212, 18), (34212,))"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Load the soybean dataset\n","raw_df = pd.read_excel(\"data/soybean.xlsx\")\n","# print(raw_df.head())\n","X = raw_df.values[:-1, [5, 6, 15, 16, 17, 26,\n","                        34, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]]  # 9 check yield 12 rm band\n","y = raw_df.values[:-1, 11]\n","X.shape, y.shape"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# # Define the data\n","# data = {\n","#     \"Size\": [850, 900, 1200, 1400, 1600, 1700, 1800, 2000, 2200, 2500],\n","#     \"Bedrooms\": [2, 3, 3, 3, 3, 4, 4, 4, 5, 5],\n","#     \"Price\": [300, 340, 400, 500, 520, 580, 600, 620, 720, 790]\n","# }\n","\n","# df = pd.DataFrame(data)\n","\n","# X = df[[\"Size\", \"Bedrooms\"]].values\n","# y = df[\"Price\"].values\n","# X.shape, y.shape"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, QuantileTransformer\n","\n","X = X.astype(np.float64)\n","y = y.astype(np.float64)\n","\n","# scaler_X = StandardScaler()\n","# X = scaler_X.fit_transform(X)\n","# scaler_y = StandardScaler()\n","# y = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()  # type: ignore\n","scaler_X = QuantileTransformer()\n","X = scaler_X.fit_transform(X)\n","scaler_y = QuantileTransformer()\n","y = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()  # type: ignore"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from models_sklearn import Ensemble, MLP, FONN1, FONN2, TREENN1, TREENN2\n","from models import MLP as MLP_2, FONN1 as FONN1_2, FONN2 as FONN2_2, TREENN1 as TREENN1_2, TREENN2 as TREENN2_2\n","\n","search_params = {\n","    'scoring': ['neg_root_mean_squared_error', 'r2'],\n","    'n_jobs': -1,\n","    'refit': 'neg_root_mean_squared_error',\n","    'cv': 3,\n","    'verbose': 1,\n","    'return_train_score': True\n","}\n","\n","ensemble_param_grid = {\n","    'random_state': [42]\n","}\n","\n","nn_param_grid = {\n","    # 'activation': ['tanh'],\n","    # 'learning_rate': ['constant'],\n","    'learning_rate': ['adaptive'],\n","    # 'max_iter': [1000, 100, 2000],\n","    # 'learning_rate_init': [1e-1, 1e-2, 1e-3],\n","    # 'learning_rate_init': [1e1, 1e-2, 1e-3],\n","    # 'learning_rate_init': [1e-2],\n","    # 'early_stopping': [True],\n","    'max_iter': [400],\n","    'random_state': [42],\n","    'early_stopping': [False],\n","    'n_iter_no_change': [10000],\n","}\n","\n","test_param_grid = {\n","    'activation': ['tanh'],\n","    'learning_rate': ['adaptive'],\n","    'max_iter': [400],\n","    'random_state': [42],\n","    'early_stopping': [False],\n","    'n_iter_no_change': [10000]\n","}\n","\n","\n","def search_strategy(model, param_grid=nn_param_grid, search_params=search_params):\n","    return GridSearchCV(model, param_grid, **search_params)\n","# search_strategy = RandomizedSearchCV\n","\n","\n","models = {}\n","\n","num_trees_input = 5\n","num_trees_hidden = 5\n","# hidden_nodes = [5]\n","hidden_nodes = [5, 40]\n","# hidden_nodes = [(5,), (10,), (40,), (100,)]\n","\n","models['Tree'] = search_strategy(Ensemble(1), ensemble_param_grid)\n","for hn in hidden_nodes:\n","    if isinstance(hn, tuple):\n","        models[f'Ensemble {sum(hn)}'] = search_strategy(\n","            Ensemble(sum(hn)), ensemble_param_grid)\n","        models[f'MLP {hn}'] = search_strategy(\n","            MLP(hn))\n","        models[f'FONN1 {num_trees_input} {hn}'] = search_strategy(\n","            FONN1(num_trees_input, (num_trees_input+hn[0], *hn[1:])))\n","        models[f'FONN2 {num_trees_hidden} {hn}'] = search_strategy(\n","            FONN2(num_trees_hidden, (*hn[:-1], num_trees_hidden+hn[-1])))\n","        models[f'TREENN1 {hn}'] = search_strategy(\n","            TREENN1((1+hn[0], *hn[1:])))\n","        models[f'TREENN2 {hn}'] = search_strategy(\n","            TREENN2((*hn[:-1], 1+hn[-1])))\n","    else:\n","        models[f'Ensemble {hn}'] = search_strategy(\n","            Ensemble(hn), ensemble_param_grid)\n","        # models[f'MLP {hn}'] = search_strategy(\n","        #     MLP(hn))\n","        # models[f'MLP_tanh {hn}'] = search_strategy(\n","        #     MLP(hn), test_param_grid)\n","        # models[f'FONN1 {num_trees_input} {hn}'] = search_strategy(\n","        #     FONN1(num_trees_input, num_trees_input+hn))\n","        # models[f'FONN2 {num_trees_hidden} {hn}'] = search_strategy(\n","        #     FONN2(hn, num_trees_hidden+hn))\n","        # models[f'TREENN1 {hn}'] = search_strategy(\n","        #     TREENN1(1+hn))\n","        # models[f'TREENN2 {hn}'] = search_strategy(\n","        #     TREENN2(1+hn))\n","\n","nn_param_grid_2 = {\n","    'activation': ['tanh'],\n","    'epochs': [400],\n","    # 'epochs': [100],\n","    # 'learning_rate': [1e1, 1e0, 1e-1, 1e-2],\n","    # 'learning_rate': [1e-2, 1e-3],\n","    'learning_rate': [1e-2],\n","}\n","test_param_grid_2 = {\n","    # 'activation': ['tanh'],\n","    'epochs': [400],\n","    # 'epochs': [100],\n","    # 'learning_rate': [1e1, 1e0, 1e-1, 1e-2],\n","    # 'learning_rate': [1e-2, 1e-3],\n","    'learning_rate': [1e-2],\n","}\n","\n","input_dim = X.shape[1]\n","output_dim = 1\n","\n","for hn in hidden_nodes:\n","    models[f'MLP_2 {hn}'] = search_strategy(\n","        MLP_2(input_dim, hn, output_dim), nn_param_grid_2)\n","    models[f'MLP_2_tanh {hn}'] = search_strategy(\n","        MLP_2(input_dim, hn, output_dim), test_param_grid_2)\n","    # models[f'FONN1_2 {num_trees_input} {hn}'] = search_strategy(\n","    #     FONN1_2(input_dim, hn, output_dim, num_trees_input), nn_param_grid_2)\n","    # models[f'FONN2_2 {num_trees_hidden} {hn}'] = search_strategy(\n","    #     FONN2_2(input_dim, hn, output_dim, num_trees_hidden), nn_param_grid_2)\n","    # models[f'TREENN1_2 {hn}'] = search_strategy(\n","    #     TREENN1_2(input_dim, hn, output_dim), nn_param_grid_2)\n","    # models[f'TREENN2_2 {hn}'] = search_strategy(\n","    #     TREENN2_2(input_dim, hn, output_dim), nn_param_grid_2)\n","    models[f'TREENN1_2 {hn}'] = search_strategy(\n","        FONN1_2(input_dim, hn, output_dim, 1), nn_param_grid_2)\n","    models[f'TREENN2_2 {hn}'] = search_strategy(\n","        FONN2_2(input_dim, hn, output_dim, 1), nn_param_grid_2)\n","    # models['Tree-based Predictions (FONN2)'] = models['FONN2'].trees\n","    # models['Tree-based Predictions (TREENN2)'] = models['TREENN2'].trees"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"]},{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n","Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"]}],"source":["import sys\n","import os\n","from warnings import simplefilter\n","if not sys.warnoptions:\n","    simplefilter(\"ignore\")\n","    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n","\n","# Train and evaluate models\n","cv_results = {}\n","results = []\n","\n","result_columns = [\n","    'mean_fit_time', 'mean_score_time', 'mean_train_neg_root_mean_squared_error', 'mean_test_neg_root_mean_squared_error', 'mean_train_r2', 'mean_test_r2', 'params'\n","]\n","\n","for name, model in models.items():\n","    model.fit(X, y)\n","    result = model.cv_results_\n","    results.append({\n","        'model': name,\n","        **{col: result[col][model.best_index_] for col in result_columns if col in result}\n","    })\n","    cv_results[name] = result\n","\n","results_df = pd.DataFrame(results)\n","results_df.set_index('model', inplace=True)\n","results_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_df.sort_values(by='mean_test_neg_root_mean_squared_error', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_df.sort_values(by='mean_test_r2', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["[(result['model'], result['params']) for result in results], cv_results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_rank = results_df[['mean_test_neg_root_mean_squared_error']].rank(numeric_only=True, ascending=False)\n","dataset_rank = dataset_rank.assign(Mean=dataset_rank.mean(1), Stddev=dataset_rank.std(1))\n","dataset_rank"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# models[\"MLP 5\"].best_estimator_.loss_curve_\n","# models[\"MLP 5\"].best_estimator_.intercepts_\n","# models[\"MLP_2 5\"].best_estimator_.coefs_\n","# models[\"MLP_2 5\"].best_estimator_.loss_curve_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","\n","for model_name, model in models.items():\n","    if hasattr(model.best_estimator_, 'loss_curve_'):\n","        ax1.plot(model.best_estimator_.loss_curve_, label=model_name)\n","        ax2.plot(model.best_estimator_.loss_curve_, label=model_name)\n","\n","ax1.set_xlabel('Iterations')\n","ax1.set_ylabel('Loss')\n","ax1.set_title('All models')\n","ax1.legend()\n","ax1.grid(True)\n","\n","ax2.set_yscale('log')\n","ax2.set_xlabel('Iterations')\n","ax2.set_ylabel('Loss')\n","ax2.set_title('All models')\n","ax2.legend()\n","ax2.grid(True)\n","\n","fig.tight_layout()\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plot_groups = {}\n","for model_name, model in models.items():\n","    if hasattr(model.best_estimator_, 'loss_curve_'):\n","        key = model_name.split('_' if '_' in model_name else ' ')[0]\n","        if key not in plot_groups:\n","            plot_groups[key] = plt.subplots(1, 2, figsize=(14, 6))\n","        fig, (ax1, ax2) = plot_groups[key]\n","        ax1.plot(model.best_estimator_.loss_curve_, label=model_name)\n","        ax2.plot(model.best_estimator_.loss_curve_, label=model_name)\n","\n","for group, plot in plot_groups.items():\n","    fig, (ax1, ax2) = plot\n","    ax1.set_xlabel('Iterations')\n","    ax1.set_ylabel('Loss')\n","    ax1.set_title(group)\n","    ax1.legend()\n","    ax1.grid(True)\n","\n","    ax2.set_yscale('log')\n","    ax2.set_xlabel('Iterations')\n","    ax2.set_ylabel('Loss')\n","    ax2.set_title(group)\n","    ax2.legend()\n","    ax2.grid(True)\n","\n","    fig.tight_layout()\n","    fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","\n","def plot_loss(model, title='Loss Curve'):\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","    ax1.plot(model.loss_curve_)\n","    ax1.set_xlabel('Iterations')\n","    ax1.set_ylabel('Loss')\n","    ax1.set_title(title)\n","    ax1.grid(True)\n","\n","    ax2.plot(model.loss_curve_)\n","    ax2.set_yscale('log')\n","    ax2.set_xlabel('Iterations')\n","    ax2.set_ylabel('Loss')\n","    ax2.set_title(title)\n","    ax2.grid(True)\n","\n","    fig.tight_layout()\n","    fig.show()\n","\n","\n","for model_name, model in models.items():\n","    if hasattr(model.best_estimator_, 'loss_curve_'):\n","        plot_loss(model.best_estimator_, model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"research","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
