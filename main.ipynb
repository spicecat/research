{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNSyFZvf0Leo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Check if the environment is activated\n",
    "if \"CONDA_DEFAULT_ENV\" in os.environ:\n",
    "    print(f\"Environment '{os.environ['CONDA_DEFAULT_ENV']}' is activated.\")\n",
    "else:\n",
    "    print(\"No specific environment is activated.\")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the Boston dataset\n",
    "# dataset = \"boston\"\n",
    "# raw_df = pd.read_csv(\"data/boston.csv\")\n",
    "# target = [\"MEDV\"]\n",
    "# features = raw_df.columns.drop(target)\n",
    "# target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the store sales dataset\n",
    "# dataset = \"store_sales\"\n",
    "# excel_file = pd.ExcelFile(\"data/store_sales.xlsx\")\n",
    "# sheet_names = excel_file.sheet_names\n",
    "\n",
    "# # Read the data\n",
    "# raw_df = pd.read_excel(excel_file, sheet_name=sheet_names[2])  # 2, 9\n",
    "# iri_key_counts = raw_df[\"IRI_KEY\"].value_counts()\n",
    "# iri_keys = iri_key_counts[iri_key_counts > 300].index\n",
    "\n",
    "\n",
    "# target = [\"Total.Volume\"]\n",
    "# features = [\"F\", \"D\", \"Unit.Price\"]\n",
    "\n",
    "# raw_df = raw_df[raw_df[\"IRI_KEY\"] == iri_keys[0]]\n",
    "\n",
    "# sheet_names, iri_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "dataset = \"california\"\n",
    "housing = fetch_california_housing()\n",
    "target = housing.target_names\n",
    "features = housing.feature_names\n",
    "raw_df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(housing.data, columns=housing.feature_names),\n",
    "        pd.DataFrame(housing.target, columns=housing.target_names),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the soybean dataset\n",
    "# dataset = \"soybean\"\n",
    "# raw_df = pd.read_excel(\"data/soybean.xlsx\")\n",
    "# # X = raw_df.values[:-1, [5, 6, 15, 16, 17, 26,\n",
    "# #                         34, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]]  # 9 check yield 12 rm band\n",
    "# # y = raw_df.values[:-1, 11]\n",
    "# X = raw_df.iloc[:-1, [5, 6, 15, 16, 17, 26,\n",
    "#                   34, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]]\n",
    "# y = raw_df.iloc[:-1, [11]]\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load LengthOfStay\n",
    "# dataset = \"LengthOfStay\"\n",
    "# raw_df = pd.read_csv(\"data/LengthOfStay.csv\")\n",
    "# raw_df = raw_df.drop(columns=[\"eid\", \"vdate\", \"discharged\"])\n",
    "# target = [\"lengthofstay\"]\n",
    "# features = raw_df.columns.drop(target)\n",
    "# target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load HospitalStay\n",
    "# dataset = \"HospitalStay\"\n",
    "# raw_df = pd.read_csv(\"data/Healthcare_Investments_and_Hospital_Stay.csv\")\n",
    "# target = [\"Hospital_Stay\"]\n",
    "# features = raw_df.columns.drop(target)\n",
    "# target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "output_folder = f\"output/{dataset}_{time.strftime('%F_%T')}\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "if not os.path.exists(f\"{output_folder}/models\"):\n",
    "    os.makedirs(f\"{output_folder}/models\")\n",
    "\n",
    "X = raw_df[features]\n",
    "y = raw_df[target]\n",
    "\n",
    "display(f\"output: {output_folder}\", X.describe(), y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Outliers\n",
    "# def remove_outliers(df, threshold=3):\n",
    "#     z_scores = np.abs((df - df.mean()) / df.std())\n",
    "#     return df[(z_scores < threshold).all(axis=1)]\n",
    "# filtered_train_data = train_data\n",
    "# for col in train_data.columns:\n",
    "#     value_counts = train_data[col].value_counts().sort(by=\"count\")\n",
    "#     valid = value_counts.filter(pl.col(\"count\") > value_counts[\"count\"].max()/len(value_counts))[col]\n",
    "#     filtered_train_data = filtered_train_data.filter(pl.col(col).is_in(valid))\n",
    "# display(filtered_train_data, filtered_train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_data(X, y, cols=4):\n",
    "    fig, axs = plt.subplots(\n",
    "        (X.shape[1]+X.shape[1]*y.shape[1] + y.shape[1]+cols-1)//cols, cols, figsize=(20, 15))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    a = 0\n",
    "    for i, col in enumerate(X.columns):\n",
    "        data = X[col].to_numpy()\n",
    "        ax = axs[a+i]\n",
    "        sns.histplot(data, kde=True, ax=ax)\n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel(col)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\")\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\")\n",
    "    a += i + 1\n",
    "    for i, col in enumerate(X.columns):\n",
    "        data = X[col].to_numpy()\n",
    "        for j, ycol in enumerate(y.columns):\n",
    "            ax = axs[a+i+j*y.shape[1]]\n",
    "            sns.scatterplot(x=data, y=y[ycol], ax=ax)\n",
    "            ax.set_title(f\"{col} vs {ycol}\")\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel(ycol)\n",
    "    a += i + 1\n",
    "    for i, col in enumerate(y.columns):\n",
    "        data = y[col].to_numpy()\n",
    "        ax = axs[a+i]\n",
    "        sns.histplot(data, kde=True, ax=ax)\n",
    "        ax.set_title(col)\n",
    "    a += i+1\n",
    "    for j in range(a, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "    \n",
    "fig = plot_data(X, y)\n",
    "fig.savefig(f\"{output_folder}/data.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search\n",
    "from optuna.integration.sklearn import OptunaSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    TargetEncoder,\n",
    ")\n",
    "\n",
    "CATEGORICAL_PREPROCESSORS = {\n",
    "    \"drop\": \"drop\",\n",
    "    \"ordinal\": OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "    \"one_hot\": OneHotEncoder(\n",
    "        handle_unknown=\"ignore\", max_categories=20, sparse_output=False\n",
    "    ),\n",
    "    \"target\": TargetEncoder(target_type=\"continuous\"),\n",
    "}\n",
    "\n",
    "SCALERS = {\n",
    "    \"identity\": None,\n",
    "    \"standard\": StandardScaler(),\n",
    "    \"minmax\": MinMaxScaler(),\n",
    "    \"robust\": RobustScaler(),\n",
    "    \"quantile\": QuantileTransformer(),\n",
    "}\n",
    "\n",
    "search_params = {\n",
    "    \"cv\": 5,\n",
    "    \"n_jobs\": -1,  # -1,\n",
    "    \"n_trials\": 10, # 1\n",
    "    # \"n_trials\": None,\n",
    "    \"random_state\": 42,\n",
    "    \"return_train_score\": True,\n",
    "    \"scoring\": \"neg_mean_squared_error\",  # r2\n",
    "    # \"timeout\": 10,\n",
    "    # \"timeout\": None,\n",
    "    \"verbose\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "def search(\n",
    "    model,\n",
    "    param_grid={},\n",
    "    categorical_preprocessor=\"drop\",\n",
    "    scaler=\"identity\",\n",
    "    search_params=search_params,\n",
    "):\n",
    "    search_params = search_params.copy()\n",
    "    # search_params[\"n_trials\"] = int(4**len(param_grid))\n",
    "\n",
    "    numerical_features = X.select_dtypes(include=[\"number\"]).columns\n",
    "    categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        [\n",
    "            (\"numerical\", \"passthrough\", numerical_features),\n",
    "            (\n",
    "                \"categorical\",\n",
    "                CATEGORICAL_PREPROCESSORS[categorical_preprocessor],\n",
    "                categorical_features,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return OptunaSearchCV(\n",
    "        Pipeline(\n",
    "            [\n",
    "                (\"categorical_preprocessor\", preprocessor),\n",
    "                (\"scaler\", SCALERS[scaler]),\n",
    "                (\"model\", model),\n",
    "            ]\n",
    "        ),\n",
    "        {f\"model__{k}\": v for k, v in param_grid.items()},\n",
    "        **search_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search grids\n",
    "from optuna.distributions import (\n",
    "    CategoricalDistribution,\n",
    "    FloatDistribution,\n",
    "    IntDistribution,\n",
    ")\n",
    "\n",
    "mlp_sk_param_grid = {\n",
    "    \"learning_rate_init\": FloatDistribution(1e-2, 1e-1, log=True),\n",
    "    \"max_iter\": IntDistribution(400, 400, log=True),\n",
    "    \"early_stopping\": CategoricalDistribution([True]),\n",
    "    \"n_iter_no_change\": IntDistribution(20,20)\n",
    "}\n",
    "\n",
    "# fonn1_sk_param_grid = {\n",
    "#     \"learning_rate_init\": FloatDistribution(1e-2, 1e-1, log=True),\n",
    "#     \"max_iter\": IntDistribution(400, 400, log=True),\n",
    "# }\n",
    "\n",
    "# fonn2_sk_param_grid = {\n",
    "#     \"learning_rate_init\": FloatDistribution(1e-2, 1e-1, log=True),\n",
    "#     \"max_iter\": IntDistribution(400, 400, log=True),\n",
    "# }\n",
    "\n",
    "mlp_TrANN_param_grid = {\n",
    "    \"learning_rate_init\": FloatDistribution(1e-2, 1e-1, log=True),\n",
    "    \"max_iter\": IntDistribution(400, 400, log=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models import MLP, FONN1, FONN2, TREENN1, TREENN2\n",
    "from models_sklearn import (\n",
    "    Tree,\n",
    "    Ensemble,\n",
    "    MLP as MLP_sk,\n",
    "    FONN1 as FONN1_sk,\n",
    "    FONN2 as FONN2_sk,\n",
    "    TREENN1 as TREENN1_sk,\n",
    "    TREENN2 as TREENN2_sk,\n",
    ")\n",
    "from models_TrANN import (\n",
    "    FONN1 as FONN1_TrANN,\n",
    "    FONN2 as FONN2_TrANN,\n",
    "    FONN3 as FONN3_TrANN,\n",
    "    TREENN1 as TREENN1_TrANN,\n",
    "    TREENN2 as TREENN2_TrANN,\n",
    "    TREENN3 as TREENN3_TrANN,\n",
    ")\n",
    "\n",
    "models = {}\n",
    "\n",
    "num_trees_input = 5\n",
    "num_trees_hidden = 5\n",
    "hidden_nodes = [10]\n",
    "# hidden_nodes = [5, 10]\n",
    "\n",
    "scalers = [\"standard\"]\n",
    "categorical_preprocessor = [\"target\"]\n",
    "\n",
    "models[\"Tree\"] = search(Tree())\n",
    "for hn in hidden_nodes:\n",
    "    models[f\"Ensemble_sk {hn}\"] = search(Ensemble(hn))\n",
    "    models[f\"MLP_sk {hn}\"] = search(MLP_sk(hn), mlp_sk_param_grid)\n",
    "    models[f\"FONN1_sk {num_trees_input} {hn}\"] = search(\n",
    "        FONN1_sk(num_trees_input, num_trees_input + hn), mlp_sk_param_grid\n",
    "    )\n",
    "    models[f\"FONN2_sk {num_trees_hidden} {hn}\"] = search(\n",
    "        FONN2_sk(hn, num_trees_hidden + hn), mlp_sk_param_grid\n",
    "    )\n",
    "for hn in hidden_nodes:\n",
    "    for c in categorical_preprocessor:\n",
    "        for s in scalers:\n",
    "            models[f\"MLP_sk_{c}_{s} {hn}\"] = search(MLP_sk(hn), mlp_sk_param_grid, c, s)\n",
    "            models[f\"FONN1_sk_{c}_{s} {num_trees_input} {hn}\"] = search(\n",
    "                FONN1_sk(num_trees_input, num_trees_input + hn),\n",
    "                mlp_sk_param_grid,\n",
    "                c,\n",
    "                s,\n",
    "            )\n",
    "            models[f\"FONN2_sk_{c}_{s} {num_trees_hidden} {hn}\"] = search(\n",
    "                FONN2_sk(hn, num_trees_hidden + hn), mlp_sk_param_grid, c, s\n",
    "           )\n",
    "\n",
    "for hn in hidden_nodes:\n",
    "    models[f\"FONN1_TrANN {num_trees_input} {hn}\"] = search(\n",
    "        FONN1_TrANN(hn, num_trees_input), mlp_TrANN_param_grid\n",
    "    )\n",
    "    models[f\"FONN2_TrANN {num_trees_hidden} {hn}\"] = search(\n",
    "        FONN2_TrANN(hn, num_trees_hidden), mlp_TrANN_param_grid\n",
    "    )\n",
    "    models[f\"FONN3_TrANN {num_trees_hidden} {hn}\"] = search(\n",
    "        FONN3_TrANN(hn, num_trees_hidden), mlp_TrANN_param_grid\n",
    "    )\n",
    "    models[f\"TREENN1_TrANN {hn}\"] = search(\n",
    "        TREENN1_TrANN(hn), mlp_TrANN_param_grid\n",
    "    )\n",
    "    models[f\"TREENN2_TrANN {hn}\"] = search(\n",
    "        TREENN2_TrANN(hn), mlp_TrANN_param_grid\n",
    "    )\n",
    "    models[f\"TREENN3_TrANN {hn}\"] = search(\n",
    "        TREENN3_TrANN(hn), mlp_TrANN_param_grid\n",
    "    )\n",
    "\n",
    "display(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train and evaluate models\n",
    "cv_results = {}\n",
    "results = []\n",
    "\n",
    "result_columns = [\n",
    "    \"model\",\n",
    "    \"mean_fit_time\",\n",
    "    \"mean_score_time\",\n",
    "    \"mean_train_score\",\n",
    "    \"mean_test_score\",\n",
    "    \"mse\",\n",
    "]\n",
    "\n",
    "\n",
    "def fit_model(name, model, X, y):\n",
    "    print(f\"Fitting {name}...\")\n",
    "    model.fit(X, y.to_numpy().ravel())\n",
    "    result = model.cv_results_\n",
    "    cv_results[name] = result\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        **{col: result[col][model.best_index_] for col in result},\n",
    "        \"mse\": mean_squared_error(y, model.predict(X)),\n",
    "    }\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    results.append(fit_model(name, model, X, y))\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results = results[result_columns]\n",
    "results.to_csv(f\"{output_folder}/results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    results,\n",
    "    results.sort_values(by=\"mean_test_score\", ascending=False),\n",
    "    results.sort_values(by=\"mean_train_score\", ascending=False),\n",
    "    results.sort_values(by=\"mse\", ascending=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({name: model.best_params_ for name,\n",
    "             model in models.items()}).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "\n",
    "predictions = pd.DataFrame(\n",
    "    {name: model.best_estimator_.predict(X).ravel() for name, model in models.items()}\n",
    ")\n",
    "predictions = pd.concat([y, predictions], axis=1)\n",
    "predictions.to_csv(f\"{output_folder}/predictions.csv\", index=False)\n",
    "\n",
    "display(predictions, predictions.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Predictions\n",
    "fig = plot_data(predictions, y)\n",
    "fig.savefig(f\"{output_folder}/predictions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all models\n",
    "\n",
    "def plot_loss(model, ax1, ax2, label):\n",
    "    ax1.plot(model.loss_curve_, label=label)\n",
    "    ax1.set_xlabel(\"Iterations\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(model.loss_curve_, label=label)\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.set_xlabel(\"Iterations\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_[\"model\"], \"loss_curve_\"):\n",
    "        plot_loss(model.best_estimator_[\"model\"], ax1, ax2, label=model_name)\n",
    "\n",
    "ax1.set_xlabel(\"Iterations\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"All models\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_xlabel(\"Iterations\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"All models\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"{output_folder}/models/models.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model groups\n",
    "\n",
    "plot_groups = {}\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_[\"model\"], \"loss_curve_\"):\n",
    "        key = model_name.split(\"_\" if \"_\" in model_name else \" \")[0]\n",
    "        if key not in plot_groups:\n",
    "            plot_groups[key] = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig, (ax1, ax2) = plot_groups[key]\n",
    "        plot_loss(model.best_estimator_[\"model\"], ax1, ax2, model_name)\n",
    "\n",
    "for group, plot in plot_groups.items():\n",
    "    fig, (ax1, ax2) = plot\n",
    "    ax1.set_xlabel(\"Iterations\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.set_title(group)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.set_xlabel(\"Iterations\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.set_title(group)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{output_folder}/models/group_{group}.png\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual models\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    if hasattr(model.best_estimator_[\"model\"], \"loss_curve_\"):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        plot_loss(model.best_estimator_[\"model\"], ax1, ax2, model_name)\n",
    "        ax1.set_title(model_name)\n",
    "        ax2.set_title(model_name)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{output_folder}/models/{model_name}.png\")\n",
    "        fig.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
