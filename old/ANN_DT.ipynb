{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJZN0JY1HQVz"
      },
      "source": [
        "To integrate decision trees into the hidden layer of the neural network, we use them as additional \"neurons\" that process the activations from the standard hidden neurons. Here's a detailed explanation and a step-by-step breakdown of how decision trees are used in this architecture:\n",
        "\n",
        "### Integrating Decision Trees into the Hidden Layer\n",
        "\n",
        "1. **Input Layer**:\n",
        "   - Number of Neurons: 13 (corresponding to the 13 features of the dataset).\n",
        "\n",
        "2. **First Part of the Hidden Layer**:\n",
        "   - Number of Neurons: 10\n",
        "   - Each neuron applies a Tanh activation function.\n",
        "\n",
        "3. **Decision Trees in the Hidden Layer**:\n",
        "   - Number of Decision Trees: 10\n",
        "   - Each decision tree is trained on the activations from the 10 hidden neurons.\n",
        "   - The decision trees output predictions based on the hidden neuron activations.\n",
        "\n",
        "4. **Combined Hidden Layer**:\n",
        "   - The outputs from the 10 hidden neurons and the 10 decision trees are combined into a single layer with 20 neurons.\n",
        "\n",
        "5. **Output Layer**:\n",
        "   - Number of Neurons: 1 (corresponding to the predicted house price).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZTAxwOYJcJh"
      },
      "source": [
        "Let's focus on how to integrate decision trees into the hidden layer of the neural network. We'll use decision trees to process the activations from the hidden neurons and then combine their outputs with the original activations before passing them to the next layer.\n",
        "\n",
        "### Steps to Integrate Decision Trees\n",
        "\n",
        "1. **Forward Pass**:\n",
        "   - Compute the activations from the hidden neurons using a Tanh activation function.\n",
        "   - Use these activations as input features to train decision trees.\n",
        "   - The decision trees output predictions based on the activations.\n",
        "   - Combine the original activations with the decision tree outputs to form the combined hidden layer.\n",
        "\n",
        "2. **Backward Pass**:\n",
        "   - Calculate the error between the predicted output and the true target values.\n",
        "   - Compute gradients and update the weights using backpropagation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3tDfRT1I2H1"
      },
      "source": [
        "Updating the weights in a neural network, including the custom architecture with decision trees in the hidden layer, involves backpropagation. Let's break down how we update the weights in this architecture:\n",
        "\n",
        "### Steps to Update Weights\n",
        "\n",
        "1. **Forward Pass**: Compute the activations for all layers, including the outputs from the decision trees.\n",
        "2. **Compute Error**: Calculate the error between the predicted output and the true target values.\n",
        "3. **Backward Pass**: Propagate the error backward through the network to compute gradients for the weights.\n",
        "4. **Gradient Clipping**: Clip gradients to prevent them from becoming too large (optional but often useful).\n",
        "5. **Update Weights**: Adjust the weights using the computed gradients and the learning rate.\n",
        "\n",
        "### Explanation of Each Step\n",
        "\n",
        "1. **Forward Pass**:\n",
        "    - Compute the pre-activation \\( z1 \\) for the hidden neurons.\n",
        "    - Apply the Tanh activation function to get \\( a1 \\).\n",
        "    - Train the decision trees using \\( a1 \\) and the target labels \\( y \\). The decision trees' predictions are concatenated with \\( a1 \\) to form the combined hidden layer.\n",
        "    - Compute the final output \\( z2 \\) using the combined hidden layer.\n",
        "\n",
        "2. **Compute Error**:\n",
        "    - Calculate the error between the network's output and the true labels.\n",
        "\n",
        "3. **Backward Pass**:\n",
        "\n",
        "    - Compute the gradients for the weights and biases in the output layer:\n",
        "\n",
        "        $$\n",
        "        d\\_weights2 = \\frac{\\partial \\text{Loss}}{\\partial \\text{weights2}} = \\frac{\\text{combined\\_hidden}^T \\cdot \\text{output\\_error}}{X.shape[0]}\n",
        "        $$\n",
        "        $$\n",
        "        d\\_bias2 = \\frac{\\partial \\text{Loss}}{\\partial \\text{bias2}} = \\text{mean}(\\text{output\\_error}, \\text{axis}=0)\n",
        "        $$\n",
        "\n",
        "    - Compute the error for the hidden layer and its gradients:\n",
        "        $$\n",
        "        \\text{hidden\\_error} = (\\text{output\\_error} \\cdot \\text{weights2}[:\\text{hidden\\_dim}].T) \\cdot (1 - a1^2)\n",
        "        $$\n",
        "        $$\n",
        "        d\\_weights1 = \\frac{\\partial \\text{Loss}}{\\partial \\text{weights1}} = \\frac{X^T \\cdot \\text{hidden\\_error}}{X.shape[0]}\n",
        "        $$\n",
        "        $$\n",
        "        d\\_bias1 = \\frac{\\partial \\text{Loss}}{\\partial \\text{bias1}} = \\text{mean}(\\text{hidden\\_error}, \\text{axis}=0)\n",
        "        $$\n",
        "\n",
        "4. **Gradient Clipping**:\n",
        "    - Optionally, clip the gradients to a maximum norm to prevent exploding gradients.\n",
        "\n",
        "5. **Update Weights and Biases**:\n",
        "    - Use the computed gradients and the learning rate to update the weights and biases:\n",
        "        $$\n",
        "        \\text{weights2} -= \\text{learning\\_rate} \\cdot d\\_weights2\n",
        "        $$\n",
        "        $$\n",
        "        \\text{bias2} -= \\text{learning\\_rate} \\cdot d\\_bias2\n",
        "        $$\n",
        "        $$\n",
        "        \\text{weights1} -= \\text{learning\\_rate} \\cdot d\\_weights1\n",
        "        $$\n",
        "        $$\n",
        "        \\text{bias1} -= \\text{learning\\_rate} \\cdot d\\_bias1\n",
        "        $$\n",
        "\n",
        "By following these steps, the weights and biases of the network, including those involved with the decision trees, are updated to minimize the prediction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manually load the Boston dataset\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "X = np.hstack([raw_df.values[::2, :-1], raw_df.values[1::2, :2]])\n",
        "y = raw_df.values[1::2, 2]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Ridge Regression\": Ridge(),\n",
        "    \"Lasso Regression\": Lasso(),\n",
        "    \"ElasticNet Regression\": ElasticNet(),\n",
        "    \"Support Vector Regression\": SVR(),\n",
        "    \"MLP Regressor\": MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n",
        "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting Regressor\": GradientBoostingRegressor(random_state=42),\n",
        "    \"XGBoost Regressor\": XGBRegressor(random_state=42)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to train and evaluate a model\n",
        "def train_evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    return r2, mae\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    r2, mae = train_evaluate_model(model, X_train, X_test, y_train, y_test)\n",
        "    results[name] = {\"R² Score\": r2, \"MAE\": mae}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom MLP with 10 trees in hidden layer\n",
        "class MLPWithDecisionTrees:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_trees_hidden):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_trees_hidden = num_trees_hidden\n",
        "\n",
        "        # Initialize weights and biases for the first hidden layer\n",
        "        self.weights1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
        "        self.bias1 = np.zeros(hidden_dim)\n",
        "\n",
        "        # Initialize weights and biases for the output layer\n",
        "        self.weights2 = np.random.randn(hidden_dim + num_trees_hidden, output_dim) * 0.01\n",
        "        self.bias2 = np.zeros(output_dim)\n",
        "\n",
        "        # Initialize decision trees for the hidden layer\n",
        "        self.trees_hidden = [DecisionTreeRegressor(max_depth=5, random_state=i) for i in range(num_trees_hidden)]\n",
        "\n",
        "    def forward(self, X, y_batch):\n",
        "        # Compute hidden layer activations\n",
        "        self.z1 = np.dot(X, self.weights1) + self.bias1\n",
        "        self.a1 = np.tanh(self.z1)  # Tanh activation\n",
        "\n",
        "        # Generate tree outputs for the hidden layer using batch labels\n",
        "        hidden_tree_outputs = np.column_stack([tree.fit(self.a1, y_batch).predict(self.a1) for tree in self.trees_hidden])\n",
        "        self.combined_hidden = np.hstack((self.a1, hidden_tree_outputs))\n",
        "\n",
        "        # Compute output layer activations\n",
        "        self.z2 = np.dot(self.combined_hidden, self.weights2) + self.bias2\n",
        "        return self.z2  # Linear output\n",
        "\n",
        "    def backward(self, X, y, output, learning_rate):\n",
        "        # Compute the error between the output and the true labels\n",
        "        output_error = output - y.reshape(-1, 1)\n",
        "\n",
        "        # Compute gradients for the weights and biases of the output layer\n",
        "        d_weights2 = np.dot(self.combined_hidden.T, output_error) / X.shape[0]\n",
        "        d_bias2 = np.mean(output_error, axis=0)\n",
        "\n",
        "        # Compute hidden layer error and gradients\n",
        "        hidden_error = np.dot(output_error, self.weights2[:self.hidden_dim].T) * (1 - self.a1 ** 2)  # Tanh derivative\n",
        "        d_weights1 = np.dot(X.T, hidden_error) / X.shape[0]\n",
        "        d_bias1 = np.mean(hidden_error, axis=0)\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        max_grad_norm = 1.0\n",
        "        d_weights2 = np.clip(d_weights2, -max_grad_norm, max_grad_norm)\n",
        "        d_bias2 = np.clip(d_bias2, -max_grad_norm, max_grad_norm)\n",
        "        d_weights1 = np.clip(d_weights1, -max_grad_norm, max_grad_norm)\n",
        "        d_bias1 = np.clip(d_bias1, -max_grad_norm, max_grad_norm)\n",
        "\n",
        "        # Update weights and biases using gradient descent\n",
        "        self.weights2 -= learning_rate * d_weights2\n",
        "        self.bias2 -= learning_rate * d_bias2\n",
        "        self.weights1 -= learning_rate * d_weights1\n",
        "        self.bias1 -= learning_rate * d_bias1\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X, y)\n",
        "            self.backward(X, y, output, learning_rate)\n",
        "            loss = np.mean((output - y.reshape(-1, 1)) ** 2)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xi4kHCowFjq2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 611.8313057991296\n",
            "Epoch 100, Loss: 12.909873483945182\n",
            "Epoch 200, Loss: 13.07198808020817\n",
            "Epoch 300, Loss: 12.16671062980413\n",
            "Epoch 400, Loss: 11.629635180695681\n",
            "Epoch 500, Loss: 9.889585188286981\n",
            "Epoch 600, Loss: 10.09015510887839\n",
            "Epoch 700, Loss: 8.79987603012809\n",
            "Epoch 800, Loss: 10.582802360832517\n",
            "Epoch 900, Loss: 10.331503681976471\n"
          ]
        }
      ],
      "source": [
        "# Train MLPWithDecisionTrees\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 10\n",
        "output_dim = 1\n",
        "num_trees_hidden = 10\n",
        "epochs = 1000\n",
        "learning_rate = 0.001\n",
        "\n",
        "custom_mlp = MLPWithDecisionTrees(input_dim, hidden_dim, output_dim, num_trees_hidden)\n",
        "custom_mlp.train(X_train, y_train, epochs, learning_rate)\n",
        "custom_predictions = custom_mlp.forward(X_test, y_test)\n",
        "custom_r2 = r2_score(y_test, custom_predictions)\n",
        "custom_mae = mean_absolute_error(y_test, custom_predictions)\n",
        "\n",
        "results[\"Custom MLP with Trees\"] = {\"R² Score\": custom_r2, \"MAE\": custom_mae}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             R² Score       MAE\n",
            "Linear Regression            0.629049  3.530902\n",
            "Ridge Regression             0.628946  3.527434\n",
            "Lasso Regression             0.583928  3.797634\n",
            "ElasticNet Regression        0.576617  3.688701\n",
            "Support Vector Regression    0.647374  2.798191\n",
            "MLP Regressor                0.754212  3.019009\n",
            "Random Forest Regressor      0.886802  2.115863\n",
            "Gradient Boosting Regressor  0.917226  1.899354\n",
            "XGBoost Regressor            0.900123  1.931674\n",
            "Custom MLP with Trees        0.964352  1.116360\n"
          ]
        }
      ],
      "source": [
        "# Convert results to a DataFrame for better visualization\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
