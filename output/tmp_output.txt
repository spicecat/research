0 [indus < 9.655] score=160.699, samples=15, value=22.460
  1 [zn < 26.500] score=16.304, samples=7, value=27.857
    2 score=16.007, samples=3, value=24.233
    2 score=16.528, samples=4, value=30.575
  1 [crim < 4.873] score=99.461, samples=8, value=17.738
    2 [crim < 1.341] score=10.080, samples=6, value=18.233
      3 score=2.880, samples=2, value=13.100
      3 score=13.680, samples=4, value=20.800
    2 score=111.005, samples=2, value=16.250
Variables used:
indus zn crim
Number of terminal nodes: 5
Residual mean deviance: 39.662
Samples: 15
Sum of squared residuals: 396.620

n iterations (60000)
hidden layers (1)
nodes in hidden layers (2)
activation function (sigmoid)
batch size (64, n/100)
epochs
https://colab.research.google.com/drive/1d8dpSOooMQ9DL1l8sjl2Il1Z2dHs3mm4?usp=sharing#scrollTo=506gxrI4Qo_O
stephen ryan, guofu zhou
bootstrapping(data, B): B trees; Add B columns to dataset
random sampling with replacement
SHAP Value: how y1 and y2 impact y
batch normalization
https://www.datacamp.com/tutorial/neural-network-models-r
https://rviews.rstudio.com/2020/07/20/shallow-neural-net-from-scratch-using-r-part-1/


import keras
import pandas as pd


if __name__ == "__main__":
    from sklearn.datasets import load_diabetes
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.utils import Bunch

    # Load the diabetes dataset
    diabetes: Bunch = load_diabetes()  # type: ignore

    # Standardize the features
    scaler = StandardScaler()
    X, y = scaler.fit_transform(diabetes.data), diabetes.target

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)
    
    print(X_train)

    diabetes = pd.read_csv('data/diabetes.csv')
    print("Diabetes:\n", diabetes.head())

    train_df = diabetes.sample(frac=0.75, random_state=4)

    val_df = diabetes.drop(train_df.index)

    max_val = train_df.max(axis=0)
    min_val = train_df.min(axis=0)

    range = max_val - min_val
    train_df = (train_df - min_val)/(range)

    val_df = (val_df - min_val)/range

    # now let's separate the targets and labels
    X_train = train_df.drop('target', axis=1)
    X_val = val_df.drop('target', axis=1)
    y_train = train_df['target']
    y_val = val_df['target']

    print("X_train:\n", X_train.head())
    print("X_val:\n", X_val.head())
    print("y_train:\n", y_train.head())
    print("y_val:\n", y_val.head())

    # We'll need to pass the shape
    # of features/inputs as an argument
    # in our model, so let's define a variable
    # to save it.
    input_shape = [X_train.shape[1]]

    print(input_shape)

    model = keras.Sequential([

        keras.layers.Dense(units=64, activation='relu',
                           input_shape=input_shape),
        keras.layers.Dense(units=64, activation='relu'),
        keras.layers.Dense(units=1)
    ])
    model.summary()

    # adam optimizer works pretty well for
    # all kinds of problems and is a good starting point
    model.compile(optimizer='adam',

                  # MAE error is good for
                  # numerical predictions
                  loss='mae')

    losses = model.fit(X_train, y_train,

                       validation_data=(X_val, y_val),

                       # it will use 'batch_size' number
                       # of examples per example
                       batch_size=256,
                       epochs=15,  # total epoch

                       )

    # this will pass the first 3 rows of features
    # of our data as input to make predictions
    print(model.predict(X_val.iloc[0:3, :]))
    print(y_val.iloc[0:3])

    loss_df = pd.DataFrame(losses.history)

    # history stores the loss/val
    # loss in each epoch

    # loss_df is a dataframe which
    # contains the losses so we can
    # plot it to visualize our model training
    loss_df.loc[:, ['loss', 'val_loss']].plot()
