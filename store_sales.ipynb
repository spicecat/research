{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yNSyFZvf0Leo"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import time\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, ARDRegression, SGDRegressor, PassiveAggressiveRegressor\n","from sklearn.svm import SVR\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor, StackingRegressor, VotingRegressor\n","from xgboost import XGBRegressor\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n","\n","from models import MLP, Ensemble, FONN1, FONN2, TREENN1, TREENN2\n","\n","np.random.seed(0)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["(['StoreWeekSalescarbbev_modify',\n","  'StoreWeekSalescigets',\n","  'StoreWeekSalescoffee',\n","  'StoreWeekSalescoldcer',\n","  'StoreWeekSalesdeod',\n","  'StoreWeekSalesdiapers',\n","  'StoreWeekSalesfactiss',\n","  'StoreWeekSalesfzdinent',\n","  'StoreWeekSalesfzpizza',\n","  'StoreWeekSaleshotdog',\n","  'StoreWeekSaleslaundet',\n","  'StoreWeekSalesmargbutr',\n","  'StoreWeekSalesmayo',\n","  'StoreWeekSalesmustketc',\n","  'StoreWeekSalespaptowl',\n","  'StoreWeekSalespeanbutr',\n","  'StoreWeekSalesshamp',\n","  'StoreWeekSalessoup',\n","  'StoreWeekSalesspagsauc',\n","  'StoreWeekSalessugarsub',\n","  'StoreWeekSalestoitisu',\n","  'StoreWeekSalestoothpa',\n","  'StoreWeekSalesyogurt',\n","  'StoreWeekSalesbeer_modify'],\n"," Index([6001821.0,  648368.0,  279300.0,  400003.0,  270862.0,  231720.0,\n","         252570.0,  659827.0,  241565.0,  237277.0,  291276.0,  273920.0,\n","         232633.0,  233246.0,  532639.0,  533864.0],\n","       dtype='float64', name='IRI_KEY'),\n"," (313, 3),\n"," (313, 1))"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Load the store sales dataset\n","excel_file = pd.ExcelFile('data/store_sales.xlsx')\n","sheet_names = excel_file.sheet_names\n","\n","# Read the data\n","df = pd.read_excel(excel_file, sheet_name=sheet_names[0])\n","iri_key_counts = df[\"IRI_KEY\"].value_counts()\n","iri_keys = iri_key_counts[iri_key_counts > 300].index\n","\n","\n","features = [\"F\", \"D\", \"Unit.Price\"]\n","target = \"Total.Volume\"\n","\n","df = df[df[\"IRI_KEY\"] == iri_keys[0]]\n","X = df[features]\n","y = df[target].values.reshape(-1, 1) # type: ignore\n","\n","scaler_X = StandardScaler()\n","X = scaler_X.fit_transform(X)\n","scaler_y = StandardScaler()\n","y = scaler_y.fit_transform(y)\n","\n","sheet_names, iri_keys, X.shape, y.shape"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["((250, 3), (63, 3), (250, 1), (63, 1))"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Split the dataset\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42)\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Function to train and evaluate a model\n","def train_evaluate_model(model, X_train, X_test, y_train, y_test):\n","    start_time = time.time()\n","    model.fit(X_train, y_train)\n","    end_time = time.time()\n","    train_time = end_time - start_time\n","\n","    start_time = time.time()\n","    predictions = model.predict(X_test)\n","    end_time = time.time()\n","    comp_time = end_time - start_time\n","\n","    r2 = r2_score(y_test, predictions)\n","    mae = mean_absolute_error(y_test, predictions)\n","    mse = mean_squared_error(y_test, predictions)\n","\n","    return r2, mae, mse, train_time, comp_time\n","\n","\n","# Initialize standard models\n","models = {\n","    \"Linear Regression\": LinearRegression(),\n","    \"Ridge Regression\": Ridge(),\n","    \"Lasso Regression\": Lasso(),\n","    \"ElasticNet Regression\": ElasticNet(),\n","    \"Bayesian Ridge Regression\": BayesianRidge(),\n","    \"ARD Regression\": ARDRegression(),\n","    \"SGD Regressor\": SGDRegressor(),\n","    \"Passive Aggressive Regressor\": PassiveAggressiveRegressor(),\n","    \"Support Vector Regression\": SVR(),\n","    \"MLP Regressor\": MLPRegressor(hidden_layer_sizes=(100,), max_iter=10000, random_state=42),\n","    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n","    \"Gradient Boosting Regressor\": GradientBoostingRegressor(random_state=42),\n","    \"XGBoost Regressor\": XGBRegressor(random_state=42),\n","    \"AdaBoost Regressor\": AdaBoostRegressor(random_state=42),\n","    \"Bagging Regressor\": BaggingRegressor(random_state=42),\n","    \"ExtraTrees Regressor\": ExtraTreesRegressor(random_state=42),\n","    \"HistGradientBoosting Regressor\": HistGradientBoostingRegressor(random_state=42),\n","    \"Stacking Regressor\": StackingRegressor(estimators=[\n","        ('lr', LinearRegression()),\n","        ('rf', RandomForestRegressor(n_estimators=10, random_state=42))\n","    ], final_estimator=Ridge()),\n","    \"Voting Regressor\": VotingRegressor(estimators=[\n","        ('lr', LinearRegression()),\n","        ('rf', RandomForestRegressor(n_estimators=10, random_state=42)),\n","        ('gb', GradientBoostingRegressor(random_state=42))\n","    ])\n","}\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Initialize custom models\n","input_dim = X_train.shape[1]\n","hidden_dim = 10\n","output_dim = 1\n","batch_size = 32\n","learning_rate = 0.0001\n","epochs = 50000\n","\n","# models[\"Custom MLP\"] = MLP(input_dim, hidden_dim, output_dim,\n","#                            batch_size=batch_size, learning_rate=learning_rate, epochs=epochs)\n","# #    learning_rate=0.01, epochs=1000)\n","\n","# num_trees_input = 10\n","# models[\"FONN1\"] = FONN1(input_dim, hidden_dim, output_dim, num_trees_input,\n","#                         batch_size=batch_size, learning_rate=learning_rate, epochs=epochs)\n","# # learning_rate=0.01, epochs=1000)\n","# models[\"Tree-based Predictions (FONN1)\"] = models[\"FONN1\"].trees\n","\n","# models[\"TREENN1\"] = TREENN1(input_dim, hidden_dim, output_dim,\n","#                             batch_size=batch_size, learning_rate=learning_rate, epochs=epochs)\n","# # learning_rate=0.01, epochs=40000)\n","# models[\"Tree-based Predictions (TREENN1)\"] = models[\"TREENN1\"].trees\n","\n","# num_trees_hidden = 10\n","# models[\"FONN2\"] = FONN2(input_dim, hidden_dim, output_dim, num_trees_hidden,\n","#                         batch_size=batch_size, learning_rate=learning_rate, epochs=epochs)\n","# # learning_rate=0.01, epochs=epochs)\n","# models[\"Tree-based Predictions (FONN2)\"] = models[\"FONN2\"].trees\n","\n","# models[\"TREENN2\"] = TREENN2(input_dim, hidden_dim, output_dim,\n","#                             batch_size=batch_size, learning_rate=learning_rate, epochs=epochs)\n","# # learning_rate=0.01, epochs=epochs)\n","# models[\"Tree-based Predictions (TREENN2)\"] = models[\"TREENN2\"].trees\n","\n","# models[\"Ensemble of 10 Trees\"] = Ensemble(10)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Linear Regression\n","Ridge Regression\n","Lasso Regression\n","ElasticNet Regression\n","Bayesian Ridge Regression\n","ARD Regression\n","SGD Regressor\n","Passive Aggressive Regressor\n","Support Vector Regression\n","MLP Regressor\n","Random Forest Regressor\n"]},{"name":"stderr","output_type":"stream","text":["/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  return fit_method(estimator, *args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Gradient Boosting Regressor\n","XGBoost Regressor\n","AdaBoost Regressor\n","Bagging Regressor\n","ExtraTrees Regressor\n"]},{"name":"stderr","output_type":"stream","text":["/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:668: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/ensemble/_bagging.py:505: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  return column_or_1d(y, warn=True)\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n","  return fit_method(estimator, *args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["HistGradientBoosting Regressor\n","Stacking Regressor\n","Voting Regressor\n"]},{"name":"stderr","output_type":"stream","text":["/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:967: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/ensemble/_voting.py:622: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>R² Score</th>\n","      <th>MAE</th>\n","      <th>MSE</th>\n","      <th>Train Time (s)</th>\n","      <th>Comp Time (s)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Linear Regression</th>\n","      <td>0.321274</td>\n","      <td>0.532588</td>\n","      <td>0.433517</td>\n","      <td>0.007542</td>\n","      <td>0.000182</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge Regression</th>\n","      <td>0.321294</td>\n","      <td>0.532445</td>\n","      <td>0.433504</td>\n","      <td>0.001237</td>\n","      <td>0.000109</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso Regression</th>\n","      <td>-0.017990</td>\n","      <td>0.639909</td>\n","      <td>0.650211</td>\n","      <td>0.000523</td>\n","      <td>0.000087</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet Regression</th>\n","      <td>-0.017990</td>\n","      <td>0.639909</td>\n","      <td>0.650211</td>\n","      <td>0.000478</td>\n","      <td>0.000068</td>\n","    </tr>\n","    <tr>\n","      <th>Bayesian Ridge Regression</th>\n","      <td>0.321181</td>\n","      <td>0.531217</td>\n","      <td>0.433576</td>\n","      <td>0.001394</td>\n","      <td>0.000110</td>\n","    </tr>\n","    <tr>\n","      <th>ARD Regression</th>\n","      <td>0.311469</td>\n","      <td>0.536107</td>\n","      <td>0.439779</td>\n","      <td>0.001498</td>\n","      <td>0.000107</td>\n","    </tr>\n","    <tr>\n","      <th>SGD Regressor</th>\n","      <td>0.318295</td>\n","      <td>0.533890</td>\n","      <td>0.435419</td>\n","      <td>0.001062</td>\n","      <td>0.000194</td>\n","    </tr>\n","    <tr>\n","      <th>Passive Aggressive Regressor</th>\n","      <td>-2.884998</td>\n","      <td>1.180568</td>\n","      <td>2.481429</td>\n","      <td>0.000637</td>\n","      <td>0.000103</td>\n","    </tr>\n","    <tr>\n","      <th>Support Vector Regression</th>\n","      <td>0.562097</td>\n","      <td>0.425470</td>\n","      <td>0.279698</td>\n","      <td>0.003708</td>\n","      <td>0.000793</td>\n","    </tr>\n","    <tr>\n","      <th>MLP Regressor</th>\n","      <td>0.476447</td>\n","      <td>0.473188</td>\n","      <td>0.334404</td>\n","      <td>0.115273</td>\n","      <td>0.000150</td>\n","    </tr>\n","    <tr>\n","      <th>Random Forest Regressor</th>\n","      <td>0.351392</td>\n","      <td>0.524572</td>\n","      <td>0.414280</td>\n","      <td>0.100366</td>\n","      <td>0.002524</td>\n","    </tr>\n","    <tr>\n","      <th>Gradient Boosting Regressor</th>\n","      <td>0.480069</td>\n","      <td>0.472652</td>\n","      <td>0.332091</td>\n","      <td>0.069462</td>\n","      <td>0.000454</td>\n","    </tr>\n","    <tr>\n","      <th>XGBoost Regressor</th>\n","      <td>0.330104</td>\n","      <td>0.519903</td>\n","      <td>0.427877</td>\n","      <td>0.043305</td>\n","      <td>0.001278</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoost Regressor</th>\n","      <td>0.367053</td>\n","      <td>0.527454</td>\n","      <td>0.404276</td>\n","      <td>0.019549</td>\n","      <td>0.001209</td>\n","    </tr>\n","    <tr>\n","      <th>Bagging Regressor</th>\n","      <td>0.227358</td>\n","      <td>0.546978</td>\n","      <td>0.493503</td>\n","      <td>0.016586</td>\n","      <td>0.000956</td>\n","    </tr>\n","    <tr>\n","      <th>ExtraTrees Regressor</th>\n","      <td>0.399943</td>\n","      <td>0.502703</td>\n","      <td>0.383269</td>\n","      <td>0.104622</td>\n","      <td>0.002846</td>\n","    </tr>\n","    <tr>\n","      <th>HistGradientBoosting Regressor</th>\n","      <td>0.418670</td>\n","      <td>0.498406</td>\n","      <td>0.371308</td>\n","      <td>0.068072</td>\n","      <td>0.000994</td>\n","    </tr>\n","    <tr>\n","      <th>Stacking Regressor</th>\n","      <td>0.337021</td>\n","      <td>0.527595</td>\n","      <td>0.423459</td>\n","      <td>0.088107</td>\n","      <td>0.000941</td>\n","    </tr>\n","    <tr>\n","      <th>Voting Regressor</th>\n","      <td>0.428354</td>\n","      <td>0.504405</td>\n","      <td>0.365122</td>\n","      <td>0.068625</td>\n","      <td>0.000897</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                R² Score       MAE       MSE  Train Time (s)  \\\n","Linear Regression               0.321274  0.532588  0.433517        0.007542   \n","Ridge Regression                0.321294  0.532445  0.433504        0.001237   \n","Lasso Regression               -0.017990  0.639909  0.650211        0.000523   \n","ElasticNet Regression          -0.017990  0.639909  0.650211        0.000478   \n","Bayesian Ridge Regression       0.321181  0.531217  0.433576        0.001394   \n","ARD Regression                  0.311469  0.536107  0.439779        0.001498   \n","SGD Regressor                   0.318295  0.533890  0.435419        0.001062   \n","Passive Aggressive Regressor   -2.884998  1.180568  2.481429        0.000637   \n","Support Vector Regression       0.562097  0.425470  0.279698        0.003708   \n","MLP Regressor                   0.476447  0.473188  0.334404        0.115273   \n","Random Forest Regressor         0.351392  0.524572  0.414280        0.100366   \n","Gradient Boosting Regressor     0.480069  0.472652  0.332091        0.069462   \n","XGBoost Regressor               0.330104  0.519903  0.427877        0.043305   \n","AdaBoost Regressor              0.367053  0.527454  0.404276        0.019549   \n","Bagging Regressor               0.227358  0.546978  0.493503        0.016586   \n","ExtraTrees Regressor            0.399943  0.502703  0.383269        0.104622   \n","HistGradientBoosting Regressor  0.418670  0.498406  0.371308        0.068072   \n","Stacking Regressor              0.337021  0.527595  0.423459        0.088107   \n","Voting Regressor                0.428354  0.504405  0.365122        0.068625   \n","\n","                                Comp Time (s)  \n","Linear Regression                    0.000182  \n","Ridge Regression                     0.000109  \n","Lasso Regression                     0.000087  \n","ElasticNet Regression                0.000068  \n","Bayesian Ridge Regression            0.000110  \n","ARD Regression                       0.000107  \n","SGD Regressor                        0.000194  \n","Passive Aggressive Regressor         0.000103  \n","Support Vector Regression            0.000793  \n","MLP Regressor                        0.000150  \n","Random Forest Regressor              0.002524  \n","Gradient Boosting Regressor          0.000454  \n","XGBoost Regressor                    0.001278  \n","AdaBoost Regressor                   0.001209  \n","Bagging Regressor                    0.000956  \n","ExtraTrees Regressor                 0.002846  \n","HistGradientBoosting Regressor       0.000994  \n","Stacking Regressor                   0.000941  \n","Voting Regressor                     0.000897  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Train and evaluate models\n","results = {}\n","for name, model in models.items():\n","    print(name)\n","    r2, mae, mse, fit_time, comp_time = train_evaluate_model(\n","        model, X_train, X_test, y_train, y_test)\n","    results[name] = {\"R² Score\": r2, \"MAE\": mae, \"MSE\": mse,\n","                     \"Train Time (s)\": fit_time, \"Comp Time (s)\": comp_time}\n","\n","# Convert results to a DataFrame for better visualization\n","results_df = pd.DataFrame(results).T\n","results_df"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Get and print tree importances\n","# tree_importances = models[\"FONN2\"].trees.get_tree_importances()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def test_models(X, y):\n","    scaler_X = StandardScaler()\n","    X = scaler_X.fit_transform(X)\n","    scaler_y = StandardScaler()\n","    y = scaler_y.fit_transform(y)\n","\n","    # Split the dataset\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2, random_state=42)\n","\n","    # Initialize standard models\n","    models = {\n","        \"Linear Regression\": LinearRegression(),\n","        \"Ridge Regression\": Ridge(),\n","        \"Lasso Regression\": Lasso(),\n","        \"ElasticNet Regression\": ElasticNet(),\n","        \"Bayesian Ridge Regression\": BayesianRidge(),\n","        \"ARD Regression\": ARDRegression(),\n","        \"SGD Regressor\": SGDRegressor(),\n","        \"Passive Aggressive Regressor\": PassiveAggressiveRegressor(),\n","        \"Support Vector Regression\": SVR(),\n","        \"MLP Regressor\": MLPRegressor(hidden_layer_sizes=(100,), max_iter=10000, random_state=42),\n","        \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n","        \"Gradient Boosting Regressor\": GradientBoostingRegressor(random_state=42),\n","        \"XGBoost Regressor\": XGBRegressor(random_state=42),\n","        \"AdaBoost Regressor\": AdaBoostRegressor(random_state=42),\n","        \"Bagging Regressor\": BaggingRegressor(random_state=42),\n","        \"ExtraTrees Regressor\": ExtraTreesRegressor(random_state=42),\n","        \"HistGradientBoosting Regressor\": HistGradientBoostingRegressor(random_state=42),\n","        \"Stacking Regressor\": StackingRegressor(estimators=[\n","            ('lr', LinearRegression()),\n","            ('rf', RandomForestRegressor(n_estimators=10, random_state=42))\n","        ], final_estimator=Ridge()),\n","        \"Voting Regressor\": VotingRegressor(estimators=[\n","            ('lr', LinearRegression()),\n","            ('rf', RandomForestRegressor(n_estimators=10, random_state=42)),\n","            ('gb', GradientBoostingRegressor(random_state=42))\n","        ])\n","    }\n","\n","    # Initialize custom models\n","    input_dim = X_train.shape[1]\n","    hidden_dim = 10\n","    output_dim = 1\n","    batch_size = 32\n","    learning_rate = 0.0001\n","    epochs = 50000\n","\n","    models[\"Custom MLP\"] = MLP(input_dim, hidden_dim, output_dim,\n","                               batch_size=batch_size, learning_rate=learning_rate, epochs=epochs)\n","    #    learning_rate=0.01, epochs=1000)\n","\n","    num_trees_input = 10\n","    models[\"FONN1\"] = FONN1(input_dim, hidden_dim, output_dim, num_trees_input,\n","                            batch_size=batch_size, learning_rate=learning_rate, epochs=epochs)\n","    # learning_rate=0.01, epochs=1000)\n","    models[\"Tree-based Predictions (FONN1)\"] = models[\"FONN1\"].trees\n","\n","    models[\"TREENN1\"] = TREENN1(input_dim, hidden_dim, output_dim,\n","                                batch_size=batch_size, learning_rate=learning_rate, epochs=epochs)\n","    # learning_rate=0.01, epochs=40000)\n","    models[\"Tree-based Predictions (TREENN1)\"] = models[\"TREENN1\"].trees\n","\n","    num_trees_hidden = 10\n","    models[\"FONN2\"] = FONN2(input_dim, hidden_dim, output_dim, num_trees_hidden,\n","                            batch_size=batch_size, learning_rate=learning_rate, epochs=epochs)\n","    # learning_rate=0.01, epochs=epochs)\n","    models[\"Tree-based Predictions (FONN2)\"] = models[\"FONN2\"].trees\n","\n","    models[\"TREENN2\"] = TREENN2(input_dim, hidden_dim, output_dim,\n","                                batch_size=batch_size, learning_rate=learning_rate, epochs=epochs)\n","    # learning_rate=0.01, epochs=epochs)\n","    models[\"Tree-based Predictions (TREENN2)\"] = models[\"TREENN2\"].trees\n","\n","    models[\"Ensemble of 10 Trees\"] = Ensemble(10)\n","\n","    # Train and evaluate models\n","    results = {}\n","    for name, model in models.items():\n","        r2, mae, mse, fit_time, comp_time = train_evaluate_model(\n","            model, X_train, X_test, y_train, y_test)\n","        results[name] = {\"R² Score\": r2, \"MAE\": mae, \"MSE\": mse,\n","                         \"Train Time (s)\": fit_time, \"Comp Time (s)\": comp_time}\n","\n","    # Convert results to a DataFrame for better visualization\n","    results_df = pd.DataFrame(results).T\n","    return results_df\n","\n","def test_sheets():\n","    for sheet_name in sheet_names:\n","        print(sheet_name)\n","        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n","        iri_key_counts = df[\"IRI_KEY\"].value_counts()\n","        iri_keys = iri_key_counts[iri_key_counts > 300].index\n","\n","        for iri_key in iri_keys:\n","            df_iri = df[df[\"IRI_KEY\"] == iri_key]\n","            X = df_iri[features]\n","            y = df_iri[target].values.reshape(-1, 1)\n","\n","            print(sheet_name, iri_key, X.shape, y.shape)\n","\n","            results = test_models(X, y)\n","            results.to_csv(f\"output/{str(sheet_name)+'_'+str(iri_key)}.csv\")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"ename":"ImportError","evalue":"HalvingGridSearchCV is experimental and the API might change without any deprecation cycle. To use it, you need to explicitly import enable_halving_search_cv:\nfrom sklearn.experimental import enable_halving_search_cv","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HalvingGridSearchCV\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_params\u001b[39m(model, X, y):\n\u001b[1;32m      4\u001b[0m     param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1e-5\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m, \u001b[38;5;241m1e-2\u001b[39m, \u001b[38;5;241m1e-1\u001b[39m],\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m5000\u001b[39m, \u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m50000\u001b[39m]\n\u001b[1;32m      7\u001b[0m     }\n","File \u001b[0;32m~/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/model_selection/__init__.py:82\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHalvingGridSearchCV\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHalvingRandomSearchCV\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is experimental and the API might change without any \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecation cycle. To use it, you need to explicitly import \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_halving_search_cv:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom sklearn.experimental import enable_halving_search_cv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         )\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mImportError\u001b[0m: HalvingGridSearchCV is experimental and the API might change without any deprecation cycle. To use it, you need to explicitly import enable_halving_search_cv:\nfrom sklearn.experimental import enable_halving_search_cv"]}],"source":["from sklearn.model_selection import HalvingGridSearchCV\n","\n","def search_params(model, X, y):\n","    param_grid = {\n","        'learning_rate': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n","        'epochs': [1000, 5000, 10000, 50000]\n","    }\n","    search = HalvingGridSearchCV(model, param_grid, random_state=42).fit(X, y)\n","    return search.best_params_\n","\n","\n","input_dim = X_train.shape[1]\n","hidden_dim = 10\n","output_dim = 1\n","batch_size = 32\n","# learning_rate = 0.0001\n","# epochs = 50000\n","\n","param_grid = {\n","    'learning_rate': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n","    'epochs': [1000, 5000, 10000, 50000]\n","}\n","\n","mlp = MLP(input_dim, hidden_dim, output_dim,\n","          batch_size=batch_size)\n","# mlp.__init__.__code__.co_varnames[1:]\n","# search_params(mlp, X_train, y_train)\n","search = HalvingGridSearchCV(mlp, param_grid, random_state=42)\n","search.fit(X_train, y_train)"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
