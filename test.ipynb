{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"yNSyFZvf0Leo"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["((10, 2), (10,))"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","\n","# Define the data\n","data = {\n","    \"Size (sq ft)\": [850, 900, 1200, 1400, 1600, 1700, 1800, 2000, 2200, 2500],\n","    \"Bedrooms\": [2, 3, 3, 3, 3, 4, 4, 4, 5, 5],\n","    \"Price ($)\": [300000, 340000, 400000, 500000, 520000, 580000, 600000, 620000, 720000, 790000]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","X = df[[\"Size (sq ft)\", \"Bedrooms\"]].values\n","y = df[\"Price ($)\"].values\n","\n","X.shape, y.shape"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","X = X.astype(np.float64)\n","y = y.astype(np.float64)\n","\n","# scaler_X = StandardScaler()\n","# X = scaler_X.fit_transform(X)\n","# scaler_y = StandardScaler()\n","# y = scaler_y.fit_transform(y.reshape(-1, 1)).ravel()  # type: ignore"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from models_sklearn import Ensemble, MLP, FONN1, FONN2, TREENN1, TREENN2\n","from models import MLP as MLP_2, FONN1 as FONN1_2, FONN2 as FONN2_2, TREENN1 as TREENN1_2, TREENN2 as TREENN2_2\n","\n","ensemble_param_grid = {\n","}\n","\n","nn_param_grid = {\n","    'activation': ['tanh'],\n","    # 'max_iter': [1000, 100, 2000],\n","    'max_iter': [400],\n","    'learning_rate': ['constant'],\n","    # 'learning_rate': ['adaptive'],\n","    # 'learning_rate_init': [1e-1, 1e-2, 1e-3],\n","    # 'learning_rate_init': [1e1, 1e-2, 1e-3],\n","    'learning_rate_init': [1e-2],\n","    # 'early_stopping': [True],\n","    'early_stopping': [False],\n","    'n_iter_no_change': [10000]\n","}\n","\n","models = {}\n","\n","num_trees_input = 5\n","num_trees_hidden = 5\n","hidden_nodes = [5]\n","# hidden_nodes = [5, 10, 40, 100]\n","# hidden_nodes = [(5,), (10,), (40,), (100,)]\n","\n","models['Tree'] = Ensemble(1)\n","for hn in hidden_nodes:\n","    if isinstance(hn, tuple):\n","        models[f'Ensemble {sum(hn)}'] = Ensemble(sum(hn))\n","        models[f'MLP {hn}'] = MLP(hn)\n","        models[f'FONN1 {num_trees_input} {hn}'] = FONN1(\n","            num_trees_input, (num_trees_input+hn[0], *hn[1:]))\n","        models[f'FONN2 {num_trees_hidden} {hn}'] = FONN2(\n","            num_trees_hidden, (*hn[:-1], num_trees_hidden+hn[-1]))\n","        models[f'TREENN1 {hn}'] = TREENN1((1+hn[0], *hn[1:]))\n","        models[f'TREENN2 {hn}'] = TREENN2((*hn[:-1], 1+hn[-1]))\n","    else:\n","        models[f'Ensemble {hn}'] = Ensemble(hn)\n","        models[f'MLP {hn}'] = MLP(hn)\n","        models[f'FONN1 {num_trees_input} {hn}'] = FONN1(\n","            num_trees_input, num_trees_input+hn)\n","        models[f'FONN2 {num_trees_hidden} {hn}'] = FONN2(\n","            hn, num_trees_hidden+hn)\n","        models[f'TREENN1 {hn}'] = TREENN1(1+hn)\n","        models[f'TREENN2 {hn}'] = TREENN2(1+hn)\n","\n","nn_param_grid_2 = {\n","    'epochs': [400],\n","    # 'epochs': [100],\n","    # 'learning_rate': [1e1, 1e0, 1e-1, 1e-2],\n","    # 'learning_rate': [1e-2, 1e-3],\n","    'learning_rate': [1e-2],\n","}\n","\n","input_dim = X.shape[1]\n","output_dim = 1\n","\n","for hn in hidden_nodes:\n","    models[f'MLP_2 {hn}'] = MLP_2(input_dim, hn, output_dim)\n","    models[f'FONN1_2 {num_trees_input} {hn}'] = FONN1_2(\n","        input_dim, hn, output_dim, num_trees_input)\n","    models[f'FONN2_2 {num_trees_hidden} {hn}'] = FONN2_2(\n","        input_dim, hn, output_dim, num_trees_hidden)\n","    # models[f'TREENN1_2 {hn}'] = ((\n","    #     TREENN1_2(input_dim, hn, output_dim), nn_param_grid_2)\n","    # models[f'TREENN2_2 {hn}'] = ((\n","    #     TREENN2_2(input_dim, hn, output_dim), nn_param_grid_2)\n","    models[f'TREENN1_2 {hn}'] = FONN1_2(input_dim, hn, output_dim, 1)\n","    models[f'TREENN2_2 {hn}'] = FONN2_2(input_dim, hn, output_dim, 1)\n","    # models['Tree-based Predictions (FONN2)'] = models['FONN2'].trees\n","    # models['Tree-based Predictions (TREENN2)'] = models['TREENN2'].trees"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# import sys\n","# import os\n","# from warnings import simplefilter\n","# if not sys.warnoptions:\n","#     simplefilter(\"ignore\")\n","#     os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n","\n","# # Train and evaluate models\n","# for name, model in models.items():\n","#     model.fit(X, y)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/chess/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"data":{"text/plain":["(3, 3, [(2, 10), (10, 5), (5, 1)], [(10,), (5,), (1,)])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["mlp = MLP((10, 5))\n","mlp.fit(X, y)\n","len(mlp.coefs_), len(mlp.intercepts_), [c.shape for c in mlp.coefs_], [i.shape for i in mlp.intercepts_]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ 0.6675725 ],\n","       [-0.01740139],\n","       [ 1.20226013],\n","       [ 1.17515261],\n","       [ 0.95843878]])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["mlp.coefs_[-1]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["(3, 3, [(2, 10), (10, 5), (5, 1)], [(10,), (5,), (1,)])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["mlp_2 = MLP_2(input_dim, (10, 5), output_dim)\n","len(mlp_2.coefs_), len(mlp_2.intercepts_), [c.shape for c in mlp_2.coefs_], [i.shape for i in mlp_2.intercepts_]\n","\n","self.coefs_ = []\n","self.intercepts_ = []\n","\n","hidden_dim = self.hidden_dim\n","if not hasattr(hidden_dim, \"__iter__\"):\n","    hidden_dim = [hidden_dim]\n","hidden_dim = list(hidden_dim)\n","\n","layer_units = [self.input_dim] + hidden_dim + [self.output_dim]\n","self.n_layers_ = len(layer_units)\n","\n","for i in range(self.n_layers_ - 1):\n","    coef_init = np.random.randn(layer_units[i], layer_units[i + 1])\n","    intercept_init = np.zeros(layer_units[i+1])\n","    self.coefs_.append(coef_init)\n","    self.intercepts_.append(intercept_init)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["(array([[ 0.17097097],\n","        [-2.41645177],\n","        [-0.53310392],\n","        [ 1.62438206],\n","        [ 0.32513365]]),\n"," array([0.]))"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["mlp_2.coefs_[-1], mlp_2.intercepts_[-1]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["[(10, 2), (10, 10), (10, 5), (10, 1)]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["activations = mlp_2._forward(X)\n","[a.shape for a in activations]"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(5, 10)\n"]},{"ename":"ValueError","evalue":"shapes (10,10) and (1,5) not aligned: 10 (dim 1) != 1 (dim 0)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Compute the gradients for the hidden layers\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(mlp_2\u001b[38;5;241m.\u001b[39mn_layers_ \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoefs_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(activations[i])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Tanh derivative\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     coef_grads[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(activations[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT, loss) \u001b[38;5;241m/\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: shapes (10,10) and (1,5) not aligned: 10 (dim 1) != 1 (dim 0)"]}],"source":["loss = activations[-1] - y\n","\n","coef_grads = [\n","    np.empty((n_fan_in_, n_fan_out_), dtype=X.dtype)\n","    for n_fan_in_, n_fan_out_ in zip(mlp_2.layer_units[:-1], mlp_2.layer_units[1:])\n","]\n","\n","intercept_grads = [\n","    np.empty(n_fan_out_, dtype=X.dtype) for n_fan_out_ in mlp_2.layer_units[1:]\n","]\n","\n","# Compute gradients for the output layer\n","coef_grads[-1] = np.dot(activations[-2].T, loss) / y.shape[0]\n","intercept_grads[-1] = np.mean(loss, axis=0)\n","print(coef_grads[-1].shape)\n","# Compute the gradients for the hidden layers\n","for i in range(mlp_2.n_layers_ - 2, 0, -1):\n","    loss = np.dot(loss, mlp_2.coefs_[i].T)\n","    loss *= (1 - np.tanh(activations[i])**2)  # Tanh derivative\n","    coef_grads[i-1] = np.dot(activations[i-1].T, loss) / y.shape[0]\n","    intercept_grads[i-1] = np.mean(loss, axis=0)\n","\n","# # Gradient clipping to prevent exploding gradients\n","# max_grad_norm = 1.0\n","# for i in range(self.n_layers_):\n","#     coef_grads[i] = np.clip(\n","#         coef_grads[i], -max_grad_norm, max_grad_norm)\n","#     intercept_grads[i] = np.clip(\n","#         intercept_grads[i], -max_grad_norm, max_grad_norm)\n","\n","#     # Update weights and biases using gradient descent\n","#     self.coefs_[i] -= self.learning_rate * coef_grads[i]\n","#     self.intercepts_[i] -= self.learning_rate * intercept_grads[i]"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"research","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
